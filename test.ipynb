{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chardet\n",
    "from langchain_text_splitters import CharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "def load_markdown(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        encoding = result['encoding']\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on\n",
    "    )\n",
    "\n",
    "    with open(data_path, 'rt', encoding=encoding) as file:\n",
    "        data_string = file.read()\n",
    "        documents = markdown_splitter.split_text(data_string)\n",
    "\n",
    "        # 파일명을 metadata에 추가\n",
    "        domain = data_path  # os.path.basename(data_path)\n",
    "        for doc in documents:\n",
    "            if not doc.metadata:\n",
    "                doc.metadata = {}\n",
    "            doc.metadata[\"domain\"] = domain  # Document 객체의 metadata 속성에 파일명 추가\n",
    "\n",
    "        return documents\n",
    "\n",
    "\n",
    "def load_txt(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        encoding = result['encoding']\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    with open(data_path, 'r', encoding=encoding) as file:\n",
    "        data_string = file.read().split(\"\\n\")\n",
    "        domain = data_path  # os.path.basename(data_path)\n",
    "        documents = text_splitter.create_documents(data_string)\n",
    "\n",
    "        for doc in documents:\n",
    "            if not doc.metadata:\n",
    "                doc.metadata = {}\n",
    "            doc.metadata[\"domain\"] = domain  # Document 객체의 metadata 속성에 파일명 추가\n",
    "\n",
    "        return documents\n",
    "\n",
    "\n",
    "def load_general(base_dir):\n",
    "    data = []\n",
    "    cnt = 0\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if \".txt\" in file:\n",
    "                cnt += 1\n",
    "                data += load_txt(os.path.join(root, file))\n",
    "\n",
    "    print(f\"the number of txt files is : {cnt}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_document(base_dir):\n",
    "    data = []\n",
    "    cnt = 0\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if \".md\" in file:\n",
    "                cnt += 1\n",
    "                data += load_markdown(os.path.join(root, file))\n",
    "\n",
    "    print(f\"the number of md files is : {cnt}\")\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_markdown_files(source_dir):\n",
    "    md_data = load_document(base_dir=source_dir)\n",
    "    text_data = load_general(base_dir=source_dir)\n",
    "\n",
    "    return md_data + text_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yechanyun/YYC/work/project/AI_Application/rag/source/ragtools-all-in-one/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/yechanyun/YYC/work/project/AI_Application/rag/source/ragtools-all-in-one/lib/python3.11/site-packages/ragas/prompt/base.py:9: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.llms.prompt import PromptValue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of md files is : 17\n",
      "the number of txt files is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples: 100%|██████████| 10/10 [00:05<00:00,  1.72it/s]                                                  \n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.synthesizers.abstract_query import ComparativeAbstractQuerySynthesizer\n",
    "# from ragas.testset.synthesizers.base import BaseSynthesizer\n",
    "# from ragas.testset.synthesizers.base_query import QuerySynthesizer\n",
    "from ragas.testset.synthesizers.specific_query import SpecificQuerySynthesizer\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "ragas_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "# critic_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "# embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "generator = TestsetGenerator.from_langchain(generator_llm)\n",
    "md_files = get_markdown_files(source_dir=\"/Users/yechanyun/YYC/work/project/AI_Application/rag/data/exynos-ai-studio-docs-main\")\n",
    "test_set = generator.generate_with_langchain_docs(md_files,\n",
    "                                                  testset_size=1,\n",
    "                                                  query_distribution=[\n",
    "                                                    (ComparativeAbstractQuerySynthesizer(llm=ragas_llm), 0.5),\n",
    "                                                    (SpecificQuerySynthesizer(llm=ragas_llm), 0.5),],\n",
    "                                                  with_debugging_logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'samples': [{'eval_sample': {'user_input': 'comparison of optimization techniques performance evaluations mixed precision quantization static uniform quantization quantizers optimizers',\n",
       "    'retrieved_contexts': None,\n",
       "    'reference_contexts': ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.',\n",
       "     'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.',\n",
       "     'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.',\n",
       "     'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.',\n",
       "     'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.',\n",
       "     'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.',\n",
       "     'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.',\n",
       "     'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.',\n",
       "     'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.',\n",
       "     'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.',\n",
       "     'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.',\n",
       "     'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.',\n",
       "     'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.',\n",
       "     \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\",\n",
       "     'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.',\n",
       "     'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.',\n",
       "     'The configuration values are utilized across different modules of elt.',\n",
       "     'The model needs to be prepared in ONNX format for optimization.',\n",
       "     \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\",\n",
       "     'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.',\n",
       "     'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.',\n",
       "     'EHT supports ONNX opset versions 13 to 17.',\n",
       "     'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.',\n",
       "     'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.',\n",
       "     \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\",\n",
       "     'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.',\n",
       "     \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\",\n",
       "     'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.',\n",
       "     \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\",\n",
       "     'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.',\n",
       "     'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.',\n",
       "     'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.',\n",
       "     'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.',\n",
       "     'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.',\n",
       "     'This document provides guidelines for preparing the input dataset.',\n",
       "     \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\",\n",
       "     'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'],\n",
       "    'response': None,\n",
       "    'multi_responses': None,\n",
       "    'reference': 'The document compares optimization techniques, including mixed precision quantization and static uniform quantization, by outlining their performance evaluations and the use of various quantizers and optimizers.',\n",
       "    'rubric': None},\n",
       "   'synthesizer_name': 'ComparativeAbstractQuerySynthesizer'},\n",
       "  {'eval_sample': {'user_input': 'How do quantization methods compare on performance and efficiency?',\n",
       "    'retrieved_contexts': None,\n",
       "    'reference_contexts': ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.',\n",
       "     'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.',\n",
       "     'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.',\n",
       "     'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.',\n",
       "     'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.',\n",
       "     'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.',\n",
       "     'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.',\n",
       "     'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.',\n",
       "     'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.',\n",
       "     'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.',\n",
       "     'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.',\n",
       "     'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.',\n",
       "     'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.',\n",
       "     \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\",\n",
       "     'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.',\n",
       "     'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.',\n",
       "     'The configuration values are utilized across different modules of elt.',\n",
       "     'The model needs to be prepared in ONNX format for optimization.',\n",
       "     \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\",\n",
       "     'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.',\n",
       "     'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.',\n",
       "     'EHT supports ONNX opset versions 13 to 17.',\n",
       "     'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.',\n",
       "     'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.',\n",
       "     \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\",\n",
       "     'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.',\n",
       "     \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\",\n",
       "     'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.',\n",
       "     \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\",\n",
       "     'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.',\n",
       "     'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.',\n",
       "     'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.',\n",
       "     'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.',\n",
       "     'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.',\n",
       "     'This document provides guidelines for preparing the input dataset.',\n",
       "     \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\",\n",
       "     'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'],\n",
       "    'response': None,\n",
       "    'multi_responses': None,\n",
       "    'reference': 'Quantization methods compare on performance and efficiency by applying various techniques such as Smooth Quantization, Mixed Precision Quantization, and advanced methods like Softmax Bias Correction and Cross Layer Equalization, which aim to optimize model performance while reducing quantization error.',\n",
       "    'rubric': None},\n",
       "   'synthesizer_name': 'ComparativeAbstractQuerySynthesizer'},\n",
       "  {'eval_sample': {'user_input': 'How do the various model conversion and optimization techniques, including ONNX-to-CNNX conversion, quantization methods, and performance evaluation strategies, compare in their effectiveness and efficiency as outlined in the provided summaries?',\n",
       "    'retrieved_contexts': None,\n",
       "    'reference_contexts': ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.',\n",
       "     'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.',\n",
       "     'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.',\n",
       "     'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.',\n",
       "     'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.',\n",
       "     'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.',\n",
       "     'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.',\n",
       "     'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.',\n",
       "     'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.',\n",
       "     'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.',\n",
       "     'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.',\n",
       "     'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.',\n",
       "     'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.',\n",
       "     \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\",\n",
       "     'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.',\n",
       "     'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.',\n",
       "     'The configuration values are utilized across different modules of elt.',\n",
       "     'The model needs to be prepared in ONNX format for optimization.',\n",
       "     \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\",\n",
       "     'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.',\n",
       "     'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.',\n",
       "     'EHT supports ONNX opset versions 13 to 17.',\n",
       "     'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.',\n",
       "     'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.',\n",
       "     \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\",\n",
       "     'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.',\n",
       "     \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\",\n",
       "     'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.',\n",
       "     \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\",\n",
       "     'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.',\n",
       "     'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.',\n",
       "     'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.',\n",
       "     'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.',\n",
       "     'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.',\n",
       "     'This document provides guidelines for preparing the input dataset.',\n",
       "     \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\",\n",
       "     'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'],\n",
       "    'response': None,\n",
       "    'multi_responses': None,\n",
       "    'reference': 'The various model conversion and optimization techniques, including ONNX-to-CNNX conversion, quantization methods, and performance evaluation strategies, are compared in their effectiveness and efficiency through a structured process that involves converting models, optimizing them, and evaluating performance before and after optimization.',\n",
       "    'rubric': None},\n",
       "   'synthesizer_name': 'ComparativeAbstractQuerySynthesizer'},\n",
       "  {'eval_sample': {'user_input': 'How do the various performance evaluation techniques and optimization methods compare across different reports for model optimization, including aspects such as quantization, mixed precision, and the use of specific tools like the Quantizer, Optimizer, and performance estimators?',\n",
       "    'retrieved_contexts': None,\n",
       "    'reference_contexts': ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.',\n",
       "     'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.',\n",
       "     'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.',\n",
       "     'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.',\n",
       "     'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.',\n",
       "     'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.',\n",
       "     'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.',\n",
       "     'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.',\n",
       "     'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.',\n",
       "     'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.',\n",
       "     'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.',\n",
       "     'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.',\n",
       "     'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.',\n",
       "     \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\",\n",
       "     'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.',\n",
       "     'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.',\n",
       "     'The configuration values are utilized across different modules of elt.',\n",
       "     'The model needs to be prepared in ONNX format for optimization.',\n",
       "     \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\",\n",
       "     'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.',\n",
       "     'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.',\n",
       "     'EHT supports ONNX opset versions 13 to 17.',\n",
       "     'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.',\n",
       "     'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.',\n",
       "     \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\",\n",
       "     'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.',\n",
       "     \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\",\n",
       "     'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.',\n",
       "     \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\",\n",
       "     'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.',\n",
       "     'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.',\n",
       "     'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.',\n",
       "     'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.',\n",
       "     'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.',\n",
       "     'This document provides guidelines for preparing the input dataset.',\n",
       "     \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\",\n",
       "     'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'],\n",
       "    'response': None,\n",
       "    'multi_responses': None,\n",
       "    'reference': 'The document compares various performance evaluation techniques and optimization methods for model optimization, including quantization, mixed precision, and specific tools like the Quantizer and Optimizer. It outlines steps for converting and optimizing models, applying quantization methods, and evaluating performance before and after optimization.',\n",
       "    'rubric': None},\n",
       "   'synthesizer_name': 'ComparativeAbstractQuerySynthesizer'},\n",
       "  {'eval_sample': {'user_input': 'How do the various optimization techniques and quantization methods for neural networks compare in terms of performance evaluation and model efficiency across different reports?',\n",
       "    'retrieved_contexts': None,\n",
       "    'reference_contexts': ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.',\n",
       "     'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.',\n",
       "     'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.',\n",
       "     'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.',\n",
       "     'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.',\n",
       "     'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.',\n",
       "     'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.',\n",
       "     'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.',\n",
       "     'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.',\n",
       "     'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.',\n",
       "     'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.',\n",
       "     'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.',\n",
       "     'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.',\n",
       "     \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\",\n",
       "     'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.',\n",
       "     'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.',\n",
       "     'The configuration values are utilized across different modules of elt.',\n",
       "     'The model needs to be prepared in ONNX format for optimization.',\n",
       "     \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\",\n",
       "     'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.',\n",
       "     'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.',\n",
       "     'EHT supports ONNX opset versions 13 to 17.',\n",
       "     'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.',\n",
       "     'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.',\n",
       "     \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\",\n",
       "     'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.',\n",
       "     \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\",\n",
       "     'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.',\n",
       "     \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\",\n",
       "     'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.',\n",
       "     'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.',\n",
       "     'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.',\n",
       "     'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.',\n",
       "     'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.',\n",
       "     'This document provides guidelines for preparing the input dataset.',\n",
       "     \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\",\n",
       "     'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'],\n",
       "    'response': None,\n",
       "    'multi_responses': None,\n",
       "    'reference': 'The various optimization techniques and quantization methods for neural networks are compared in terms of performance evaluation and model efficiency through a structured process that includes converting models, optimizing them, and evaluating performance before and after optimization.',\n",
       "    'rubric': None},\n",
       "   'synthesizer_name': 'ComparativeAbstractQuerySynthesizer'},\n",
       "  {'eval_sample': {'user_input': 'What parameters need to be set to enable SSD detection in the perf_estimator configuration?',\n",
       "    'retrieved_contexts': None,\n",
       "    'reference_contexts': ['perf_estimator : dict**\\n- **O2_enable : bool**: O2 optimization (true or false)\\n- **O2_fm_forwarding : bool**: feature-map forwarding (true or false)\\n- **SEG : bool**: Set true if input model is Deeplab V3+\\n- **SSD : bool**: Set true if input model is SSD detection\\n- **bit_width_factor_FM : int**: Select feature map bit width factor (1 or 2)\\n- **bit_width_factor_FP16 : bool**: Set bit width factor as floating point (true or false)\\n- **bit_width_factor_weight : int**: Select weight bit width factor (1 or 2)\\n- **core_num : int**: 1 for single core, 2 for instance-1\\n- **device : str**: Select device type [Gen-3, Gen-4, Gen-5, Gen-5a]\\n- **json_report : bool**: Enable report json format\\n- **nq_fold : bool**: Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32)  \\n---  \\n**'],\n",
       "    'response': None,\n",
       "    'multi_responses': None,\n",
       "    'reference': 'To enable SSD detection in the perf_estimator configuration, the parameter SSD needs to be set to true.',\n",
       "    'rubric': None},\n",
       "   'synthesizer_name': 'SpecificQuerySynthesizer'},\n",
       "  {'eval_sample': {'user_input': 'What steps are involved in applying user-defined optimization methods using the Optimizer Template?',\n",
       "    'retrieved_contexts': None,\n",
       "    'reference_contexts': ['Optimizer is a tool that allows users to easily optimize their models. Users can add their own optimization methods using the \"Optimizer Template\" of Optimizer. The following process is required to apply the user-defined template to the user model.  \\n- Create custom templates.\\n- Prepare model to be optimized\\n- Validate optimized model'],\n",
       "    'response': None,\n",
       "    'multi_responses': None,\n",
       "    'reference': 'The steps involved in applying user-defined optimization methods using the Optimizer Template include creating custom templates, preparing the model to be optimized, and validating the optimized model.',\n",
       "    'rubric': None},\n",
       "   'synthesizer_name': 'SpecificQuerySynthesizer'},\n",
       "  {'eval_sample': {'user_input': 'What is the significance of specifying the output model path in the conversion process of machine learning models?',\n",
       "    'retrieved_contexts': None,\n",
       "    'reference_contexts': ['Example codes to use them:\\n```python\\nfrom converter import api\\ncnnx_to_snc_params = api.Cnnx2SncParameters(\\ninput_model_path = \"/path/to/model.onnx\",\\ninput_encodings_path = \"/path/to/model.encodings\",\\noutput_model_path = \"/output/path/for/model.snc\"\\n)\\n\\napi.Converter.cnnx_to_snc(cnnx_to_snc_params)\\n\\ntflite_to_onnx_params = api.TfLite2OnnxParameters(\\ninput_model_path = \"/path/to/model.tflite\",\\noutput_model_path = \"./output/path/for/model.onnx\",\\n)\\n\\napi.Converter.tflite_to_onnx(tflite_to_onnx_params)\\n```'],\n",
       "    'response': None,\n",
       "    'multi_responses': None,\n",
       "    'reference': 'Specifying the output model path in the conversion process of machine learning models is significant because it determines where the converted model will be saved, ensuring that the user can access and utilize the model after the conversion is complete.',\n",
       "    'rubric': None},\n",
       "   'synthesizer_name': 'SpecificQuerySynthesizer'},\n",
       "  {'eval_sample': {'user_input': \"What is the purpose of the 'simulator' in the context of the provided dictionary?\",\n",
       "    'retrieved_contexts': None,\n",
       "    'reference_contexts': ['simulator : dict**  \\n- **metric : string**: The metric to be used for measurement\\n- **threshold : float** : The threshold value of the metric that determines agreement / disagreement\\n- **input_data_path : string**: The path to the dataset for model inference  \\n---  \\n**'],\n",
       "    'response': None,\n",
       "    'multi_responses': None,\n",
       "    'reference': \"The purpose of the 'simulator' in the provided dictionary is to define a structure that includes a metric for measurement, a threshold value for determining agreement or disagreement, and a path to the dataset for model inference.\",\n",
       "    'rubric': None},\n",
       "   'synthesizer_name': 'SpecificQuerySynthesizer'},\n",
       "  {'eval_sample': {'user_input': 'What is the purpose of Softmax Bias Correction in advanced quantization methods?',\n",
       "    'retrieved_contexts': None,\n",
       "    'reference_contexts': ['Currently supported advanced quantization methods are as follows.  \\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n- A method that tune the weights range of the channels in one tensor to reduce quantization error  \\n- Dummy Convolution for the Concat Operator\\n- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.'],\n",
       "    'response': None,\n",
       "    'multi_responses': None,\n",
       "    'reference': 'The purpose of Softmax Bias Correction in advanced quantization methods is to add a float bias to the Softmax output layer to reduce the performance degradation caused by quantization.',\n",
       "    'rubric': None},\n",
       "   'synthesizer_name': 'SpecificQuerySynthesizer'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'samples': [{'user_input': 'comparison of optimization techniques performance evaluations mixed precision quantization static uniform quantization quantizers optimizers', 'retrieved_contexts': None, 'reference_contexts': ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.', 'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.', 'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.', 'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.', 'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.', 'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.', 'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.', 'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.', 'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.', 'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.', 'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.', 'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\", 'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.', 'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.', 'The configuration values are utilized across different modules of elt.', 'The model needs to be prepared in ONNX format for optimization.', \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\", 'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.', 'EHT supports ONNX opset versions 13 to 17.', 'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.', 'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.', \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\", 'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.', \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\", 'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.', \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\", 'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.', 'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.', 'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.', 'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.', 'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.', 'This document provides guidelines for preparing the input dataset.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\", 'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'], 'response': None, 'multi_responses': None, 'reference': 'The document compares optimization techniques, including mixed precision quantization and static uniform quantization, by outlining their performance evaluations and the use of various quantizers and optimizers.', 'rubric': None}, {'user_input': 'How do quantization methods compare on performance and efficiency?', 'retrieved_contexts': None, 'reference_contexts': ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.', 'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.', 'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.', 'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.', 'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.', 'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.', 'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.', 'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.', 'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.', 'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.', 'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.', 'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\", 'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.', 'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.', 'The configuration values are utilized across different modules of elt.', 'The model needs to be prepared in ONNX format for optimization.', \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\", 'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.', 'EHT supports ONNX opset versions 13 to 17.', 'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.', 'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.', \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\", 'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.', \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\", 'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.', \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\", 'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.', 'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.', 'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.', 'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.', 'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.', 'This document provides guidelines for preparing the input dataset.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\", 'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'], 'response': None, 'multi_responses': None, 'reference': 'Quantization methods compare on performance and efficiency by applying various techniques such as Smooth Quantization, Mixed Precision Quantization, and advanced methods like Softmax Bias Correction and Cross Layer Equalization, which aim to optimize model performance while reducing quantization error.', 'rubric': None}, {'user_input': 'How do the various model conversion and optimization techniques, including ONNX-to-CNNX conversion, quantization methods, and performance evaluation strategies, compare in their effectiveness and efficiency as outlined in the provided summaries?', 'retrieved_contexts': None, 'reference_contexts': ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.', 'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.', 'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.', 'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.', 'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.', 'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.', 'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.', 'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.', 'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.', 'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.', 'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.', 'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\", 'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.', 'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.', 'The configuration values are utilized across different modules of elt.', 'The model needs to be prepared in ONNX format for optimization.', \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\", 'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.', 'EHT supports ONNX opset versions 13 to 17.', 'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.', 'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.', \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\", 'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.', \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\", 'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.', \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\", 'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.', 'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.', 'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.', 'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.', 'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.', 'This document provides guidelines for preparing the input dataset.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\", 'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'], 'response': None, 'multi_responses': None, 'reference': 'The various model conversion and optimization techniques, including ONNX-to-CNNX conversion, quantization methods, and performance evaluation strategies, are compared in their effectiveness and efficiency through a structured process that involves converting models, optimizing them, and evaluating performance before and after optimization.', 'rubric': None}, {'user_input': 'How do the various performance evaluation techniques and optimization methods compare across different reports for model optimization, including aspects such as quantization, mixed precision, and the use of specific tools like the Quantizer, Optimizer, and performance estimators?', 'retrieved_contexts': None, 'reference_contexts': ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.', 'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.', 'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.', 'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.', 'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.', 'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.', 'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.', 'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.', 'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.', 'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.', 'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.', 'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\", 'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.', 'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.', 'The configuration values are utilized across different modules of elt.', 'The model needs to be prepared in ONNX format for optimization.', \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\", 'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.', 'EHT supports ONNX opset versions 13 to 17.', 'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.', 'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.', \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\", 'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.', \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\", 'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.', \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\", 'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.', 'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.', 'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.', 'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.', 'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.', 'This document provides guidelines for preparing the input dataset.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\", 'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'], 'response': None, 'multi_responses': None, 'reference': 'The document compares various performance evaluation techniques and optimization methods for model optimization, including quantization, mixed precision, and specific tools like the Quantizer and Optimizer. It outlines steps for converting and optimizing models, applying quantization methods, and evaluating performance before and after optimization.', 'rubric': None}, {'user_input': 'How do the various optimization techniques and quantization methods for neural networks compare in terms of performance evaluation and model efficiency across different reports?', 'retrieved_contexts': None, 'reference_contexts': ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.', 'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.', 'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.', 'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.', 'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.', 'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.', 'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.', 'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.', 'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.', 'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.', 'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.', 'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\", 'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.', 'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.', 'The configuration values are utilized across different modules of elt.', 'The model needs to be prepared in ONNX format for optimization.', \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\", 'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.', 'EHT supports ONNX opset versions 13 to 17.', 'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.', 'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.', \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\", 'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.', \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\", 'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.', \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\", 'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.', 'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.', 'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.', 'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.', 'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.', 'This document provides guidelines for preparing the input dataset.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\", 'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'], 'response': None, 'multi_responses': None, 'reference': 'The various optimization techniques and quantization methods for neural networks are compared in terms of performance evaluation and model efficiency through a structured process that includes converting models, optimizing them, and evaluating performance before and after optimization.', 'rubric': None}, {'user_input': 'What parameters need to be set to enable SSD detection in the perf_estimator configuration?', 'retrieved_contexts': None, 'reference_contexts': ['perf_estimator : dict**\\n- **O2_enable : bool**: O2 optimization (true or false)\\n- **O2_fm_forwarding : bool**: feature-map forwarding (true or false)\\n- **SEG : bool**: Set true if input model is Deeplab V3+\\n- **SSD : bool**: Set true if input model is SSD detection\\n- **bit_width_factor_FM : int**: Select feature map bit width factor (1 or 2)\\n- **bit_width_factor_FP16 : bool**: Set bit width factor as floating point (true or false)\\n- **bit_width_factor_weight : int**: Select weight bit width factor (1 or 2)\\n- **core_num : int**: 1 for single core, 2 for instance-1\\n- **device : str**: Select device type [Gen-3, Gen-4, Gen-5, Gen-5a]\\n- **json_report : bool**: Enable report json format\\n- **nq_fold : bool**: Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32)  \\n---  \\n**'], 'response': None, 'multi_responses': None, 'reference': 'To enable SSD detection in the perf_estimator configuration, the parameter SSD needs to be set to true.', 'rubric': None}, {'user_input': 'What steps are involved in applying user-defined optimization methods using the Optimizer Template?', 'retrieved_contexts': None, 'reference_contexts': ['Optimizer is a tool that allows users to easily optimize their models. Users can add their own optimization methods using the \"Optimizer Template\" of Optimizer. The following process is required to apply the user-defined template to the user model.  \\n- Create custom templates.\\n- Prepare model to be optimized\\n- Validate optimized model'], 'response': None, 'multi_responses': None, 'reference': 'The steps involved in applying user-defined optimization methods using the Optimizer Template include creating custom templates, preparing the model to be optimized, and validating the optimized model.', 'rubric': None}, {'user_input': 'What is the significance of specifying the output model path in the conversion process of machine learning models?', 'retrieved_contexts': None, 'reference_contexts': ['Example codes to use them:\\n```python\\nfrom converter import api\\ncnnx_to_snc_params = api.Cnnx2SncParameters(\\ninput_model_path = \"/path/to/model.onnx\",\\ninput_encodings_path = \"/path/to/model.encodings\",\\noutput_model_path = \"/output/path/for/model.snc\"\\n)\\n\\napi.Converter.cnnx_to_snc(cnnx_to_snc_params)\\n\\ntflite_to_onnx_params = api.TfLite2OnnxParameters(\\ninput_model_path = \"/path/to/model.tflite\",\\noutput_model_path = \"./output/path/for/model.onnx\",\\n)\\n\\napi.Converter.tflite_to_onnx(tflite_to_onnx_params)\\n```'], 'response': None, 'multi_responses': None, 'reference': 'Specifying the output model path in the conversion process of machine learning models is significant because it determines where the converted model will be saved, ensuring that the user can access and utilize the model after the conversion is complete.', 'rubric': None}, {'user_input': \"What is the purpose of the 'simulator' in the context of the provided dictionary?\", 'retrieved_contexts': None, 'reference_contexts': ['simulator : dict**  \\n- **metric : string**: The metric to be used for measurement\\n- **threshold : float** : The threshold value of the metric that determines agreement / disagreement\\n- **input_data_path : string**: The path to the dataset for model inference  \\n---  \\n**'], 'response': None, 'multi_responses': None, 'reference': \"The purpose of the 'simulator' in the provided dictionary is to define a structure that includes a metric for measurement, a threshold value for determining agreement or disagreement, and a path to the dataset for model inference.\", 'rubric': None}, {'user_input': 'What is the purpose of Softmax Bias Correction in advanced quantization methods?', 'retrieved_contexts': None, 'reference_contexts': ['Currently supported advanced quantization methods are as follows.  \\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n- A method that tune the weights range of the channels in one tensor to reduce quantization error  \\n- Dummy Convolution for the Concat Operator\\n- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.'], 'response': None, 'multi_responses': None, 'reference': 'The purpose of Softmax Bias Correction in advanced quantization methods is to add a float bias to the Softmax output layer to reduce the performance degradation caused by quantization.', 'rubric': None}]}\n"
     ]
    }
   ],
   "source": [
    "evaluation_dataset = test_set.to_evaluation_dataset()\n",
    "evaluation_dataset\n",
    "print(evaluation_dataset.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset.to_jsonl(\"./evaluation_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.to_jsonl(\"./testset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yechanyun/YYC/work/project/AI_Application/rag/source/ragtools-all-in-one/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/yechanyun/YYC/work/project/AI_Application/rag/source/ragtools-all-in-one/lib/python3.11/site-packages/ragas/prompt/base.py:9: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.llms.prompt import PromptValue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating RAGAS json...(10/10){'samples': [{'user_input': 'comparison of optimization techniques performance evaluations mixed precision quantization static uniform quantization quantizers optimizers', 'retrieved_contexts': ['Section: Quantizer\\n# Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4', 'Section: # Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision.\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4', 'Section: Quantizer\\n# Quantizer\\n## Advanced Quantization Methods\\nContent:\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error', 'Section: # Quantizer\\nContent:\\n## Advanced Quantization Methods\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error\\n- Dummy Convolution for the Concat Operator\\n\\t- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.\\n## Debug API\\n### Layer-wise mixed precision quantiztion debug API\\nThe quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary.', 'Section: # Quantizer\\nContent:\\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API.', 'Section: Quantizer\\n# Quantizer\\nContent:\\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization. Users can apply mixed precision quantization to models by specifying activation names or operators.', \"Section: Optimizer\\n# Optimizer\\nContent:\\n- support optimization method.<br>\\n\\tOptimizer supports many optimization features so that the model works efficiently on the device.\\n\\t- Fold\\n\\t\\t- GeGLU\\n\\t\\t- GroupNorm\\n\\t\\t- LayerNorm\\n\\t\\t- PReLU\\n\\t\\t- RMSNorm\\n\\t- Fuse\\n\\t\\t- SiLU\\n\\t\\t- BatchNorm into Convolution\\n\\t\\t- Cast\\n\\t\\t- Deconvolution bias\\n\\t\\t- Math\\n\\t- Insert\\n\\t\\t- Depthwise Convolution for activation\\n\\t- Replace\\n\\t\\t- Average Pooling to Depthwise convolution\\n\\t\\t- Eltwise concat convolution\\n\\t\\t- Convolution kernel 1 to 3\\n\\t\\t- Matrix multiplication to dynamic convolution\\n\\t\\t- ReduceMean to Global average pool\\n\\t\\t- ReduceSum to Convolution\\n\\t\\t- Slice to Split\\n\\t- Change attribute\\n\\t\\t- axis of softmax\\nIn addition, Optimizer Template, one of the Optimizer's powerful features, supports custom optimization methods that users have.\", 'Section: Optimizer\\n# Optimizer\\nContent:\\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\\n- shape_inference<br>\\n\\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\\n- 4dim conversion<br>\\n\\tIn order for the .nnc extension model to operate on a device, the input and output shapes of all operators in the model must be in four dimensions.', 'Section: # **How to use**\\n## **Detailed explanation for eht yaml file**\\nContent:\\nmodel_type : string**\\nSpecify the type of the input model. Optimization is performed differently depending on the model type, with details as follows\\n- CV\\n- LVM\\n- LLM\\n\\n**quantizer : dict**\\n- **precision_weight : str** : precision of weight(ex. int8, int16, fp16)\\n- **precision_activation : str** : precision of activation(ex. int8, int16, fp16)\\n- **mpq_operator_dict : dict** : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above.\\n- **alpha : float** : smoothquant migration strength', 'Section: # Optimizer\\nContent:\\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\\n- shape_inference<br>\\n\\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\\n- 4dim conversion<br>\\n\\tTo deploy a model with a .nnc extension on a device, all operators must have input and output shapes with four dimensions. To facilitate this, the 4D conversion process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. '], 'reference_contexts': ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.', 'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.', 'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.', 'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.', 'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.', 'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.', 'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.', 'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.', 'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.', 'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.', 'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.', 'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\", 'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.', 'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.', 'The configuration values are utilized across different modules of elt.', 'The model needs to be prepared in ONNX format for optimization.', \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\", 'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.', 'EHT supports ONNX opset versions 13 to 17.', 'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.', 'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.', \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\", 'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.', \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\", 'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.', \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\", 'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.', 'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.', 'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.', 'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.', 'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.', 'This document provides guidelines for preparing the input dataset.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\", 'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'], 'response': 'The context provided does not contain specific performance evaluations or comparisons of optimization techniques such as mixed precision quantization and static uniform quantization, particularly in relation to quantizers and optimizers. \\n\\nHowever, I can summarize their characteristics based on the information given:\\n\\n1. **Fixed Precision Quantization**:\\n   - Applies static uniform quantization to weights and activations.\\n   - Users specify a single precision (bit-width) for the entire model.\\n   - This method is straightforward but may not optimize performance as effectively as mixed precision methods.\\n\\n2. **Mixed Precision Quantization (MPQ)**:\\n   - Allows different parts of the model to utilize different precisions.\\n   - Two approaches: by name (specific activation/weight names) or by operator (different precisions for different operators).\\n   - Can lead to better performance by optimizing the precision of specific model components.\\n\\n3. **Optimizers**:\\n   - Aim to improve model performance on Exynos chips by modifying the architecture for efficiency.\\n   - Supports various optimization features such as folding, fusing, inserting, replacing, and changing attributes.\\n   - Provides shape inference and 4D conversion to ensure compatibility with the model deployment.\\n\\nIn general, mixed precision quantization might yield better performance in certain scenarios by more selectively applying quantization. In contrast, fixed precision quantization could be simpler but might not maximize efficiency. The optimizer further refines the model for better performance on target hardware, accommodating changes such as input shapes and dimensionality.\\n\\nFor detailed performance evaluations, testing these techniques in specific scenarios would provide more conclusive insights. If you have further questions or need specific details, feel free to ask!', 'multi_responses': None, 'reference': 'The document compares optimization techniques, including mixed precision quantization and static uniform quantization, by outlining their performance evaluations and the use of various quantizers and optimizers.', 'rubric': None}, {'user_input': 'How do quantization methods compare on performance and efficiency?', 'retrieved_contexts': ['Section: Quantizer\\n# Quantizer\\n## Advanced Quantization Methods\\nContent:\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error', 'Section: Quantizer\\n# Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4', 'Section: # Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision.\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4', 'Section: # Quantizer\\nContent:\\n## Advanced Quantization Methods\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error\\n- Dummy Convolution for the Concat Operator\\n\\t- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.\\n## Debug API\\n### Layer-wise mixed precision quantiztion debug API\\nThe quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary.', 'Section: Quantizer\\n# Quantizer\\nContent:\\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization. Users can apply mixed precision quantization to models by specifying activation names or operators.', 'Section: # Quantizer\\nContent:\\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API.', 'Section: # Simulator\\nContent:\\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\\nSimulator includes the following features(with example codes):', 'Section: Simulator\\n# Simulator\\nContent:\\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\\nSimulator includes the following features:\\n- `get_quantization_sim`: Attain a CNNX quantization simulation session for manual run.\\n- `load_input_data`: Load the input data from the given dataset path.\\n- `run_inference`: Conduct inference on a CNNX model.\\n- `compare_model_by_inference`: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric.\\n- `compare_model_by_layer` (TBD): Compare two CNNX models on intermediate tensors for each layer. To be released.', 'Section: # **How to use**\\n## **Detailed explanation for eht yaml file**\\nContent:\\nmodel_type : string**\\nSpecify the type of the input model. Optimization is performed differently depending on the model type, with details as follows\\n- CV\\n- LVM\\n- LLM\\n\\n**quantizer : dict**\\n- **precision_weight : str** : precision of weight(ex. int8, int16, fp16)\\n- **precision_activation : str** : precision of activation(ex. int8, int16, fp16)\\n- **mpq_operator_dict : dict** : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above.\\n- **alpha : float** : smoothquant migration strength', 'Section: # Simulator\\nContent:\\n- `get_quantization_sim`: Attain a CNNX quantization simulation session for manual run.\\n\\t```python\\n\\tfrom simulator imoprt api\\n\\toutput_names = [LIST_OF_ONNX_OUTPUTS]\\n\\tinput_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1]\\n\\tquantsim = api.get_quantization_sim(params)\\n\\tinference_session = quantsim.session\\n\\tresult = session.run(output_names, input_dict)\\n\\tprint(result)\\n\\t```\\n- `load_input_data`: Load the input data from the given dataset path.\\n- `run_inference`: Conduct inference on a CNNX model.\\n- `compare_model_by_inference`: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric.'], 'reference_contexts': ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.', 'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.', 'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.', 'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.', 'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.', 'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.', 'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.', 'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.', 'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.', 'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.', 'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.', 'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\", 'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.', 'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.', 'The configuration values are utilized across different modules of elt.', 'The model needs to be prepared in ONNX format for optimization.', \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\", 'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.', 'EHT supports ONNX opset versions 13 to 17.', 'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.', 'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.', \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\", 'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.', \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\", 'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.', \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\", 'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.', 'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.', 'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.', 'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.', 'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.', 'This document provides guidelines for preparing the input dataset.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\", 'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'], 'response': 'We do not have the information you requested. If you wish to contact support about this inquiry, please send an email to seed.ai@samsung.com.', 'multi_responses': None, 'reference': 'Quantization methods compare on performance and efficiency by applying various techniques such as Smooth Quantization, Mixed Precision Quantization, and advanced methods like Softmax Bias Correction and Cross Layer Equalization, which aim to optimize model performance while reducing quantization error.', 'rubric': None}, {'user_input': 'How do the various model conversion and optimization techniques, including ONNX-to-CNNX conversion, quantization methods, and performance evaluation strategies, compare in their effectiveness and efficiency as outlined in the provided summaries?', 'retrieved_contexts': ['Section: # Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: # Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: # Model Optimization Flow\\n## LLM\\nContent:\\n1. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **CNNX-to SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## LLM\\nContent:\\n1. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **CNNX-to SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: # Converter\\nContent:\\n- ONNX2CNNX Converter\\n\\t- Converts an ONNX model to CNNX.\\n\\t- Requires specification of both the output model path and the output encodings path.\\n- CNNX2SNC Converter\\n\\t- Converts a CNNX model to SNC.\\nExample codes to use them:\\n```python\\nfrom converter import api\\ncnnx_to_snc_params = api.Cnnx2SncParameters(\\n\\tinput_model_path = \"/path/to/model.onnx\",\\n\\tinput_encodings_path = \"/path/to/model.encodings\",\\n\\toutput_model_path = \"/output/path/for/model.snc\"\\n\\t)\\napi.Converter.cnnx_to_snc(cnnx_to_snc_params)\\ntflite_to_onnx_params = api.TfLite2OnnxParameters(\\n\\tinput_model_path = \"/path/to/model.tflite\",\\n\\toutput_model_path = \"./output/path/for/model.onnx\",\\n\\t)\\napi.Converter.tflite_to_onnx(tflite_to_onnx_params)\\n```', 'Section: # Simulator\\nContent:\\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\\nSimulator includes the following features(with example codes):', 'Section: Simulator\\n# Simulator\\nContent:\\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\\nSimulator includes the following features:\\n- `get_quantization_sim`: Attain a CNNX quantization simulation session for manual run.\\n- `load_input_data`: Load the input data from the given dataset path.\\n- `run_inference`: Conduct inference on a CNNX model.\\n- `compare_model_by_inference`: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric.\\n- `compare_model_by_layer` (TBD): Compare two CNNX models on intermediate tensors for each layer. To be released.', 'Section: Model Requirements and Constraints\\n# Model Requirements and Constraints\\n## CNNX model\\nContent:\\nCNNX model consists of ONNX model and encodings file. Encodings file should contain quantization information and optimization module information according to the given ONNX model in json format.\\nExample for encodings file is shown below:\\n```json\\n{\\n\\t\"activation_encodings\": {\\n\\t\\t\"/act/a0\": [\\n\\t\\t\\t{\\n\\t\\t\\t\\t\"bitwidth\": 8,\\n\\t\\t\\t\\t\"dtype\": \"int\",\\n\\t\\t\\t\\t\"is_symmetric\": \"False\",\\n\\t\\t\\t\\t\"max\": 0.0,\\n\\t\\t\\t\\t\"min\": 0.0,\\n\\t\\t\\t\\t\"offset\": 0,\\n\\t\\t\\t\\t\"scale\": 0.0\\n\\t\\t\\t}'], 'reference_contexts': ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.', 'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.', 'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.', 'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.', 'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.', 'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.', 'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.', 'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.', 'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.', 'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.', 'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.', 'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\", 'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.', 'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.', 'The configuration values are utilized across different modules of elt.', 'The model needs to be prepared in ONNX format for optimization.', \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\", 'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.', 'EHT supports ONNX opset versions 13 to 17.', 'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.', 'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.', \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\", 'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.', \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\", 'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.', \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\", 'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.', 'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.', 'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.', 'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.', 'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.', 'This document provides guidelines for preparing the input dataset.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\", 'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'], 'response': \"The provided summaries outline several model conversion and optimization techniques used in the ENN SDK, each with its unique processes and goals. Here's a comparison based on the outlined techniques:\\n\\n1. **ONNX-to-CNNX Conversion**: \\n   - This is the initial step in both the CV and LVM flows, where an opset16 ONNX model is converted into CNNX format, with the output stored in `{result_dir}/cnnx`. The effectiveness of this step is crucial as it prepares the model for subsequent optimization.\\n\\n2. **Optimization**:\\n   - In both CV and LVM workflows, the optimization phase involves simplifying the CNNX model. The CV flow uses a Simplifier and an Optimization template, while the LVM flow combines the Simplifier with a 4-Dimensional Conversion followed by an Optimization template. \\n   - The LLM flow focuses only on optimization and does not include CNNX-to-(Intermediate) conversion.\\n   - The effectiveness and efficiency can be deemed higher in LVM due to the enhanced complexity handled by the additional conversion step.\\n\\n3. **Performance Evaluation of Optimization**: \\n   - Each workflow includes a performance evaluation phase, which compares inference results before and after optimization, serving as a critical feedback mechanism for validating the optimization's effectiveness and efficiency. \\n\\n4. **Quantization**:\\n   - For quantization, the CV flow applies Fixed Precision Quantization, while the LVM flow applies more advanced techniques, including Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction. \\n   - The LVM flow's approach suggests it may yield better performance and fidelity in the quantization process since it employs multiple sophisticated techniques compared to the single approach in CV.\\n\\n5. **Performance Evaluation of Quantization**: \\n   - Similar to the optimization evaluation, this phase assesses the impact of quantization on model performance. Both workflows conduct these comparisons to ensure that quantization does not negatively affect the model's inference capabilities.\\n\\n6. **CNNX-to-SNC Conversion**: \\n   - Both workflows have a final conversion step to SNC format. The effectiveness of this step is essential for deploying the optimized and quantized model on target hardware. \\n\\nIn summary, the LVM approach provides a more comprehensive strategy with additional steps in optimization and quantization methodologies, suggesting potentially higher effectiveness and efficiency compared to the CV workflow. The performance evaluations in both models serve as a critical component to ensure that each step contributes positively to the overall performance of the model.\", 'multi_responses': None, 'reference': 'The various model conversion and optimization techniques, including ONNX-to-CNNX conversion, quantization methods, and performance evaluation strategies, are compared in their effectiveness and efficiency through a structured process that involves converting models, optimizing them, and evaluating performance before and after optimization.', 'rubric': None}, {'user_input': 'How do the various performance evaluation techniques and optimization methods compare across different reports for model optimization, including aspects such as quantization, mixed precision, and the use of specific tools like the Quantizer, Optimizer, and performance estimators?', 'retrieved_contexts': ['Section: # Quantizer\\nContent:\\n## Advanced Quantization Methods\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error\\n- Dummy Convolution for the Concat Operator\\n\\t- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.\\n## Debug API\\n### Layer-wise mixed precision quantiztion debug API\\nThe quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary.', 'Section: Quantizer\\n# Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4', 'Section: # Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision.\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4', 'Section: Quantizer\\n# Quantizer\\n## Advanced Quantization Methods\\nContent:\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error', 'Section: # Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: Optimizer\\n# Optimizer\\nContent:\\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\\n- shape_inference<br>\\n\\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\\n- 4dim conversion<br>\\n\\tIn order for the .nnc extension model to operate on a device, the input and output shapes of all operators in the model must be in four dimensions.', 'Section: # Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: # Quantizer\\nContent:\\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API.', 'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`'], 'reference_contexts': ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.', 'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.', 'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.', 'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.', 'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.', 'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.', 'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.', 'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.', 'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.', 'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.', 'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.', 'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\", 'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.', 'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.', 'The configuration values are utilized across different modules of elt.', 'The model needs to be prepared in ONNX format for optimization.', \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\", 'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.', 'EHT supports ONNX opset versions 13 to 17.', 'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.', 'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.', \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\", 'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.', \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\", 'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.', \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\", 'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.', 'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.', 'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.', 'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.', 'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.', 'This document provides guidelines for preparing the input dataset.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\", 'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'], 'response': \"The performance evaluation techniques and optimization methods in model optimization involve several steps and tools, including the Quantizer and Optimizer modules, each focusing on areas such as quantization and mixed precision of models. Here's a comparison of the methodologies as highlighted in the provided context:\\n\\n### Performance Evaluation Techniques\\n1. **Performance Evaluation of Optimization**:\\n   - This is done before and after applying optimizations to compare inference results.\\n   - It assesses the impact of optimization on the model's performance.\\n\\n2. **Performance Evaluation of Quantization**:\\n   - Similar to the optimization evaluation, this step compares inference results before and after quantization.\\n   - It critically checks how quantization affects the model's accuracy and performance.\\n\\n### Optimization Methods\\n1. **Quantizer Module**:\\n   - Offers basic methods such as Fixed Precision Quantization and Mixed Precision Quantization (MPQ).\\n     - **Fixed Precision Quantization** involves static uniform quantization for the entire model.\\n     - **Mixed Precision Quantization** allows varying precisions for different parts of the model, either by activation names or operators. \\n   - Advanced quantization methods like Smooth Quant and Softmax Bias Correction are also available to improve performance post-quantization.\\n\\n2. **Optimizer Module**:\\n   - Aims to enhance models for performance on Exynos chips, allowing modifications such as shape inference and ensuring inputs/outputs conform to a four-dimensional shape required for operation.\\n   - It supports optimization templates that help in fine-tuning the model.\\n\\n### Comparison Across Reports\\n- **Quantization**:\\n  - The reports suggest applying Fixed Precision Quantization followed by evaluations, while the LVM flow involves advanced methods like Smooth Quant, Mixed Precision Quantization, and Softmax Bias Correction sequentially. This layered approach may yield better performance or resilience to quantization errors.\\n\\n- **Mixed Precision**:\\n  - Employed more flexibly in different contexts, the Quantizer allows for precise control over which activations or operators should use differing bit-widths, which may help reduce the model's overall size and improve speed without significantly sacrificing accuracy.\\n\\n- **Tools Used**:\\n  - The **Quantizer** focuses on quantization strategies and provides APIs for mixed precision debugging, which enhances model adaptability. On the other hand, the **Optimizer** deals with ensuring model compatibility with hardware and aligns the architecture to benefit from Exynos chip features. Both play pivotal roles but target different aspects of model optimization.\\n\\nIn summary, the choice and combination of optimization techniques across\", 'multi_responses': None, 'reference': 'The document compares various performance evaluation techniques and optimization methods for model optimization, including quantization, mixed precision, and specific tools like the Quantizer and Optimizer. It outlines steps for converting and optimizing models, applying quantization methods, and evaluating performance before and after optimization.', 'rubric': None}, {'user_input': 'How do the various optimization techniques and quantization methods for neural networks compare in terms of performance evaluation and model efficiency across different reports?', 'retrieved_contexts': ['Section: # Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: # Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: # Quantizer\\nContent:\\n## Advanced Quantization Methods\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error\\n- Dummy Convolution for the Concat Operator\\n\\t- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.\\n## Debug API\\n### Layer-wise mixed precision quantiztion debug API\\nThe quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary.', 'Section: # Model Optimization Flow\\n## LLM\\nContent:\\n1. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **CNNX-to SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: Quantizer\\n# Quantizer\\n## Advanced Quantization Methods\\nContent:\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error', 'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## LLM\\nContent:\\n1. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **CNNX-to SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: Optimizer\\n# Optimizer\\nContent:\\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\\n- shape_inference<br>\\n\\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\\n- 4dim conversion<br>\\n\\tIn order for the .nnc extension model to operate on a device, the input and output shapes of all operators in the model must be in four dimensions.', 'Section: # Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision.\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4'], 'reference_contexts': ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.', 'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.', 'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.', 'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.', 'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.', 'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.', 'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.', 'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.', 'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.', 'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.', 'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.', 'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\", 'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.', 'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.', 'The configuration values are utilized across different modules of elt.', 'The model needs to be prepared in ONNX format for optimization.', \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\", 'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.', 'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.', 'EHT supports ONNX opset versions 13 to 17.', 'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.', 'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.', \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\", 'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.', \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\", 'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.', \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\", 'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.', 'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.', 'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.', 'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.', 'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.', 'This document provides guidelines for preparing the input dataset.', \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\", 'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'], 'response': 'We do not have the information you requested. If you wish to contact support about this inquiry, please send an email to seed.ai@samsung.com.', 'multi_responses': None, 'reference': 'The various optimization techniques and quantization methods for neural networks are compared in terms of performance evaluation and model efficiency through a structured process that includes converting models, optimizing them, and evaluating performance before and after optimization.', 'rubric': None}, {'user_input': 'What parameters need to be set to enable SSD detection in the perf_estimator configuration?', 'retrieved_contexts': ['Section: # **How to use**\\n## **Detailed explanation for elt yaml file**\\nContent:\\nperf_estimator : dict**\\n- **O2_enable : bool** : O2 optimization (true or false)\\n- **O2_fm_forwarding : bool** : feature-map forwarding (true or false)\\n- **SEG : bool** : Set true if input model is Deeplab V3+\\n- **SSD : bool** : Set true if input model is SSD detection\\n- **bit_width_factor_FM : int** : Select feature map bit width factor (1 or 2)\\n- **bit_width_factor_FP16 : bool** : Set bit width factor as floating point (true or false)\\n- **bit_width_factor_weight : int** : Select weight bit width factor (1 or 2)\\n- **core_num : int** : 1 for single core, 2 for instance-1\\n- **device : str** : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a]\\n- **json_report : bool** : Enable report json format', 'Section: # **How to use**\\n## **Detailed explanation for elt yaml file**\\nContent:\\n- **nq_fold : bool** : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32)\\n\\n**profiler : dict**\\n- **iter : int** : This decides how many time the model inference will be processed.\\n- **mode : str** : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost]\\n- **target : str** : profiling target. [model, system]', 'Section: ENN Framwork API Functions\\n# ENN Framwork API Functions\\n## Security, preference, get meta information..\\n### Functions Documentation\\nContent:\\n#### function EnnSetPreferencePresetId\\n```cpp\\nEnnReturn EnnSetPreferencePresetId(\\n\\tconst uint32_t val\\n)\\n```\\nSetting Preset ID for operation performance.\\n**Parameters**:\\n- **val** [IN] value to set preset ID\\n**Return**: EnnReturn result, 0 is success\\n#### function EnnSetPreferencePerfConfigId\\n```cpp\\nEnnReturn EnnSetPreferencePerfConfigId(\\n\\tconst uint32_t val\\n)\\n```\\nSetting PerfConfig ID for operation performance.\\n**Parameters**:\\n- **val** [IN] value to set PerfConfig ID\\n**Return**: EnnReturn result, 0 is success\\n#### function EnnSetPreferencePerfMode\\n```cpp\\nEnnReturn EnnSetPreferencePerfMode(\\n\\tconst uint32_t val\\n)\\n```\\nSetting Performance Mode.\\n**Parameters**:\\n- **val** [IN] value to set Performance Mode\\n**Return**: EnnReturn result, 0 is success', 'Section: # **How to use**\\n## **Detailed explanation for elt yaml file**\\nContent:\\n- **assign_gpu : str** : Assign specific layer to gpu device\\n- **best_fit_generalized : bool** : Control whether generalized best fit allocation is to be used.\\n- **cast_in : str** : Type casting fp32 to fp16 for nnc input data\\n- **cast_out : str** : Type casting fp16 to fp32 for nnc output data\\n- **cfs : bool** : Enable cfifo sync\\n- **compiler : str** : Compiler option\\n- **datalayout_conversion_in : str** : Data layout(NHWC) conversion for nnc input data\\n- **datalayout_conversion_out : str** : Data layout(NHWC) conversion for nnc output data\\n- **debug_str : str** : debug str for compiler\\n- **dequant_type : str** : dequantiztion type\\n- **device : str** : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7]', 'Section: # **How to use**\\n## **Detailed explanation for elt yaml file**\\nContent:\\n- **test_type : str** : ENN running mode [lib, service]\\n- **tv_threshold : float** : The value is used for tolerance threshold of output match verification.\\n- **bitmatch_test : bool** : if set true, visual profiler will compile nnc first\\n- **core_num : str** : The number of NPU core. [single, multiple]\\n- **device : str** : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7]\\n- **device_id : str** : id of the device connected to the server or PC running the enntools docker\\n- **remote_ssh_config_path : str** : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml)\\n- **ssh_bool : str** : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE]', 'Section: # **How to use**\\n## **Detailed explanation for elt yaml file**\\nContent:\\n- **data_format : str** : [channel_first, channel_last]\\n- **debug : bool** : dump layerwise sqnr between new snc and hw quantized snc.\\n- **gpu_enable : bool** : enable infer model on gpu if gpu is available\\n- **gpu_id : int** : gpu id for quantization profiling\\n- **input_dtype : str** : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model).\\n- **mode : str** : [elt, eht_cnnx, eht_snc]\\n- **output_dtype : str** : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none]\\n- **profile_batchsize : int** : Batchsize for profile (value 100 is recommened).', 'Section: # **How to use**\\n## **Detailed explanation for elt yaml file**\\nContent:\\nmodel_analyzer : dict**\\n- **check : bool** : Check the op support status\\n- **device : str** : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6]\\n- **level : int** : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint)\\n- **snc_input : bool** : analyze the snc model, false: analyze the original model\\n\\n**database_gen : dict**\\n- **database_spec : str** : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml)\\n\\n**converter : dict**\\n- **device : str** : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7]', \"Content:\\n# **Dataset preparation**\\nThis is a guideline for preparing the input dataset.\\n## **Dataset format**\\nThe dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur.\\nAdditionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format.\\nThe h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace.\\n# Model Requirements and Constraints\\n## model format\\n- Model should be prepared in ONNX format to start optimization.\\n## opset version\\n- EHT currently support ONNX opset version 13 ~ 17.\", 'Question: Please tell us about the purpose of EDS Assistant.\\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.', 'Section: # **How to use**\\n## **Detailed explanation for eht yaml file**\\nContent:\\n- **metric : string**: The metric to be used for measurement\\n- **threshold : float** : The threshold value of the metric that determines agreement / disagreement\\n- **input_data_path : string**: The path to the dataset for model inference\\n\\n**optimizer : dict**\\n- **skip_4_dim_conversion : bool** : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion.\\n- **overwrite_input_shapes : dict** : Enter the input shape for models with undefined input shapes.\\n- **custom_template_path : dict**: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value.'], 'reference_contexts': ['perf_estimator : dict**\\n- **O2_enable : bool**: O2 optimization (true or false)\\n- **O2_fm_forwarding : bool**: feature-map forwarding (true or false)\\n- **SEG : bool**: Set true if input model is Deeplab V3+\\n- **SSD : bool**: Set true if input model is SSD detection\\n- **bit_width_factor_FM : int**: Select feature map bit width factor (1 or 2)\\n- **bit_width_factor_FP16 : bool**: Set bit width factor as floating point (true or false)\\n- **bit_width_factor_weight : int**: Select weight bit width factor (1 or 2)\\n- **core_num : int**: 1 for single core, 2 for instance-1\\n- **device : str**: Select device type [Gen-3, Gen-4, Gen-5, Gen-5a]\\n- **json_report : bool**: Enable report json format\\n- **nq_fold : bool**: Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32)  \\n---  \\n**'], 'response': 'To enable SSD detection in the `perf_estimator` configuration, you need to set the following parameter to true:\\n\\n- **SEG**: Set this to `false` (since it is specific to Deeplab V3+), but the SSD detection needs to have the **SSD** parameter set to true.\\n\\nHere’s a summary of the relevant parameters:\\n\\n```yaml\\nperf_estimator:\\n  SSD: true  # Enable SSD detection\\n  SEG: false # This is not applicable for SSD\\n``` \\n\\nMake sure to include any other relevant parameters as per your requirements.', 'multi_responses': None, 'reference': 'To enable SSD detection in the perf_estimator configuration, the parameter SSD needs to be set to true.', 'rubric': None}, {'user_input': 'What steps are involved in applying user-defined optimization methods using the Optimizer Template?', 'retrieved_contexts': ['Section: # Optimizer\\n## How to Create Custom Templates\\nContent:\\nOptimizer is a tool that allows users to easily optimize their models. Users can add their own optimization methods using the \"Optimizer Template\" of Optimizer. The following process is required to apply the user-defined template to the user model.\\n- Create custom templates.\\n- Prepare model to be optimized\\n- Validate optimized model', 'Section: Optimizer\\n# Optimizer\\n## How to Create Custom Templates\\nContent:\\nOptimizer is a tool that allows users to easily optimize their models. Users can add their own optimization methods using the \"Optimizer Template\" of Optimizer. The following process is required to apply the user-defined template to the user model.\\n- Create custom templates.\\n- Prepare model to be optimized\\n- Validate optimized model', 'Section: # Optimizer\\n## How to Create Custom Templates\\nContent:\\n### Optimizer Template\\nThe Optimizer template provides a scenario of how the model should be optimized. Its components are shown in the figure below.\\n\\nOptimization Template Component\\n- Optimize Name: Name of the optimization you want to apply\\n- Trigger condition : Trigger condition for starting application of the Optimization technique in graph search (typically, the trigger condition is the operator type)\\n- Origin Module: Module before optimization\\n- Origin condition: Check that the Origin Module is candidate to this optimization technique (applied if trigger condition is triggered)\\n- Optimized Module: Module after optmization\\n- Optimization: Modification condition of Optimized applied model (optional)\\n### Flow\\nThe Optimizer Templates are executed as shown in the flow chart below. In searching for the model, the node meets the trigger_op method of the template for the first time. Then it loops through the origin_condition of the case. If the condition is correct, it optimizes the module through steps.', 'Section: Optimizer\\n# Optimizer\\n## How to Create Custom Templates\\nContent:\\n### Optimizer Template\\nThe Optimizer template provides a scenario of how the model should be optimized. Its components are shown in the figure below.\\n\\nOptimization Template Component\\n- Optimize Name: Name of the optimization you want to apply\\n- Trigger condition : Trigger condition for starting application of the Optimization technique in graph search (typically, the trigger condition is the operator type)\\n- Origin Module: Module before optimization\\n- Origin condition: Check that the Origin Module is candidate to this optimization technique (applied if trigger condition is triggered)\\n- Optimized Module: Module after optmization\\n- Optimization: Modification condition of Optimized applied model (optional)\\n### Flow\\nThe Optimizer Templates are executed as shown in the flow chart below. In searching for the model, the node meets the trigger_op method of the template for the first time. Then it loops through the origin_condition of the case. If the condition is correct, it optimizes the module through steps.', 'Section: Optimizer\\n# Optimizer\\n## How to Create Custom Templates\\n### Create custom templates\\nContent:\\nInherit the above classes and declare each class corresponding to Step, Case, and Template. Import package classes of model_editor for updating the model. If you need more information about model_editor, please see the guide about model_editor.\\nWe **activate the trigger_op method** when the node is one of the four basic arithmetic operators and the input feature map has 4 dimension. In the **origin_condition method of the case class**, more detailled conditions are checked: the previous node of the arithmetic operator must be a convolution layer, the arithmetic operator must be the only node following the convolution node, and it must not be folded yet.', \"Section: # Optimizer\\n## How to Create Custom Templates\\n### Create custom templates\\nContent:\\nclass OptimizerTemplateInternal(OptimizerTemplate):\\n\\tdef __repr__(self):\\n\\t\\treturn '{}(name={})'.format(\\n\\t\\t\\tself.__class__.__name__, self.name)\\n\\t@property\\n\\tdef name(self) -> AnyStr:\\n\\t\\treturn self._name\\n\\t@name.setter\\n\\tdef name(self, value: AnyStr):\\n\\t\\tself._name = value\\n\\t@property\\n\\tdef case_list(self) -> List[TemplateCaseInternal]:\\n\\t\\treturn self._case_list\\n\\t@case_list.setter\\n\\tdef case_list(self, value: List[TemplateCaseInternal]):\\n\\t\\tself._case_list = value\\n\\tdef trigger_op(self, node: CnnxNode) -> bool:\\n\\t\\traise NotImplementedError\\n\\tdef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\\n\\t\\tif self.trigger_op(node):\\n\\t\\t\\tfor case in self.case_list:\\n\\t\\t\\t\\tif case.launch(model, node):\\n\\t\\t\\t\\t\\treturn True\\n\\t\\treturn False\\n```\\nInherit the above classes and declare each class corresponding to Step, Case, and Template. Import package classes of model_editor for updating the model. If you need more information about model_editor, please see the guide about model_editor.\", 'Section: Optimizer\\n# Optimizer\\n## How to Create Custom Templates\\n### How to apply a custom template\\nContent:\\nWrite the path of the custom template and the class name of the template. Following the above example, it can be written as follows:\\n```yaml\\ninput_model_path: /ssd2/shared_data/models/kpi/denoise_model_1.5.424_1x2304x1296x3/denoise_model_1.5.424_1x2304x1296x3.onnx\\noutput_folder_path: /home/results_denoise_1.5___\\ninput_model_format: onnx\\nmodel_type: kpi_cv\\nquantizer:\\n\\t~~~~~\\nsimulator:\\n\\t~~~~~~\\noptimizer:\\n\\t~~~~~~\\n\\tcustom_template_path:\\n\\t/usr/local/lib/python3.10/dist-packages/optimizer/core/templates/fuse_deconv_bias.py:\\n\\t- TemplateFuseDeconvBias\\n```', \"Section: Optimizer\\n# Optimizer\\n## How to Create Custom Templates\\n### Create custom templates\\nContent:\\nclass OptimizerTemplateInternal(OptimizerTemplate):\\n\\tdef __repr__(self):\\n\\t\\treturn '{}(name={})'.format(\\n\\t\\t\\tself.__class__.__name__, self.name)\\n\\t@property\\n\\tdef name(self) -> AnyStr:\\n\\t\\treturn self._name\\n\\t@name.setter\\n\\tdef name(self, value: AnyStr):\\n\\t\\tself._name = value\\n\\t@property\\n\\tdef case_list(self) -> List[TemplateCaseInternal]:\\n\\t\\treturn self._case_list\\n\\t@case_list.setter\\n\\tdef case_list(self, value: List[TemplateCaseInternal]):\\n\\t\\tself._case_list = value\\n\\tdef trigger_op(self, node: CnnxNode) -> bool:\\n\\t\\traise NotImplementedError\\n\\tdef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\\n\\t\\tif self.trigger_op(node):\\n\\t\\t\\tfor case in self.case_list:\\n\\t\\t\\t\\tif case.launch(model, node):\\n\\t\\t\\t\\t\\treturn True\\n\\t\\treturn False\\n```\", 'Section: Optimizer\\n# Optimizer\\n## How to Create Custom Templates\\n### Create custom templates\\nContent:\\n\\t@property\\n\\tdef name(self) -> AnyStr:\\n\\t\\treturn self._name\\n\\t@name.setter\\n\\tdef name(self, value: AnyStr):\\n\\t\\tself._name = value\\n\\t@property\\n\\tdef origin(self) -> Dict:\\n\\t\\treturn self._origin\\n\\t@origin.setter\\n\\tdef origin(self, value: Dict):\\n\\t\\tself._originn = value\\n\\t@property\\n\\tdef step_list(self) -> Dict:\\n\\t\\treturn self._step_list\\n\\t@step_list.setter\\n\\tdef step_list(self, value: Dict):\\n\\t\\tself._step_list = value\\n\\tdef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\\n\\t\\traise NotImplementedError\\n\\tdef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\\n\\t\\tif self.origin_condition(model, node):\\n\\t\\t\\tfor st in self.step_list:\\n\\t\\t\\t\\tif not st.optimization(model, node):\\n\\t\\t\\t\\t\\treturn False\\n\\t\\t\\treturn True', 'Section: Optimizer\\n# Optimizer\\n## How to Create Custom Templates\\n### Create custom templates\\nContent:\\nIn this part, we introduce how to write custom templates step by step. As an example, we use a template that fuses basic arithmetic operations layers into a convolution layer, when possible. We provide `TemplateStepInternal`, `TemplateCaseInternal`, `OptimizerTemplateInternal` to prevent abnormal behavior of the code.\\n```python\\nclass TemplateStepInternal(TemplateStep):\\n\\t@property\\n\\tdef name(self) -> AnyStr:\\n\\t\\treturn self._name\\n\\t@name.setter\\n\\tdef name(self, value: AnyStr):\\n\\t\\tself._name = value\\n\\t@property\\n\\tdef origin(self) -> Dict:\\n\\t\\treturn self._origin\\n\\t@origin.setter\\n\\tdef origin(self, value: Dict):\\n\\t\\tself._origin = value\\n\\tdef optimization(self, model: CnnxModel, node: CnnxNode) -> Union[Tuple[List[CnnxNode], List[CnnxNode], List[Port], List[Port]], Dict]:\\n\\t\\treturn NotImplementedError\\nclass TemplateCaseInternal(TemplateCase):'], 'reference_contexts': ['Optimizer is a tool that allows users to easily optimize their models. Users can add their own optimization methods using the \"Optimizer Template\" of Optimizer. The following process is required to apply the user-defined template to the user model.  \\n- Create custom templates.\\n- Prepare model to be optimized\\n- Validate optimized model'], 'response': 'To apply user-defined optimization methods using the Optimizer Template, the following steps are required:\\n\\n1. **Create custom templates**: Define the custom optimization scenarios using the provided classes and methods.\\n2. **Prepare the model to be optimized**: Ensure that your model is structured appropriately for optimization.\\n3. **Validate the optimized model**: After applying the optimization, check that the model functions correctly and that the optimization was successful.\\n\\nThese steps allow you to customize how the model is optimized by using the components outlined in the Optimizer Template.', 'multi_responses': None, 'reference': 'The steps involved in applying user-defined optimization methods using the Optimizer Template include creating custom templates, preparing the model to be optimized, and validating the optimized model.', 'rubric': None}, {'user_input': 'What is the significance of specifying the output model path in the conversion process of machine learning models?', 'retrieved_contexts': ['Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## LLM\\nContent:\\n1. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **CNNX-to SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: # Model Optimization Flow\\n## LLM\\nContent:\\n1. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **CNNX-to SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: Changing the Model\\n# Changing the Model in an Android Sample Application\\n## Example: Switching to MobileNet V3\\n### Updating the Android Project\\nContent:\\n\\t\\tval OUTPUT_DATA_TYPE = DataType.FLOAT32\\n\\t\\tconst val OUTPUT_CONVERSION_SCALE = 1F\\n\\t\\tconst val OUTPUT_CONVERSION_OFFSET = 0F\\n\\t\\tconst val LABEL_FILE = \"labels1001.txt\"\\n\\t}\\n\\t```\\n3. Adjust the `postProcess()` function for the output of the new model, which is a logits-vector instead of $[0,1]$ scores:\\n\\t```kotlin\\n\\tprivate fun postProcess(modelOutput: ByteArray): Map<String, Float> {\\n\\t\\tval output = when (OUTPUT_DATA_TYPE) {\\n\\t\\t\\tDataType.FLOAT32 -> {\\n\\t\\t\\t\\tval byteBuffer = ByteBuffer.wrap(modelOutput).order(ByteOrder.nativeOrder())', 'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\nContent:\\nThis section details the optimization flow according to model type. Currently, optimization scenarios are predetermined for specific model types, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). We plan to update the features to allow flexible scheduling of optimization functions.', 'Section: # Model Optimization Flow\\nContent:\\nThis section details the optimization flow according to model type. Currently, optimization scenarios are predetermined for specific model types, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). We plan to update the features to allow flexible scheduling of optimization functions.', 'Section: # Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: # Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`', 'Section: # Converter\\nContent:\\n- ONNX2CNNX Converter\\n\\t- Converts an ONNX model to CNNX.\\n\\t- Requires specification of both the output model path and the output encodings path.\\n- CNNX2SNC Converter\\n\\t- Converts a CNNX model to SNC.\\nExample codes to use them:\\n```python\\nfrom converter import api\\ncnnx_to_snc_params = api.Cnnx2SncParameters(\\n\\tinput_model_path = \"/path/to/model.onnx\",\\n\\tinput_encodings_path = \"/path/to/model.encodings\",\\n\\toutput_model_path = \"/output/path/for/model.snc\"\\n\\t)\\napi.Converter.cnnx_to_snc(cnnx_to_snc_params)\\ntflite_to_onnx_params = api.TfLite2OnnxParameters(\\n\\tinput_model_path = \"/path/to/model.tflite\",\\n\\toutput_model_path = \"./output/path/for/model.onnx\",\\n\\t)\\napi.Converter.tflite_to_onnx(tflite_to_onnx_params)\\n```'], 'reference_contexts': ['Example codes to use them:\\n```python\\nfrom converter import api\\ncnnx_to_snc_params = api.Cnnx2SncParameters(\\ninput_model_path = \"/path/to/model.onnx\",\\ninput_encodings_path = \"/path/to/model.encodings\",\\noutput_model_path = \"/output/path/for/model.snc\"\\n)\\n\\napi.Converter.cnnx_to_snc(cnnx_to_snc_params)\\n\\ntflite_to_onnx_params = api.TfLite2OnnxParameters(\\ninput_model_path = \"/path/to/model.tflite\",\\noutput_model_path = \"./output/path/for/model.onnx\",\\n)\\n\\napi.Converter.tflite_to_onnx(tflite_to_onnx_params)\\n```'], 'response': 'Specifying the output model path in the conversion process of machine learning models is significant for several reasons:\\n\\n1. **Target Location**: It defines where the converted model will be saved. This is crucial for organizing model files and ensuring that they can be easily located and accessed later.\\n\\n2. **File Management**: It helps manage different versions or formats of models. By specifying different output paths for various conversions (e.g., from ONNX to CNNX or from CNNX to SNC), developers can keep track of their models and avoid overwriting existing files.\\n\\n3. **Integration with Workflows**: A designated output path allows for better integration with workflows and pipelines where subsequent processes may rely on the converted model files. Clear output paths enable easier automation and orchestration of tasks.\\n\\n4. **Error Handling**: Specifying the output path can facilitate debugging and troubleshooting. If a conversion fails, developers can quickly check the specified output location for any incomplete or corrupted files.\\n\\n5. **Documentation and Clarity**: It serves as documentation for the transformation taking place, clearly indicating the destination and format of the output model, which aids in understanding the flow of model processing.\\n\\nOverall, the output model path is a pivotal aspect of the model conversion process, ensuring efficient file management and facilitating later stages of model deployment and execution.', 'multi_responses': None, 'reference': 'Specifying the output model path in the conversion process of machine learning models is significant because it determines where the converted model will be saved, ensuring that the user can access and utilize the model after the conversion is complete.', 'rubric': None}, {'user_input': \"What is the purpose of the 'simulator' in the context of the provided dictionary?\", 'retrieved_contexts': ['Section: Simulator\\n# Simulator\\nContent:\\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\\nSimulator includes the following features:\\n- `get_quantization_sim`: Attain a CNNX quantization simulation session for manual run.\\n- `load_input_data`: Load the input data from the given dataset path.\\n- `run_inference`: Conduct inference on a CNNX model.\\n- `compare_model_by_inference`: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric.\\n- `compare_model_by_layer` (TBD): Compare two CNNX models on intermediate tensors for each layer. To be released.', 'Section: # Simulator\\nContent:\\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\\nSimulator includes the following features(with example codes):', \"Section: Backend API\\n# **Backend API**\\n## **Simulator**\\n### `run_inference`\\nContent:\\nInferenceRunnerParameters schema details**\\n**input_model_path : string**\\nSpecifies the file path of the input model. It should point to a file, not a parent folder.\\n\\n**input_encondings_path : string**\\nIndicates the file path for the encodings file of the cnnx model.\\n\\n**input_data_path : string**\\nSpecifies the path to the folder containing data required for inference.\\n\\n**use_cuda : bool**\\nDetermines the inference method: when true, uses GPU for inference; when false, performs inference using CPU only.\\n\\n**layerwise_check_name : list**\\nAccepts input values to retrieve intermediate featuremap information along with the output during inference. When featuremap names are provided, the inference runner's return value includes the values of those featuremaps appended.\\n\\n**Example of run_inference API**\\n```python\\nfrom simulator.schema.base import InferenceRunnerParameters\\nfrom simulator.api import Simulator\\nparams = InferenceRunnerParameters(\\n\\tinput_model_path={input_model_path}\\n\\tinput_encodings_path={input_encodings_path},\\n\\tinput_data_path={input_data_path},\\n\\t)\\nres = Simulator.run_inference(params)\\n```\", 'Section: Backend API\\n# **Backend API**\\n## **Simulator**\\n### `compare_model_by_inference`\\nContent:\\nDetermines the inference method: when true, uses GPU for inference; when false, performs inference using CPU only.\\n\\n**Example of compare_model_by_inference API**\\n```python\\nfrom simulator.schema.base import InferenceCheckerParameters\\nfrom simulator.api import Simulator\\nparams = InferenceCheckerParameters(\\n\\t\\t\\tinput_data_path = \"/path/to/data/dir/\",\\n\\t\\t\\tinput_model_path_0 = \"/path/to/model_0.onnx\",\\n\\t\\t\\tinput_encodings_path_0 = \"/path/to/model_0.encodings\",\\n\\t\\t\\tinput_model_path_1 = \"/path/to/model_1.onnx\",\\n\\t\\t\\tinput_encodings_path_1 = \"/path/to/model_1.encodings\",\\n\\t\\t\\tmetric = \"snr\",\\n\\t\\t\\tthreshold = 30,\\n\\t\\t\\tuse_cuda = True\\n\\t\\t)\\nres = Simulator.compare_model_by_inference(params)\\n```', 'Section: Backend API\\n# **Backend API**\\n## **Simulator**\\n### `get_inference_session`\\nContent:\\nGetInferenceSessionParameters schema details**\\n**input_model_path : string**\\nSpecifies the file path of the input model. It should point to a file, not a parent folder.\\n\\n**input_encondings_path : string**\\nIndicates the file path for the encodings file of the cnnx model.\\n\\n**use_cuda : bool**\\nDetermines the inference method: when true, uses GPU for inference; when false, performs inference using CPU only.\\n\\n**Example of run_inference API**```python\\nfrom simulator.schema.base import InferenceSessionParameters\\nfrom simulator.api import Simulator\\nparams = InferenceSessionParameters(\\n\\tinput_model_path={input_model_path}\\n\\tinput_encodings_path={input_encodings_path},\\n\\tuse_cuda={input_data_path},\\n\\t)\\nres = Simulator.get_inference_session(params)\\n```', 'Section: # Simulator\\nContent:\\n- `get_quantization_sim`: Attain a CNNX quantization simulation session for manual run.\\n\\t```python\\n\\tfrom simulator imoprt api\\n\\toutput_names = [LIST_OF_ONNX_OUTPUTS]\\n\\tinput_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1]\\n\\tquantsim = api.get_quantization_sim(params)\\n\\tinference_session = quantsim.session\\n\\tresult = session.run(output_names, input_dict)\\n\\tprint(result)\\n\\t```\\n- `load_input_data`: Load the input data from the given dataset path.\\n- `run_inference`: Conduct inference on a CNNX model.\\n- `compare_model_by_inference`: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric.', 'Section: Backend API\\n# **Backend API**\\n## **Simulator**\\n### `compare_model_by_inference`\\nContent:\\nInferenceCheckerParameters object details**\\n**input_model_path0 : string**\\nThe onnx model file path to be compared.\\n\\n**input_encondings_path_0 : string**\\nThe model encodings file path for the onnx model 0.\\n\\n**input_model_path_1 : string**\\nThe onnx model file path to be compared.\\n\\n**input_encondings_path_1 : string**\\nThe model encodings file path for the onnx model 1.\\n\\n**input_data_path : string**\\nSpecifies the path to the folder containing data required for inference.\\n\\n**meric : string**\\nMetric in which the error is calculated (“snr”, “rmse”, …)\\n\\n**threshold : float**\\nThe threshold by which two models are considered as equivalent.\\n\\n**use_cuda : bool', 'Section: # **How to use**\\n## **Detailed explanation for eht yaml file**\\nContent:\\n- **calibration_data_path : string** : The path to the representative data to be used for calibration\\n- **calibration_args : dict :** Arguments for processing calibration\\n\\t- **samples : int** : How many calibration data samples to use\\n\\t- **seed : int** : A value set as a seed for random selection\\n- **add_dummy_conv: bool** : Whether apply the dummy conv algorithm\\n- **input_dtype: dict** : Input data type for the quantized model\\n- **output_dtype: dict** : Output data type for the quantized model\\n\\n**simulator : dict', 'Section: Dataset preparation\\n# **Dataset preparation**\\nContent:\\nThis is a guideline for preparing the input dataset for the simulator and calibration dataset for the quantizer module.\\n## **Detailed instruction for dataset preparation**\\nAnalyze model inputs using Netron. The example model receives three inputs: encoder_hidden_states, sample, and timestep.\\n\\nCreate a JSON file named \\'input_folder_mapping.json\\'. In this file, define key-value pairs where each key represents a model input name and its corresponding value is the name of the folder containing the input data for that model input.\\n```json\\n{\\n\\t\"encoder_hidden_states\": \"input_encoder_hidden_states\",\\n\\t\"sample\": \"sample\",\\n\\t\"timestep\": \"timestep\"\\n}\\n```\\nAfter preparing the dataset and JSON file, create a dataset directory and place it in the specified location.\\n\\nIt\\'s important to note that the file names for each input should be identical, as shown in the example above.\\n## Dataset Format Support\\nCurrently, the supported data type is npy files consisting of float type. Support for other data formats is planned for the future.| Data Format | Support |\\n|--------------------------------------------------------|---------|\\n| numpy array(.npy, float) | O |\\n| numpy array(.npy, int) | TBD |\\n| binary data(.bin) | TBD |\\n| image(.jpg) | TBD |', 'Section: # Simulator\\nContent:\\n- `compare_model_by_layer` : Compare two CNNX models on intermediate tensors for each layer.\\n\\t```python\\n\\tfrom simulator imoprt api\\n\\t\\tparams = api.LayerwiseCheckerParameters(\\n\\t\\t\\tinput_data_path = \"/path/to/input/data/,\\n\\t\\t\\tinput_model_path_0 = \"/path/to/model/0.onnx\",\\n\\t\\t\\tinput_encodings_path_0 = \"/path/to/model/0.encodings\",\\n\\t\\t\\tinput_model_path_1 = \"/path/to/model/0.onnx\",\\n\\t\\t\\tinput_encodings_path_1 = \"/path/to/model/0.encodings\",\\n\\t\\t\\tmetric = \"snr\",\\n\\t\\t\\tthreshold = 100,\\n\\t\\t\\tuse_cuda = False,\\n\\t\\t\\tlayer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")]\\n\\t\\t\\texport_featuremap = True,\\n\\t\\t\\texport_featuremap_path = \"/path/to/save/exported/featuremap\"\\n\\t\\t)\\n\\t\\tres = api.compare_model_by_layer(params)\\n\\t\\tprint(\"Layerwise check\")\\n\\t\\tfor input_name, result in res.items():\\n\\t\\t\\t_, result_dict = result\\n\\t\\t\\tfor output_name, snr_value in result_dict.items():\\n\\t\\t\\t\\tprint(f\"{output_name}: {snr_value}\")\\n\\t```'], 'reference_contexts': ['simulator : dict**  \\n- **metric : string**: The metric to be used for measurement\\n- **threshold : float** : The threshold value of the metric that determines agreement / disagreement\\n- **input_data_path : string**: The path to the dataset for model inference  \\n---  \\n**'], 'response': \"The 'simulator' in the context of the provided dictionary refers to a component that conducts inference on a CNNX model, which includes an ONNX model file and a quantization information encodings file. It allows for checking output and intermediate tensors during execution and simulates quantized inference through a process known as simulated quantization. \\n\\nKey features of the simulator include:\\n- Conducting inference on CNNX models.\\n- Loading input data from specified datasets.\\n- Comparing the inference outputs of two CNNX models based on a defined metric (currently SNR).\\n- Attaining quantization simulation sessions for manual runs.\\n\\nOverall, the simulator aids in validating model behavior and performance under quantized conditions.\", 'multi_responses': None, 'reference': \"The purpose of the 'simulator' in the provided dictionary is to define a structure that includes a metric for measurement, a threshold value for determining agreement or disagreement, and a path to the dataset for model inference.\", 'rubric': None}, {'user_input': 'What is the purpose of Softmax Bias Correction in advanced quantization methods?', 'retrieved_contexts': ['Section: Quantizer\\n# Quantizer\\n## Advanced Quantization Methods\\nContent:\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error', 'Section: # Quantizer\\nContent:\\n## Advanced Quantization Methods\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error\\n- Dummy Convolution for the Concat Operator\\n\\t- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.\\n## Debug API\\n### Layer-wise mixed precision quantiztion debug API\\nThe quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary.', 'Section: Quantizer\\n# Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4', 'Section: # Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision.\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4', 'Section: # Quantizer\\nContent:\\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API.', 'Section: Quantizer\\n# Quantizer\\nContent:\\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization. Users can apply mixed precision quantization to models by specifying activation names or operators.', 'Section: # Simulator\\nContent:\\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\\nSimulator includes the following features(with example codes):', 'Section: # Optimizer\\nContent:\\nThe channel axes can be determined by certain layers, such as Convolutional and Linear layers, which have fixed channel axes at positions 1 and -1, respectively. However, they can be modified by operators like MatMul and Reshape, which can expand or reduce them. To account for these changes, the conversion process tracks channel axes in both the forward and backward pass. As a result, the 4D conversion process may require additional time to complete.\\n- support optimization method.<br>\\n\\tOptimizer supports many optimization features so that the model works efficiently on the device.\\n\\t- Fold\\n\\t\\t- GeGLU\\n\\t\\t- GeLU\\n\\t\\t- GroupNorm\\n\\t\\t- LayerNorm\\n\\t\\t- PReLU\\n\\t\\t- RMSNorm\\n\\t\\t- SiLU', 'Section: # **How to use**\\n## **Detailed explanation for elt yaml file**\\nContent:\\n- **do_quantize : bool** : Enable quantization\\n- **eltsum_cvt : bool** : enable eltwise conversion process; eltsum -> concat + conv\\n- **graph_opt : str** : Graph Optimizer Model Type, Model Name for Graph Optimizer.\\n- **mean : str** : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN\\n- **onnx_simplify : bool** : enable onnx_simplify process\\n- **optimize : bool** : Use graph optimization\\n- **quantize_type : str** : Select quantization type, quantized model (include caffeQAT) is \"qat\"\\n- **scale : str** : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE\\n- **bw_ofm : int** : Bitwidth of intermediate feature map(A).', \"Section: # Optimizer\\nContent:\\n\\t- Fuse\\n\\t\\t- SiLU (to GroupNorm)\\n\\t\\t- BatchNorm into Convolution\\n\\t\\t- Cast\\n\\t\\t- Deconvolution bias\\n\\t\\t- Math\\n\\t\\t- multiple reshape and transpose in a row (when possible)\\n\\t\\t- multiple concat in a row\\n\\t- Insert\\n\\t\\t- Depthwise Convolution for activation\\n\\t- Remove\\n\\t\\t- unecesary slices\\n\\t- Replace\\n\\t\\t- Average Pooling to Depthwise convolution\\n\\t\\t- Eltwise concat convolution\\n\\t\\t- expand with concat (by concatenating the same input multiple times)\\n\\t\\t- Convolution kernel 1 to 3\\n\\t\\t- Matrix multiplication to dynamic convolution\\n\\t\\t- ReduceMean to Global Average Pool\\n\\t\\t- ReduceSum to Convolution\\n\\t\\t- Slice to Split\\n\\t\\t- Global Average Pool to 2 Average Pool\\n\\t- Change attribute\\n\\t\\t- axis of softmax\\nIn addition, Optimizer Template, one of the Optimizer's powerful features, supports custom optimization methods made by users.\"], 'reference_contexts': ['Currently supported advanced quantization methods are as follows.  \\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n- A method that tune the weights range of the channels in one tensor to reduce quantization error  \\n- Dummy Convolution for the Concat Operator\\n- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.'], 'response': 'The purpose of Softmax Bias Correction in advanced quantization methods is to add a float bias to the Softmax output layer in order to reduce the performance degradation that can be caused by quantization. This approach aims to improve the accuracy of models where the Softmax layer is a critical component, particularly in scenarios where quantization might otherwise introduce significant errors. You can find more details in the Quantizer section of the documentation.', 'multi_responses': None, 'reference': 'The purpose of Softmax Bias Correction in advanced quantization methods is to add a float bias to the Softmax output layer to reduce the performance degradation caused by quantization.', 'rubric': None}]}\n"
     ]
    }
   ],
   "source": [
    "from tsk_ragtools import text_gen as tg\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_jsonl('./evaluation_dataset.jsonl')\n",
    "evaluation_dataset_dict = evaluation_dataset.dict()\n",
    "\n",
    "for i, obj in enumerate(evaluation_dataset_dict[\"samples\"]):\n",
    "  print(\n",
    "    f\"\\rGenerating RAGAS json...({i+1}/{len(evaluation_dataset_dict['samples'])})\",\n",
    "    end=\"\",\n",
    "    flush=True,\n",
    "  )\n",
    "  response = tg.answer_question(obj[\"user_input\"])\n",
    "  obj.update({\"retrieved_contexts\": response[\"list\"], \"response\": response[\"response\"]})\n",
    "\n",
    "print(evaluation_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = EvaluationDataset.from_dict(mapping=evaluation_dataset_dict[\"samples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 30/30 [00:50<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas import evaluate\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "\n",
    "metrics = [LLMContextRecall(), FactualCorrectness(), Faithfulness()]\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics, llm=evaluator_llm,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_input': {0: 'comparison of optimization techniques performance evaluations mixed precision quantization static uniform quantization quantizers optimizers',\n",
       "  1: 'How do quantization methods compare on performance and efficiency?',\n",
       "  2: 'How do the various model conversion and optimization techniques, including ONNX-to-CNNX conversion, quantization methods, and performance evaluation strategies, compare in their effectiveness and efficiency as outlined in the provided summaries?',\n",
       "  3: 'How do the various performance evaluation techniques and optimization methods compare across different reports for model optimization, including aspects such as quantization, mixed precision, and the use of specific tools like the Quantizer, Optimizer, and performance estimators?',\n",
       "  4: 'How do the various optimization techniques and quantization methods for neural networks compare in terms of performance evaluation and model efficiency across different reports?',\n",
       "  5: 'What parameters need to be set to enable SSD detection in the perf_estimator configuration?',\n",
       "  6: 'What steps are involved in applying user-defined optimization methods using the Optimizer Template?',\n",
       "  7: 'What is the significance of specifying the output model path in the conversion process of machine learning models?',\n",
       "  8: \"What is the purpose of the 'simulator' in the context of the provided dictionary?\",\n",
       "  9: 'What is the purpose of Softmax Bias Correction in advanced quantization methods?'},\n",
       " 'retrieved_contexts': {0: ['Section: Quantizer\\n# Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4',\n",
       "   'Section: # Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision.\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4',\n",
       "   'Section: Quantizer\\n# Quantizer\\n## Advanced Quantization Methods\\nContent:\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error',\n",
       "   'Section: # Quantizer\\nContent:\\n## Advanced Quantization Methods\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error\\n- Dummy Convolution for the Concat Operator\\n\\t- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.\\n## Debug API\\n### Layer-wise mixed precision quantiztion debug API\\nThe quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary.',\n",
       "   'Section: # Quantizer\\nContent:\\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API.',\n",
       "   'Section: Quantizer\\n# Quantizer\\nContent:\\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization. Users can apply mixed precision quantization to models by specifying activation names or operators.',\n",
       "   \"Section: Optimizer\\n# Optimizer\\nContent:\\n- support optimization method.<br>\\n\\tOptimizer supports many optimization features so that the model works efficiently on the device.\\n\\t- Fold\\n\\t\\t- GeGLU\\n\\t\\t- GroupNorm\\n\\t\\t- LayerNorm\\n\\t\\t- PReLU\\n\\t\\t- RMSNorm\\n\\t- Fuse\\n\\t\\t- SiLU\\n\\t\\t- BatchNorm into Convolution\\n\\t\\t- Cast\\n\\t\\t- Deconvolution bias\\n\\t\\t- Math\\n\\t- Insert\\n\\t\\t- Depthwise Convolution for activation\\n\\t- Replace\\n\\t\\t- Average Pooling to Depthwise convolution\\n\\t\\t- Eltwise concat convolution\\n\\t\\t- Convolution kernel 1 to 3\\n\\t\\t- Matrix multiplication to dynamic convolution\\n\\t\\t- ReduceMean to Global average pool\\n\\t\\t- ReduceSum to Convolution\\n\\t\\t- Slice to Split\\n\\t- Change attribute\\n\\t\\t- axis of softmax\\nIn addition, Optimizer Template, one of the Optimizer's powerful features, supports custom optimization methods that users have.\",\n",
       "   'Section: Optimizer\\n# Optimizer\\nContent:\\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\\n- shape_inference<br>\\n\\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\\n- 4dim conversion<br>\\n\\tIn order for the .nnc extension model to operate on a device, the input and output shapes of all operators in the model must be in four dimensions.',\n",
       "   'Section: # **How to use**\\n## **Detailed explanation for eht yaml file**\\nContent:\\nmodel_type : string**\\nSpecify the type of the input model. Optimization is performed differently depending on the model type, with details as follows\\n- CV\\n- LVM\\n- LLM\\n\\n**quantizer : dict**\\n- **precision_weight : str** : precision of weight(ex. int8, int16, fp16)\\n- **precision_activation : str** : precision of activation(ex. int8, int16, fp16)\\n- **mpq_operator_dict : dict** : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above.\\n- **alpha : float** : smoothquant migration strength',\n",
       "   'Section: # Optimizer\\nContent:\\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\\n- shape_inference<br>\\n\\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\\n- 4dim conversion<br>\\n\\tTo deploy a model with a .nnc extension on a device, all operators must have input and output shapes with four dimensions. To facilitate this, the 4D conversion process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. '],\n",
       "  1: ['Section: Quantizer\\n# Quantizer\\n## Advanced Quantization Methods\\nContent:\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error',\n",
       "   'Section: Quantizer\\n# Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4',\n",
       "   'Section: # Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision.\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4',\n",
       "   'Section: # Quantizer\\nContent:\\n## Advanced Quantization Methods\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error\\n- Dummy Convolution for the Concat Operator\\n\\t- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.\\n## Debug API\\n### Layer-wise mixed precision quantiztion debug API\\nThe quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary.',\n",
       "   'Section: Quantizer\\n# Quantizer\\nContent:\\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization. Users can apply mixed precision quantization to models by specifying activation names or operators.',\n",
       "   'Section: # Quantizer\\nContent:\\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API.',\n",
       "   'Section: # Simulator\\nContent:\\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\\nSimulator includes the following features(with example codes):',\n",
       "   'Section: Simulator\\n# Simulator\\nContent:\\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\\nSimulator includes the following features:\\n- `get_quantization_sim`: Attain a CNNX quantization simulation session for manual run.\\n- `load_input_data`: Load the input data from the given dataset path.\\n- `run_inference`: Conduct inference on a CNNX model.\\n- `compare_model_by_inference`: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric.\\n- `compare_model_by_layer` (TBD): Compare two CNNX models on intermediate tensors for each layer. To be released.',\n",
       "   'Section: # **How to use**\\n## **Detailed explanation for eht yaml file**\\nContent:\\nmodel_type : string**\\nSpecify the type of the input model. Optimization is performed differently depending on the model type, with details as follows\\n- CV\\n- LVM\\n- LLM\\n\\n**quantizer : dict**\\n- **precision_weight : str** : precision of weight(ex. int8, int16, fp16)\\n- **precision_activation : str** : precision of activation(ex. int8, int16, fp16)\\n- **mpq_operator_dict : dict** : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above.\\n- **alpha : float** : smoothquant migration strength',\n",
       "   'Section: # Simulator\\nContent:\\n- `get_quantization_sim`: Attain a CNNX quantization simulation session for manual run.\\n\\t```python\\n\\tfrom simulator imoprt api\\n\\toutput_names = [LIST_OF_ONNX_OUTPUTS]\\n\\tinput_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1]\\n\\tquantsim = api.get_quantization_sim(params)\\n\\tinference_session = quantsim.session\\n\\tresult = session.run(output_names, input_dict)\\n\\tprint(result)\\n\\t```\\n- `load_input_data`: Load the input data from the given dataset path.\\n- `run_inference`: Conduct inference on a CNNX model.\\n- `compare_model_by_inference`: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric.'],\n",
       "  2: ['Section: # Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: # Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: # Model Optimization Flow\\n## LLM\\nContent:\\n1. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **CNNX-to SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## LLM\\nContent:\\n1. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **CNNX-to SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: # Converter\\nContent:\\n- ONNX2CNNX Converter\\n\\t- Converts an ONNX model to CNNX.\\n\\t- Requires specification of both the output model path and the output encodings path.\\n- CNNX2SNC Converter\\n\\t- Converts a CNNX model to SNC.\\nExample codes to use them:\\n```python\\nfrom converter import api\\ncnnx_to_snc_params = api.Cnnx2SncParameters(\\n\\tinput_model_path = \"/path/to/model.onnx\",\\n\\tinput_encodings_path = \"/path/to/model.encodings\",\\n\\toutput_model_path = \"/output/path/for/model.snc\"\\n\\t)\\napi.Converter.cnnx_to_snc(cnnx_to_snc_params)\\ntflite_to_onnx_params = api.TfLite2OnnxParameters(\\n\\tinput_model_path = \"/path/to/model.tflite\",\\n\\toutput_model_path = \"./output/path/for/model.onnx\",\\n\\t)\\napi.Converter.tflite_to_onnx(tflite_to_onnx_params)\\n```',\n",
       "   'Section: # Simulator\\nContent:\\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\\nSimulator includes the following features(with example codes):',\n",
       "   'Section: Simulator\\n# Simulator\\nContent:\\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\\nSimulator includes the following features:\\n- `get_quantization_sim`: Attain a CNNX quantization simulation session for manual run.\\n- `load_input_data`: Load the input data from the given dataset path.\\n- `run_inference`: Conduct inference on a CNNX model.\\n- `compare_model_by_inference`: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric.\\n- `compare_model_by_layer` (TBD): Compare two CNNX models on intermediate tensors for each layer. To be released.',\n",
       "   'Section: Model Requirements and Constraints\\n# Model Requirements and Constraints\\n## CNNX model\\nContent:\\nCNNX model consists of ONNX model and encodings file. Encodings file should contain quantization information and optimization module information according to the given ONNX model in json format.\\nExample for encodings file is shown below:\\n```json\\n{\\n\\t\"activation_encodings\": {\\n\\t\\t\"/act/a0\": [\\n\\t\\t\\t{\\n\\t\\t\\t\\t\"bitwidth\": 8,\\n\\t\\t\\t\\t\"dtype\": \"int\",\\n\\t\\t\\t\\t\"is_symmetric\": \"False\",\\n\\t\\t\\t\\t\"max\": 0.0,\\n\\t\\t\\t\\t\"min\": 0.0,\\n\\t\\t\\t\\t\"offset\": 0,\\n\\t\\t\\t\\t\"scale\": 0.0\\n\\t\\t\\t}'],\n",
       "  3: ['Section: # Quantizer\\nContent:\\n## Advanced Quantization Methods\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error\\n- Dummy Convolution for the Concat Operator\\n\\t- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.\\n## Debug API\\n### Layer-wise mixed precision quantiztion debug API\\nThe quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary.',\n",
       "   'Section: Quantizer\\n# Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4',\n",
       "   'Section: # Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision.\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4',\n",
       "   'Section: Quantizer\\n# Quantizer\\n## Advanced Quantization Methods\\nContent:\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error',\n",
       "   'Section: # Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: Optimizer\\n# Optimizer\\nContent:\\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\\n- shape_inference<br>\\n\\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\\n- 4dim conversion<br>\\n\\tIn order for the .nnc extension model to operate on a device, the input and output shapes of all operators in the model must be in four dimensions.',\n",
       "   'Section: # Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: # Quantizer\\nContent:\\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API.',\n",
       "   'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`'],\n",
       "  4: ['Section: # Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: # Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: # Quantizer\\nContent:\\n## Advanced Quantization Methods\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error\\n- Dummy Convolution for the Concat Operator\\n\\t- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.\\n## Debug API\\n### Layer-wise mixed precision quantiztion debug API\\nThe quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary.',\n",
       "   'Section: # Model Optimization Flow\\n## LLM\\nContent:\\n1. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **CNNX-to SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: Quantizer\\n# Quantizer\\n## Advanced Quantization Methods\\nContent:\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error',\n",
       "   'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## LLM\\nContent:\\n1. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **CNNX-to SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: Optimizer\\n# Optimizer\\nContent:\\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\\n- shape_inference<br>\\n\\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\\n- 4dim conversion<br>\\n\\tIn order for the .nnc extension model to operate on a device, the input and output shapes of all operators in the model must be in four dimensions.',\n",
       "   'Section: # Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision.\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4'],\n",
       "  5: ['Section: # **How to use**\\n## **Detailed explanation for elt yaml file**\\nContent:\\nperf_estimator : dict**\\n- **O2_enable : bool** : O2 optimization (true or false)\\n- **O2_fm_forwarding : bool** : feature-map forwarding (true or false)\\n- **SEG : bool** : Set true if input model is Deeplab V3+\\n- **SSD : bool** : Set true if input model is SSD detection\\n- **bit_width_factor_FM : int** : Select feature map bit width factor (1 or 2)\\n- **bit_width_factor_FP16 : bool** : Set bit width factor as floating point (true or false)\\n- **bit_width_factor_weight : int** : Select weight bit width factor (1 or 2)\\n- **core_num : int** : 1 for single core, 2 for instance-1\\n- **device : str** : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a]\\n- **json_report : bool** : Enable report json format',\n",
       "   'Section: # **How to use**\\n## **Detailed explanation for elt yaml file**\\nContent:\\n- **nq_fold : bool** : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32)\\n\\n**profiler : dict**\\n- **iter : int** : This decides how many time the model inference will be processed.\\n- **mode : str** : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost]\\n- **target : str** : profiling target. [model, system]',\n",
       "   'Section: ENN Framwork API Functions\\n# ENN Framwork API Functions\\n## Security, preference, get meta information..\\n### Functions Documentation\\nContent:\\n#### function EnnSetPreferencePresetId\\n```cpp\\nEnnReturn EnnSetPreferencePresetId(\\n\\tconst uint32_t val\\n)\\n```\\nSetting Preset ID for operation performance.\\n**Parameters**:\\n- **val** [IN] value to set preset ID\\n**Return**: EnnReturn result, 0 is success\\n#### function EnnSetPreferencePerfConfigId\\n```cpp\\nEnnReturn EnnSetPreferencePerfConfigId(\\n\\tconst uint32_t val\\n)\\n```\\nSetting PerfConfig ID for operation performance.\\n**Parameters**:\\n- **val** [IN] value to set PerfConfig ID\\n**Return**: EnnReturn result, 0 is success\\n#### function EnnSetPreferencePerfMode\\n```cpp\\nEnnReturn EnnSetPreferencePerfMode(\\n\\tconst uint32_t val\\n)\\n```\\nSetting Performance Mode.\\n**Parameters**:\\n- **val** [IN] value to set Performance Mode\\n**Return**: EnnReturn result, 0 is success',\n",
       "   'Section: # **How to use**\\n## **Detailed explanation for elt yaml file**\\nContent:\\n- **assign_gpu : str** : Assign specific layer to gpu device\\n- **best_fit_generalized : bool** : Control whether generalized best fit allocation is to be used.\\n- **cast_in : str** : Type casting fp32 to fp16 for nnc input data\\n- **cast_out : str** : Type casting fp16 to fp32 for nnc output data\\n- **cfs : bool** : Enable cfifo sync\\n- **compiler : str** : Compiler option\\n- **datalayout_conversion_in : str** : Data layout(NHWC) conversion for nnc input data\\n- **datalayout_conversion_out : str** : Data layout(NHWC) conversion for nnc output data\\n- **debug_str : str** : debug str for compiler\\n- **dequant_type : str** : dequantiztion type\\n- **device : str** : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7]',\n",
       "   'Section: # **How to use**\\n## **Detailed explanation for elt yaml file**\\nContent:\\n- **test_type : str** : ENN running mode [lib, service]\\n- **tv_threshold : float** : The value is used for tolerance threshold of output match verification.\\n- **bitmatch_test : bool** : if set true, visual profiler will compile nnc first\\n- **core_num : str** : The number of NPU core. [single, multiple]\\n- **device : str** : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7]\\n- **device_id : str** : id of the device connected to the server or PC running the enntools docker\\n- **remote_ssh_config_path : str** : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml)\\n- **ssh_bool : str** : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE]',\n",
       "   'Section: # **How to use**\\n## **Detailed explanation for elt yaml file**\\nContent:\\n- **data_format : str** : [channel_first, channel_last]\\n- **debug : bool** : dump layerwise sqnr between new snc and hw quantized snc.\\n- **gpu_enable : bool** : enable infer model on gpu if gpu is available\\n- **gpu_id : int** : gpu id for quantization profiling\\n- **input_dtype : str** : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model).\\n- **mode : str** : [elt, eht_cnnx, eht_snc]\\n- **output_dtype : str** : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none]\\n- **profile_batchsize : int** : Batchsize for profile (value 100 is recommened).',\n",
       "   'Section: # **How to use**\\n## **Detailed explanation for elt yaml file**\\nContent:\\nmodel_analyzer : dict**\\n- **check : bool** : Check the op support status\\n- **device : str** : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6]\\n- **level : int** : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint)\\n- **snc_input : bool** : analyze the snc model, false: analyze the original model\\n\\n**database_gen : dict**\\n- **database_spec : str** : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml)\\n\\n**converter : dict**\\n- **device : str** : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7]',\n",
       "   \"Content:\\n# **Dataset preparation**\\nThis is a guideline for preparing the input dataset.\\n## **Dataset format**\\nThe dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur.\\nAdditionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format.\\nThe h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace.\\n# Model Requirements and Constraints\\n## model format\\n- Model should be prepared in ONNX format to start optimization.\\n## opset version\\n- EHT currently support ONNX opset version 13 ~ 17.\",\n",
       "   'Question: Please tell us about the purpose of EDS Assistant.\\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.',\n",
       "   'Section: # **How to use**\\n## **Detailed explanation for eht yaml file**\\nContent:\\n- **metric : string**: The metric to be used for measurement\\n- **threshold : float** : The threshold value of the metric that determines agreement / disagreement\\n- **input_data_path : string**: The path to the dataset for model inference\\n\\n**optimizer : dict**\\n- **skip_4_dim_conversion : bool** : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion.\\n- **overwrite_input_shapes : dict** : Enter the input shape for models with undefined input shapes.\\n- **custom_template_path : dict**: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value.'],\n",
       "  6: ['Section: # Optimizer\\n## How to Create Custom Templates\\nContent:\\nOptimizer is a tool that allows users to easily optimize their models. Users can add their own optimization methods using the \"Optimizer Template\" of Optimizer. The following process is required to apply the user-defined template to the user model.\\n- Create custom templates.\\n- Prepare model to be optimized\\n- Validate optimized model',\n",
       "   'Section: Optimizer\\n# Optimizer\\n## How to Create Custom Templates\\nContent:\\nOptimizer is a tool that allows users to easily optimize their models. Users can add their own optimization methods using the \"Optimizer Template\" of Optimizer. The following process is required to apply the user-defined template to the user model.\\n- Create custom templates.\\n- Prepare model to be optimized\\n- Validate optimized model',\n",
       "   'Section: # Optimizer\\n## How to Create Custom Templates\\nContent:\\n### Optimizer Template\\nThe Optimizer template provides a scenario of how the model should be optimized. Its components are shown in the figure below.\\n\\nOptimization Template Component\\n- Optimize Name: Name of the optimization you want to apply\\n- Trigger condition : Trigger condition for starting application of the Optimization technique in graph search (typically, the trigger condition is the operator type)\\n- Origin Module: Module before optimization\\n- Origin condition: Check that the Origin Module is candidate to this optimization technique (applied if trigger condition is triggered)\\n- Optimized Module: Module after optmization\\n- Optimization: Modification condition of Optimized applied model (optional)\\n### Flow\\nThe Optimizer Templates are executed as shown in the flow chart below. In searching for the model, the node meets the trigger_op method of the template for the first time. Then it loops through the origin_condition of the case. If the condition is correct, it optimizes the module through steps.',\n",
       "   'Section: Optimizer\\n# Optimizer\\n## How to Create Custom Templates\\nContent:\\n### Optimizer Template\\nThe Optimizer template provides a scenario of how the model should be optimized. Its components are shown in the figure below.\\n\\nOptimization Template Component\\n- Optimize Name: Name of the optimization you want to apply\\n- Trigger condition : Trigger condition for starting application of the Optimization technique in graph search (typically, the trigger condition is the operator type)\\n- Origin Module: Module before optimization\\n- Origin condition: Check that the Origin Module is candidate to this optimization technique (applied if trigger condition is triggered)\\n- Optimized Module: Module after optmization\\n- Optimization: Modification condition of Optimized applied model (optional)\\n### Flow\\nThe Optimizer Templates are executed as shown in the flow chart below. In searching for the model, the node meets the trigger_op method of the template for the first time. Then it loops through the origin_condition of the case. If the condition is correct, it optimizes the module through steps.',\n",
       "   'Section: Optimizer\\n# Optimizer\\n## How to Create Custom Templates\\n### Create custom templates\\nContent:\\nInherit the above classes and declare each class corresponding to Step, Case, and Template. Import package classes of model_editor for updating the model. If you need more information about model_editor, please see the guide about model_editor.\\nWe **activate the trigger_op method** when the node is one of the four basic arithmetic operators and the input feature map has 4 dimension. In the **origin_condition method of the case class**, more detailled conditions are checked: the previous node of the arithmetic operator must be a convolution layer, the arithmetic operator must be the only node following the convolution node, and it must not be folded yet.',\n",
       "   \"Section: # Optimizer\\n## How to Create Custom Templates\\n### Create custom templates\\nContent:\\nclass OptimizerTemplateInternal(OptimizerTemplate):\\n\\tdef __repr__(self):\\n\\t\\treturn '{}(name={})'.format(\\n\\t\\t\\tself.__class__.__name__, self.name)\\n\\t@property\\n\\tdef name(self) -> AnyStr:\\n\\t\\treturn self._name\\n\\t@name.setter\\n\\tdef name(self, value: AnyStr):\\n\\t\\tself._name = value\\n\\t@property\\n\\tdef case_list(self) -> List[TemplateCaseInternal]:\\n\\t\\treturn self._case_list\\n\\t@case_list.setter\\n\\tdef case_list(self, value: List[TemplateCaseInternal]):\\n\\t\\tself._case_list = value\\n\\tdef trigger_op(self, node: CnnxNode) -> bool:\\n\\t\\traise NotImplementedError\\n\\tdef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\\n\\t\\tif self.trigger_op(node):\\n\\t\\t\\tfor case in self.case_list:\\n\\t\\t\\t\\tif case.launch(model, node):\\n\\t\\t\\t\\t\\treturn True\\n\\t\\treturn False\\n```\\nInherit the above classes and declare each class corresponding to Step, Case, and Template. Import package classes of model_editor for updating the model. If you need more information about model_editor, please see the guide about model_editor.\",\n",
       "   'Section: Optimizer\\n# Optimizer\\n## How to Create Custom Templates\\n### How to apply a custom template\\nContent:\\nWrite the path of the custom template and the class name of the template. Following the above example, it can be written as follows:\\n```yaml\\ninput_model_path: /ssd2/shared_data/models/kpi/denoise_model_1.5.424_1x2304x1296x3/denoise_model_1.5.424_1x2304x1296x3.onnx\\noutput_folder_path: /home/results_denoise_1.5___\\ninput_model_format: onnx\\nmodel_type: kpi_cv\\nquantizer:\\n\\t~~~~~\\nsimulator:\\n\\t~~~~~~\\noptimizer:\\n\\t~~~~~~\\n\\tcustom_template_path:\\n\\t/usr/local/lib/python3.10/dist-packages/optimizer/core/templates/fuse_deconv_bias.py:\\n\\t- TemplateFuseDeconvBias\\n```',\n",
       "   \"Section: Optimizer\\n# Optimizer\\n## How to Create Custom Templates\\n### Create custom templates\\nContent:\\nclass OptimizerTemplateInternal(OptimizerTemplate):\\n\\tdef __repr__(self):\\n\\t\\treturn '{}(name={})'.format(\\n\\t\\t\\tself.__class__.__name__, self.name)\\n\\t@property\\n\\tdef name(self) -> AnyStr:\\n\\t\\treturn self._name\\n\\t@name.setter\\n\\tdef name(self, value: AnyStr):\\n\\t\\tself._name = value\\n\\t@property\\n\\tdef case_list(self) -> List[TemplateCaseInternal]:\\n\\t\\treturn self._case_list\\n\\t@case_list.setter\\n\\tdef case_list(self, value: List[TemplateCaseInternal]):\\n\\t\\tself._case_list = value\\n\\tdef trigger_op(self, node: CnnxNode) -> bool:\\n\\t\\traise NotImplementedError\\n\\tdef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\\n\\t\\tif self.trigger_op(node):\\n\\t\\t\\tfor case in self.case_list:\\n\\t\\t\\t\\tif case.launch(model, node):\\n\\t\\t\\t\\t\\treturn True\\n\\t\\treturn False\\n```\",\n",
       "   'Section: Optimizer\\n# Optimizer\\n## How to Create Custom Templates\\n### Create custom templates\\nContent:\\n\\t@property\\n\\tdef name(self) -> AnyStr:\\n\\t\\treturn self._name\\n\\t@name.setter\\n\\tdef name(self, value: AnyStr):\\n\\t\\tself._name = value\\n\\t@property\\n\\tdef origin(self) -> Dict:\\n\\t\\treturn self._origin\\n\\t@origin.setter\\n\\tdef origin(self, value: Dict):\\n\\t\\tself._originn = value\\n\\t@property\\n\\tdef step_list(self) -> Dict:\\n\\t\\treturn self._step_list\\n\\t@step_list.setter\\n\\tdef step_list(self, value: Dict):\\n\\t\\tself._step_list = value\\n\\tdef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\\n\\t\\traise NotImplementedError\\n\\tdef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\\n\\t\\tif self.origin_condition(model, node):\\n\\t\\t\\tfor st in self.step_list:\\n\\t\\t\\t\\tif not st.optimization(model, node):\\n\\t\\t\\t\\t\\treturn False\\n\\t\\t\\treturn True',\n",
       "   'Section: Optimizer\\n# Optimizer\\n## How to Create Custom Templates\\n### Create custom templates\\nContent:\\nIn this part, we introduce how to write custom templates step by step. As an example, we use a template that fuses basic arithmetic operations layers into a convolution layer, when possible. We provide `TemplateStepInternal`, `TemplateCaseInternal`, `OptimizerTemplateInternal` to prevent abnormal behavior of the code.\\n```python\\nclass TemplateStepInternal(TemplateStep):\\n\\t@property\\n\\tdef name(self) -> AnyStr:\\n\\t\\treturn self._name\\n\\t@name.setter\\n\\tdef name(self, value: AnyStr):\\n\\t\\tself._name = value\\n\\t@property\\n\\tdef origin(self) -> Dict:\\n\\t\\treturn self._origin\\n\\t@origin.setter\\n\\tdef origin(self, value: Dict):\\n\\t\\tself._origin = value\\n\\tdef optimization(self, model: CnnxModel, node: CnnxNode) -> Union[Tuple[List[CnnxNode], List[CnnxNode], List[Port], List[Port]], Dict]:\\n\\t\\treturn NotImplementedError\\nclass TemplateCaseInternal(TemplateCase):'],\n",
       "  7: ['Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## LLM\\nContent:\\n1. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **CNNX-to SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: # Model Optimization Flow\\n## LLM\\nContent:\\n1. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **CNNX-to SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: Changing the Model\\n# Changing the Model in an Android Sample Application\\n## Example: Switching to MobileNet V3\\n### Updating the Android Project\\nContent:\\n\\t\\tval OUTPUT_DATA_TYPE = DataType.FLOAT32\\n\\t\\tconst val OUTPUT_CONVERSION_SCALE = 1F\\n\\t\\tconst val OUTPUT_CONVERSION_OFFSET = 0F\\n\\t\\tconst val LABEL_FILE = \"labels1001.txt\"\\n\\t}\\n\\t```\\n3. Adjust the `postProcess()` function for the output of the new model, which is a logits-vector instead of $[0,1]$ scores:\\n\\t```kotlin\\n\\tprivate fun postProcess(modelOutput: ByteArray): Map<String, Float> {\\n\\t\\tval output = when (OUTPUT_DATA_TYPE) {\\n\\t\\t\\tDataType.FLOAT32 -> {\\n\\t\\t\\t\\tval byteBuffer = ByteBuffer.wrap(modelOutput).order(ByteOrder.nativeOrder())',\n",
       "   'Section: Explanation of model optimization flow\\n# Model Optimization Flow\\nContent:\\nThis section details the optimization flow according to model type. Currently, optimization scenarios are predetermined for specific model types, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). We plan to update the features to allow flexible scheduling of optimization functions.',\n",
       "   'Section: # Model Optimization Flow\\nContent:\\nThis section details the optimization flow according to model type. Currently, optimization scenarios are predetermined for specific model types, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). We plan to update the features to allow flexible scheduling of optimization functions.',\n",
       "   'Section: # Model Optimization Flow\\n## LVM\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\\n\\toutput_path :\\n\\t`{result_dir}/smoothquant`\\n\\t`{result_dir}/mixed_precision_quant`\\n\\t`{result_dir}/softmax_bias_correction`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: # Model Optimization Flow\\n## CV\\nContent:\\n1. **ONNX-to-CNNX Conversion**\\n\\tConverts an opset16 ONNX model to CNNX format.\\n\\toutput_path : `{result_dir}/cnnx`\\n2. **Optimization**\\n\\tPasses the CNNX model through Simplifier and applies an Optimization template.\\n\\toutput_path : `{result_dir}/optimized`\\n3. **Performance Evaluation of Optimization**\\n\\tCompares the inference results of the model before and after optimization.\\n4. **Quantization**\\n\\tApplies Fixed Precision Quantization to the optimized CNNX model.\\n\\toutput_path : `{result_dir}/quantized`\\n5. **Performance Evaluation of Quantization**\\n\\tCompares the inference results of the model before and after quantization.\\n6. **CNNX-to-SNC Conversion**\\n\\tConverts the CNNX model to SNC format.\\n\\toutput_path : `{result_dir}/snc`',\n",
       "   'Section: # Converter\\nContent:\\n- ONNX2CNNX Converter\\n\\t- Converts an ONNX model to CNNX.\\n\\t- Requires specification of both the output model path and the output encodings path.\\n- CNNX2SNC Converter\\n\\t- Converts a CNNX model to SNC.\\nExample codes to use them:\\n```python\\nfrom converter import api\\ncnnx_to_snc_params = api.Cnnx2SncParameters(\\n\\tinput_model_path = \"/path/to/model.onnx\",\\n\\tinput_encodings_path = \"/path/to/model.encodings\",\\n\\toutput_model_path = \"/output/path/for/model.snc\"\\n\\t)\\napi.Converter.cnnx_to_snc(cnnx_to_snc_params)\\ntflite_to_onnx_params = api.TfLite2OnnxParameters(\\n\\tinput_model_path = \"/path/to/model.tflite\",\\n\\toutput_model_path = \"./output/path/for/model.onnx\",\\n\\t)\\napi.Converter.tflite_to_onnx(tflite_to_onnx_params)\\n```'],\n",
       "  8: ['Section: Simulator\\n# Simulator\\nContent:\\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\\nSimulator includes the following features:\\n- `get_quantization_sim`: Attain a CNNX quantization simulation session for manual run.\\n- `load_input_data`: Load the input data from the given dataset path.\\n- `run_inference`: Conduct inference on a CNNX model.\\n- `compare_model_by_inference`: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric.\\n- `compare_model_by_layer` (TBD): Compare two CNNX models on intermediate tensors for each layer. To be released.',\n",
       "   'Section: # Simulator\\nContent:\\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\\nSimulator includes the following features(with example codes):',\n",
       "   \"Section: Backend API\\n# **Backend API**\\n## **Simulator**\\n### `run_inference`\\nContent:\\nInferenceRunnerParameters schema details**\\n**input_model_path : string**\\nSpecifies the file path of the input model. It should point to a file, not a parent folder.\\n\\n**input_encondings_path : string**\\nIndicates the file path for the encodings file of the cnnx model.\\n\\n**input_data_path : string**\\nSpecifies the path to the folder containing data required for inference.\\n\\n**use_cuda : bool**\\nDetermines the inference method: when true, uses GPU for inference; when false, performs inference using CPU only.\\n\\n**layerwise_check_name : list**\\nAccepts input values to retrieve intermediate featuremap information along with the output during inference. When featuremap names are provided, the inference runner's return value includes the values of those featuremaps appended.\\n\\n**Example of run_inference API**\\n```python\\nfrom simulator.schema.base import InferenceRunnerParameters\\nfrom simulator.api import Simulator\\nparams = InferenceRunnerParameters(\\n\\tinput_model_path={input_model_path}\\n\\tinput_encodings_path={input_encodings_path},\\n\\tinput_data_path={input_data_path},\\n\\t)\\nres = Simulator.run_inference(params)\\n```\",\n",
       "   'Section: Backend API\\n# **Backend API**\\n## **Simulator**\\n### `compare_model_by_inference`\\nContent:\\nDetermines the inference method: when true, uses GPU for inference; when false, performs inference using CPU only.\\n\\n**Example of compare_model_by_inference API**\\n```python\\nfrom simulator.schema.base import InferenceCheckerParameters\\nfrom simulator.api import Simulator\\nparams = InferenceCheckerParameters(\\n\\t\\t\\tinput_data_path = \"/path/to/data/dir/\",\\n\\t\\t\\tinput_model_path_0 = \"/path/to/model_0.onnx\",\\n\\t\\t\\tinput_encodings_path_0 = \"/path/to/model_0.encodings\",\\n\\t\\t\\tinput_model_path_1 = \"/path/to/model_1.onnx\",\\n\\t\\t\\tinput_encodings_path_1 = \"/path/to/model_1.encodings\",\\n\\t\\t\\tmetric = \"snr\",\\n\\t\\t\\tthreshold = 30,\\n\\t\\t\\tuse_cuda = True\\n\\t\\t)\\nres = Simulator.compare_model_by_inference(params)\\n```',\n",
       "   'Section: Backend API\\n# **Backend API**\\n## **Simulator**\\n### `get_inference_session`\\nContent:\\nGetInferenceSessionParameters schema details**\\n**input_model_path : string**\\nSpecifies the file path of the input model. It should point to a file, not a parent folder.\\n\\n**input_encondings_path : string**\\nIndicates the file path for the encodings file of the cnnx model.\\n\\n**use_cuda : bool**\\nDetermines the inference method: when true, uses GPU for inference; when false, performs inference using CPU only.\\n\\n**Example of run_inference API**```python\\nfrom simulator.schema.base import InferenceSessionParameters\\nfrom simulator.api import Simulator\\nparams = InferenceSessionParameters(\\n\\tinput_model_path={input_model_path}\\n\\tinput_encodings_path={input_encodings_path},\\n\\tuse_cuda={input_data_path},\\n\\t)\\nres = Simulator.get_inference_session(params)\\n```',\n",
       "   'Section: # Simulator\\nContent:\\n- `get_quantization_sim`: Attain a CNNX quantization simulation session for manual run.\\n\\t```python\\n\\tfrom simulator imoprt api\\n\\toutput_names = [LIST_OF_ONNX_OUTPUTS]\\n\\tinput_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1]\\n\\tquantsim = api.get_quantization_sim(params)\\n\\tinference_session = quantsim.session\\n\\tresult = session.run(output_names, input_dict)\\n\\tprint(result)\\n\\t```\\n- `load_input_data`: Load the input data from the given dataset path.\\n- `run_inference`: Conduct inference on a CNNX model.\\n- `compare_model_by_inference`: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric.',\n",
       "   'Section: Backend API\\n# **Backend API**\\n## **Simulator**\\n### `compare_model_by_inference`\\nContent:\\nInferenceCheckerParameters object details**\\n**input_model_path0 : string**\\nThe onnx model file path to be compared.\\n\\n**input_encondings_path_0 : string**\\nThe model encodings file path for the onnx model 0.\\n\\n**input_model_path_1 : string**\\nThe onnx model file path to be compared.\\n\\n**input_encondings_path_1 : string**\\nThe model encodings file path for the onnx model 1.\\n\\n**input_data_path : string**\\nSpecifies the path to the folder containing data required for inference.\\n\\n**meric : string**\\nMetric in which the error is calculated (“snr”, “rmse”, …)\\n\\n**threshold : float**\\nThe threshold by which two models are considered as equivalent.\\n\\n**use_cuda : bool',\n",
       "   'Section: # **How to use**\\n## **Detailed explanation for eht yaml file**\\nContent:\\n- **calibration_data_path : string** : The path to the representative data to be used for calibration\\n- **calibration_args : dict :** Arguments for processing calibration\\n\\t- **samples : int** : How many calibration data samples to use\\n\\t- **seed : int** : A value set as a seed for random selection\\n- **add_dummy_conv: bool** : Whether apply the dummy conv algorithm\\n- **input_dtype: dict** : Input data type for the quantized model\\n- **output_dtype: dict** : Output data type for the quantized model\\n\\n**simulator : dict',\n",
       "   'Section: Dataset preparation\\n# **Dataset preparation**\\nContent:\\nThis is a guideline for preparing the input dataset for the simulator and calibration dataset for the quantizer module.\\n## **Detailed instruction for dataset preparation**\\nAnalyze model inputs using Netron. The example model receives three inputs: encoder_hidden_states, sample, and timestep.\\n\\nCreate a JSON file named \\'input_folder_mapping.json\\'. In this file, define key-value pairs where each key represents a model input name and its corresponding value is the name of the folder containing the input data for that model input.\\n```json\\n{\\n\\t\"encoder_hidden_states\": \"input_encoder_hidden_states\",\\n\\t\"sample\": \"sample\",\\n\\t\"timestep\": \"timestep\"\\n}\\n```\\nAfter preparing the dataset and JSON file, create a dataset directory and place it in the specified location.\\n\\nIt\\'s important to note that the file names for each input should be identical, as shown in the example above.\\n## Dataset Format Support\\nCurrently, the supported data type is npy files consisting of float type. Support for other data formats is planned for the future.| Data Format | Support |\\n|--------------------------------------------------------|---------|\\n| numpy array(.npy, float) | O |\\n| numpy array(.npy, int) | TBD |\\n| binary data(.bin) | TBD |\\n| image(.jpg) | TBD |',\n",
       "   'Section: # Simulator\\nContent:\\n- `compare_model_by_layer` : Compare two CNNX models on intermediate tensors for each layer.\\n\\t```python\\n\\tfrom simulator imoprt api\\n\\t\\tparams = api.LayerwiseCheckerParameters(\\n\\t\\t\\tinput_data_path = \"/path/to/input/data/,\\n\\t\\t\\tinput_model_path_0 = \"/path/to/model/0.onnx\",\\n\\t\\t\\tinput_encodings_path_0 = \"/path/to/model/0.encodings\",\\n\\t\\t\\tinput_model_path_1 = \"/path/to/model/0.onnx\",\\n\\t\\t\\tinput_encodings_path_1 = \"/path/to/model/0.encodings\",\\n\\t\\t\\tmetric = \"snr\",\\n\\t\\t\\tthreshold = 100,\\n\\t\\t\\tuse_cuda = False,\\n\\t\\t\\tlayer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")]\\n\\t\\t\\texport_featuremap = True,\\n\\t\\t\\texport_featuremap_path = \"/path/to/save/exported/featuremap\"\\n\\t\\t)\\n\\t\\tres = api.compare_model_by_layer(params)\\n\\t\\tprint(\"Layerwise check\")\\n\\t\\tfor input_name, result in res.items():\\n\\t\\t\\t_, result_dict = result\\n\\t\\t\\tfor output_name, snr_value in result_dict.items():\\n\\t\\t\\t\\tprint(f\"{output_name}: {snr_value}\")\\n\\t```'],\n",
       "  9: ['Section: Quantizer\\n# Quantizer\\n## Advanced Quantization Methods\\nContent:\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error',\n",
       "   'Section: # Quantizer\\nContent:\\n## Advanced Quantization Methods\\nCurrently supported advanced quantization methods are as follows.\\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n\\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n\\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n\\t- A method that tune the weights range of the channels in one tensor to reduce quantization error\\n- Dummy Convolution for the Concat Operator\\n\\t- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.\\n## Debug API\\n### Layer-wise mixed precision quantiztion debug API\\nThe quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary.',\n",
       "   'Section: Quantizer\\n# Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4',\n",
       "   'Section: # Quantizer\\n## Basic Quantization Methods\\nContent:\\n### Fixed Precision Quantization\\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision.\\n### Mixed Precision Quantization\\nAllows different parts of the model to use different precisions<br>\\nTwo approaches are supported:\\n- Mixed precision by name: Users specify precisions for specific activation or weight names\\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4',\n",
       "   'Section: # Quantizer\\nContent:\\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API.',\n",
       "   'Section: Quantizer\\n# Quantizer\\nContent:\\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization. Users can apply mixed precision quantization to models by specifying activation names or operators.',\n",
       "   'Section: # Simulator\\nContent:\\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\\nSimulator includes the following features(with example codes):',\n",
       "   'Section: # Optimizer\\nContent:\\nThe channel axes can be determined by certain layers, such as Convolutional and Linear layers, which have fixed channel axes at positions 1 and -1, respectively. However, they can be modified by operators like MatMul and Reshape, which can expand or reduce them. To account for these changes, the conversion process tracks channel axes in both the forward and backward pass. As a result, the 4D conversion process may require additional time to complete.\\n- support optimization method.<br>\\n\\tOptimizer supports many optimization features so that the model works efficiently on the device.\\n\\t- Fold\\n\\t\\t- GeGLU\\n\\t\\t- GeLU\\n\\t\\t- GroupNorm\\n\\t\\t- LayerNorm\\n\\t\\t- PReLU\\n\\t\\t- RMSNorm\\n\\t\\t- SiLU',\n",
       "   'Section: # **How to use**\\n## **Detailed explanation for elt yaml file**\\nContent:\\n- **do_quantize : bool** : Enable quantization\\n- **eltsum_cvt : bool** : enable eltwise conversion process; eltsum -> concat + conv\\n- **graph_opt : str** : Graph Optimizer Model Type, Model Name for Graph Optimizer.\\n- **mean : str** : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN\\n- **onnx_simplify : bool** : enable onnx_simplify process\\n- **optimize : bool** : Use graph optimization\\n- **quantize_type : str** : Select quantization type, quantized model (include caffeQAT) is \"qat\"\\n- **scale : str** : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE\\n- **bw_ofm : int** : Bitwidth of intermediate feature map(A).',\n",
       "   \"Section: # Optimizer\\nContent:\\n\\t- Fuse\\n\\t\\t- SiLU (to GroupNorm)\\n\\t\\t- BatchNorm into Convolution\\n\\t\\t- Cast\\n\\t\\t- Deconvolution bias\\n\\t\\t- Math\\n\\t\\t- multiple reshape and transpose in a row (when possible)\\n\\t\\t- multiple concat in a row\\n\\t- Insert\\n\\t\\t- Depthwise Convolution for activation\\n\\t- Remove\\n\\t\\t- unecesary slices\\n\\t- Replace\\n\\t\\t- Average Pooling to Depthwise convolution\\n\\t\\t- Eltwise concat convolution\\n\\t\\t- expand with concat (by concatenating the same input multiple times)\\n\\t\\t- Convolution kernel 1 to 3\\n\\t\\t- Matrix multiplication to dynamic convolution\\n\\t\\t- ReduceMean to Global Average Pool\\n\\t\\t- ReduceSum to Convolution\\n\\t\\t- Slice to Split\\n\\t\\t- Global Average Pool to 2 Average Pool\\n\\t- Change attribute\\n\\t\\t- axis of softmax\\nIn addition, Optimizer Template, one of the Optimizer's powerful features, supports custom optimization methods made by users.\"]},\n",
       " 'reference_contexts': {0: ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.',\n",
       "   'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.',\n",
       "   'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.',\n",
       "   'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.',\n",
       "   'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.',\n",
       "   'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.',\n",
       "   'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.',\n",
       "   'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.',\n",
       "   'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.',\n",
       "   'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.',\n",
       "   'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.',\n",
       "   'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.',\n",
       "   'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.',\n",
       "   \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\",\n",
       "   'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.',\n",
       "   'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.',\n",
       "   'The configuration values are utilized across different modules of elt.',\n",
       "   'The model needs to be prepared in ONNX format for optimization.',\n",
       "   \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\",\n",
       "   'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.',\n",
       "   'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.',\n",
       "   'EHT supports ONNX opset versions 13 to 17.',\n",
       "   'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.',\n",
       "   'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.',\n",
       "   \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\",\n",
       "   'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.',\n",
       "   \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\",\n",
       "   'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.',\n",
       "   \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\",\n",
       "   'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.',\n",
       "   'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.',\n",
       "   'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.',\n",
       "   'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.',\n",
       "   'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.',\n",
       "   'This document provides guidelines for preparing the input dataset.',\n",
       "   \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\",\n",
       "   'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'],\n",
       "  1: ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.',\n",
       "   'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.',\n",
       "   'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.',\n",
       "   'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.',\n",
       "   'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.',\n",
       "   'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.',\n",
       "   'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.',\n",
       "   'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.',\n",
       "   'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.',\n",
       "   'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.',\n",
       "   'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.',\n",
       "   'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.',\n",
       "   'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.',\n",
       "   \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\",\n",
       "   'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.',\n",
       "   'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.',\n",
       "   'The configuration values are utilized across different modules of elt.',\n",
       "   'The model needs to be prepared in ONNX format for optimization.',\n",
       "   \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\",\n",
       "   'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.',\n",
       "   'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.',\n",
       "   'EHT supports ONNX opset versions 13 to 17.',\n",
       "   'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.',\n",
       "   'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.',\n",
       "   \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\",\n",
       "   'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.',\n",
       "   \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\",\n",
       "   'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.',\n",
       "   \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\",\n",
       "   'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.',\n",
       "   'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.',\n",
       "   'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.',\n",
       "   'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.',\n",
       "   'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.',\n",
       "   'This document provides guidelines for preparing the input dataset.',\n",
       "   \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\",\n",
       "   'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'],\n",
       "  2: ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.',\n",
       "   'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.',\n",
       "   'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.',\n",
       "   'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.',\n",
       "   'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.',\n",
       "   'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.',\n",
       "   'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.',\n",
       "   'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.',\n",
       "   'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.',\n",
       "   'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.',\n",
       "   'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.',\n",
       "   'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.',\n",
       "   'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.',\n",
       "   \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\",\n",
       "   'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.',\n",
       "   'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.',\n",
       "   'The configuration values are utilized across different modules of elt.',\n",
       "   'The model needs to be prepared in ONNX format for optimization.',\n",
       "   \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\",\n",
       "   'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.',\n",
       "   'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.',\n",
       "   'EHT supports ONNX opset versions 13 to 17.',\n",
       "   'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.',\n",
       "   'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.',\n",
       "   \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\",\n",
       "   'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.',\n",
       "   \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\",\n",
       "   'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.',\n",
       "   \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\",\n",
       "   'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.',\n",
       "   'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.',\n",
       "   'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.',\n",
       "   'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.',\n",
       "   'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.',\n",
       "   'This document provides guidelines for preparing the input dataset.',\n",
       "   \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\",\n",
       "   'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'],\n",
       "  3: ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.',\n",
       "   'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.',\n",
       "   'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.',\n",
       "   'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.',\n",
       "   'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.',\n",
       "   'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.',\n",
       "   'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.',\n",
       "   'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.',\n",
       "   'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.',\n",
       "   'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.',\n",
       "   'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.',\n",
       "   'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.',\n",
       "   'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.',\n",
       "   \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\",\n",
       "   'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.',\n",
       "   'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.',\n",
       "   'The configuration values are utilized across different modules of elt.',\n",
       "   'The model needs to be prepared in ONNX format for optimization.',\n",
       "   \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\",\n",
       "   'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.',\n",
       "   'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.',\n",
       "   'EHT supports ONNX opset versions 13 to 17.',\n",
       "   'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.',\n",
       "   'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.',\n",
       "   \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\",\n",
       "   'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.',\n",
       "   \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\",\n",
       "   'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.',\n",
       "   \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\",\n",
       "   'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.',\n",
       "   'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.',\n",
       "   'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.',\n",
       "   'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.',\n",
       "   'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.',\n",
       "   'This document provides guidelines for preparing the input dataset.',\n",
       "   \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\",\n",
       "   'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'],\n",
       "  4: ['The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.',\n",
       "   'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.',\n",
       "   'The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators for MPQ, and the module also offers a debug API for MPQ.',\n",
       "   'The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module to be optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component details any modifications applied to the optimized model.',\n",
       "   'The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.',\n",
       "   'The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support status and analyzes models at different levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section specifies data formats and simulation dataset paths. The performance estimator includes options for optimization and bit width factors. Lastly, the profiler section details profiling modes, targets, and device configurations.',\n",
       "   'The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.',\n",
       "   'For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.',\n",
       "   'The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels, utilizing a quantized CNNX model and a debug dictionary.',\n",
       "   'The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision.',\n",
       "   'The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, Dummy Convolution for the Concat Operator can be used during fixed or mixed precision quantization to maintain activation scales, potentially improving model performance.',\n",
       "   'EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.',\n",
       "   'The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantizer, simulator, and optimizer, detailing parameters like input shapes and device name. The custom template path is indicated for a specific template file.',\n",
       "   \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.\",\n",
       "   'Recommended specifications for a computer include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. The minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.',\n",
       "   'The converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets. Example code snippets demonstrate how to use these converters effectively.',\n",
       "   'The configuration values are utilized across different modules of elt.',\n",
       "   'The model needs to be prepared in ONNX format for optimization.',\n",
       "   \"The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located directly under the DATA folder created after running the 'enntools init' command.\",\n",
       "   'Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in the accompanying figure.',\n",
       "   'The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, evaluating performance before and after optimization, applying fixed precision quantization, evaluating performance post-quantization, and finally converting the CNNX model to SNC format.',\n",
       "   'EHT supports ONNX opset versions 13 to 17.',\n",
       "   'Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.',\n",
       "   'Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with three key features: shape_inference for modifying input shapes, 4D conversion to ensure operators have four-dimensional input and output shapes, and various optimization methods for efficiency. The 4D conversion process tracks channel axes during model deployment, which may increase processing time. Additionally, the Optimizer supports numerous optimization techniques, including Fold, GeGLU, GroupNorm, and more, to streamline model operations. Users can also create custom optimization methods using the Optimizer Template.',\n",
       "   \"This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization and launching steps based on the node's characteristics. The optimization method updates convolution node values using arithmetic node values and removes unnecessary nodes from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases and triggers operations based on specific input shapes. Overall, the framework aims to enhance model efficiency by optimizing node connections.\",\n",
       "   'The document outlines specifications for model optimization, including model types (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model evaluation, along with input data paths for inference. Additionally, it describes optimizer settings, including options for skipping dimension conversion, defining input shapes, and applying custom optimization templates.',\n",
       "   \"The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a container dataset directory using the 'exynos_ai_studio:0.1.0' image.\",\n",
       "   'The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.',\n",
       "   \"The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before performing a conversion using 'enntools'.\",\n",
       "   'Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating templates, preparing the model for optimization, and validating the optimized model.',\n",
       "   'To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands.',\n",
       "   'The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.',\n",
       "   'The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.',\n",
       "   'The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It allows for checking output and intermediate tensors during execution. Simulated quantization mimics quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models by their inference outputs or intermediate tensors. The comparison uses SNR as a metric, and example codes are provided for each feature.',\n",
       "   'This document provides guidelines for preparing the input dataset.',\n",
       "   \"The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information.\",\n",
       "   'The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).'],\n",
       "  5: ['perf_estimator : dict**\\n- **O2_enable : bool**: O2 optimization (true or false)\\n- **O2_fm_forwarding : bool**: feature-map forwarding (true or false)\\n- **SEG : bool**: Set true if input model is Deeplab V3+\\n- **SSD : bool**: Set true if input model is SSD detection\\n- **bit_width_factor_FM : int**: Select feature map bit width factor (1 or 2)\\n- **bit_width_factor_FP16 : bool**: Set bit width factor as floating point (true or false)\\n- **bit_width_factor_weight : int**: Select weight bit width factor (1 or 2)\\n- **core_num : int**: 1 for single core, 2 for instance-1\\n- **device : str**: Select device type [Gen-3, Gen-4, Gen-5, Gen-5a]\\n- **json_report : bool**: Enable report json format\\n- **nq_fold : bool**: Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32)  \\n---  \\n**'],\n",
       "  6: ['Optimizer is a tool that allows users to easily optimize their models. Users can add their own optimization methods using the \"Optimizer Template\" of Optimizer. The following process is required to apply the user-defined template to the user model.  \\n- Create custom templates.\\n- Prepare model to be optimized\\n- Validate optimized model'],\n",
       "  7: ['Example codes to use them:\\n```python\\nfrom converter import api\\ncnnx_to_snc_params = api.Cnnx2SncParameters(\\ninput_model_path = \"/path/to/model.onnx\",\\ninput_encodings_path = \"/path/to/model.encodings\",\\noutput_model_path = \"/output/path/for/model.snc\"\\n)\\n\\napi.Converter.cnnx_to_snc(cnnx_to_snc_params)\\n\\ntflite_to_onnx_params = api.TfLite2OnnxParameters(\\ninput_model_path = \"/path/to/model.tflite\",\\noutput_model_path = \"./output/path/for/model.onnx\",\\n)\\n\\napi.Converter.tflite_to_onnx(tflite_to_onnx_params)\\n```'],\n",
       "  8: ['simulator : dict**  \\n- **metric : string**: The metric to be used for measurement\\n- **threshold : float** : The threshold value of the metric that determines agreement / disagreement\\n- **input_data_path : string**: The path to the dataset for model inference  \\n---  \\n**'],\n",
       "  9: ['Currently supported advanced quantization methods are as follows.  \\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\\n- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\\n- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\\n- A method that tune the weights range of the channels in one tensor to reduce quantization error  \\n- Dummy Convolution for the Concat Operator\\n- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.']},\n",
       " 'response': {0: 'The context provided does not contain specific performance evaluations or comparisons of optimization techniques such as mixed precision quantization and static uniform quantization, particularly in relation to quantizers and optimizers. \\n\\nHowever, I can summarize their characteristics based on the information given:\\n\\n1. **Fixed Precision Quantization**:\\n   - Applies static uniform quantization to weights and activations.\\n   - Users specify a single precision (bit-width) for the entire model.\\n   - This method is straightforward but may not optimize performance as effectively as mixed precision methods.\\n\\n2. **Mixed Precision Quantization (MPQ)**:\\n   - Allows different parts of the model to utilize different precisions.\\n   - Two approaches: by name (specific activation/weight names) or by operator (different precisions for different operators).\\n   - Can lead to better performance by optimizing the precision of specific model components.\\n\\n3. **Optimizers**:\\n   - Aim to improve model performance on Exynos chips by modifying the architecture for efficiency.\\n   - Supports various optimization features such as folding, fusing, inserting, replacing, and changing attributes.\\n   - Provides shape inference and 4D conversion to ensure compatibility with the model deployment.\\n\\nIn general, mixed precision quantization might yield better performance in certain scenarios by more selectively applying quantization. In contrast, fixed precision quantization could be simpler but might not maximize efficiency. The optimizer further refines the model for better performance on target hardware, accommodating changes such as input shapes and dimensionality.\\n\\nFor detailed performance evaluations, testing these techniques in specific scenarios would provide more conclusive insights. If you have further questions or need specific details, feel free to ask!',\n",
       "  1: 'We do not have the information you requested. If you wish to contact support about this inquiry, please send an email to seed.ai@samsung.com.',\n",
       "  2: \"The provided summaries outline several model conversion and optimization techniques used in the ENN SDK, each with its unique processes and goals. Here's a comparison based on the outlined techniques:\\n\\n1. **ONNX-to-CNNX Conversion**: \\n   - This is the initial step in both the CV and LVM flows, where an opset16 ONNX model is converted into CNNX format, with the output stored in `{result_dir}/cnnx`. The effectiveness of this step is crucial as it prepares the model for subsequent optimization.\\n\\n2. **Optimization**:\\n   - In both CV and LVM workflows, the optimization phase involves simplifying the CNNX model. The CV flow uses a Simplifier and an Optimization template, while the LVM flow combines the Simplifier with a 4-Dimensional Conversion followed by an Optimization template. \\n   - The LLM flow focuses only on optimization and does not include CNNX-to-(Intermediate) conversion.\\n   - The effectiveness and efficiency can be deemed higher in LVM due to the enhanced complexity handled by the additional conversion step.\\n\\n3. **Performance Evaluation of Optimization**: \\n   - Each workflow includes a performance evaluation phase, which compares inference results before and after optimization, serving as a critical feedback mechanism for validating the optimization's effectiveness and efficiency. \\n\\n4. **Quantization**:\\n   - For quantization, the CV flow applies Fixed Precision Quantization, while the LVM flow applies more advanced techniques, including Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction. \\n   - The LVM flow's approach suggests it may yield better performance and fidelity in the quantization process since it employs multiple sophisticated techniques compared to the single approach in CV.\\n\\n5. **Performance Evaluation of Quantization**: \\n   - Similar to the optimization evaluation, this phase assesses the impact of quantization on model performance. Both workflows conduct these comparisons to ensure that quantization does not negatively affect the model's inference capabilities.\\n\\n6. **CNNX-to-SNC Conversion**: \\n   - Both workflows have a final conversion step to SNC format. The effectiveness of this step is essential for deploying the optimized and quantized model on target hardware. \\n\\nIn summary, the LVM approach provides a more comprehensive strategy with additional steps in optimization and quantization methodologies, suggesting potentially higher effectiveness and efficiency compared to the CV workflow. The performance evaluations in both models serve as a critical component to ensure that each step contributes positively to the overall performance of the model.\",\n",
       "  3: \"The performance evaluation techniques and optimization methods in model optimization involve several steps and tools, including the Quantizer and Optimizer modules, each focusing on areas such as quantization and mixed precision of models. Here's a comparison of the methodologies as highlighted in the provided context:\\n\\n### Performance Evaluation Techniques\\n1. **Performance Evaluation of Optimization**:\\n   - This is done before and after applying optimizations to compare inference results.\\n   - It assesses the impact of optimization on the model's performance.\\n\\n2. **Performance Evaluation of Quantization**:\\n   - Similar to the optimization evaluation, this step compares inference results before and after quantization.\\n   - It critically checks how quantization affects the model's accuracy and performance.\\n\\n### Optimization Methods\\n1. **Quantizer Module**:\\n   - Offers basic methods such as Fixed Precision Quantization and Mixed Precision Quantization (MPQ).\\n     - **Fixed Precision Quantization** involves static uniform quantization for the entire model.\\n     - **Mixed Precision Quantization** allows varying precisions for different parts of the model, either by activation names or operators. \\n   - Advanced quantization methods like Smooth Quant and Softmax Bias Correction are also available to improve performance post-quantization.\\n\\n2. **Optimizer Module**:\\n   - Aims to enhance models for performance on Exynos chips, allowing modifications such as shape inference and ensuring inputs/outputs conform to a four-dimensional shape required for operation.\\n   - It supports optimization templates that help in fine-tuning the model.\\n\\n### Comparison Across Reports\\n- **Quantization**:\\n  - The reports suggest applying Fixed Precision Quantization followed by evaluations, while the LVM flow involves advanced methods like Smooth Quant, Mixed Precision Quantization, and Softmax Bias Correction sequentially. This layered approach may yield better performance or resilience to quantization errors.\\n\\n- **Mixed Precision**:\\n  - Employed more flexibly in different contexts, the Quantizer allows for precise control over which activations or operators should use differing bit-widths, which may help reduce the model's overall size and improve speed without significantly sacrificing accuracy.\\n\\n- **Tools Used**:\\n  - The **Quantizer** focuses on quantization strategies and provides APIs for mixed precision debugging, which enhances model adaptability. On the other hand, the **Optimizer** deals with ensuring model compatibility with hardware and aligns the architecture to benefit from Exynos chip features. Both play pivotal roles but target different aspects of model optimization.\\n\\nIn summary, the choice and combination of optimization techniques across\",\n",
       "  4: 'We do not have the information you requested. If you wish to contact support about this inquiry, please send an email to seed.ai@samsung.com.',\n",
       "  5: 'To enable SSD detection in the `perf_estimator` configuration, you need to set the following parameter to true:\\n\\n- **SEG**: Set this to `false` (since it is specific to Deeplab V3+), but the SSD detection needs to have the **SSD** parameter set to true.\\n\\nHere’s a summary of the relevant parameters:\\n\\n```yaml\\nperf_estimator:\\n  SSD: true  # Enable SSD detection\\n  SEG: false # This is not applicable for SSD\\n``` \\n\\nMake sure to include any other relevant parameters as per your requirements.',\n",
       "  6: 'To apply user-defined optimization methods using the Optimizer Template, the following steps are required:\\n\\n1. **Create custom templates**: Define the custom optimization scenarios using the provided classes and methods.\\n2. **Prepare the model to be optimized**: Ensure that your model is structured appropriately for optimization.\\n3. **Validate the optimized model**: After applying the optimization, check that the model functions correctly and that the optimization was successful.\\n\\nThese steps allow you to customize how the model is optimized by using the components outlined in the Optimizer Template.',\n",
       "  7: 'Specifying the output model path in the conversion process of machine learning models is significant for several reasons:\\n\\n1. **Target Location**: It defines where the converted model will be saved. This is crucial for organizing model files and ensuring that they can be easily located and accessed later.\\n\\n2. **File Management**: It helps manage different versions or formats of models. By specifying different output paths for various conversions (e.g., from ONNX to CNNX or from CNNX to SNC), developers can keep track of their models and avoid overwriting existing files.\\n\\n3. **Integration with Workflows**: A designated output path allows for better integration with workflows and pipelines where subsequent processes may rely on the converted model files. Clear output paths enable easier automation and orchestration of tasks.\\n\\n4. **Error Handling**: Specifying the output path can facilitate debugging and troubleshooting. If a conversion fails, developers can quickly check the specified output location for any incomplete or corrupted files.\\n\\n5. **Documentation and Clarity**: It serves as documentation for the transformation taking place, clearly indicating the destination and format of the output model, which aids in understanding the flow of model processing.\\n\\nOverall, the output model path is a pivotal aspect of the model conversion process, ensuring efficient file management and facilitating later stages of model deployment and execution.',\n",
       "  8: \"The 'simulator' in the context of the provided dictionary refers to a component that conducts inference on a CNNX model, which includes an ONNX model file and a quantization information encodings file. It allows for checking output and intermediate tensors during execution and simulates quantized inference through a process known as simulated quantization. \\n\\nKey features of the simulator include:\\n- Conducting inference on CNNX models.\\n- Loading input data from specified datasets.\\n- Comparing the inference outputs of two CNNX models based on a defined metric (currently SNR).\\n- Attaining quantization simulation sessions for manual runs.\\n\\nOverall, the simulator aids in validating model behavior and performance under quantized conditions.\",\n",
       "  9: 'The purpose of Softmax Bias Correction in advanced quantization methods is to add a float bias to the Softmax output layer in order to reduce the performance degradation that can be caused by quantization. This approach aims to improve the accuracy of models where the Softmax layer is a critical component, particularly in scenarios where quantization might otherwise introduce significant errors. You can find more details in the Quantizer section of the documentation.'},\n",
       " 'reference': {0: 'The document compares optimization techniques, including mixed precision quantization and static uniform quantization, by outlining their performance evaluations and the use of various quantizers and optimizers.',\n",
       "  1: 'Quantization methods compare on performance and efficiency by applying various techniques such as Smooth Quantization, Mixed Precision Quantization, and advanced methods like Softmax Bias Correction and Cross Layer Equalization, which aim to optimize model performance while reducing quantization error.',\n",
       "  2: 'The various model conversion and optimization techniques, including ONNX-to-CNNX conversion, quantization methods, and performance evaluation strategies, are compared in their effectiveness and efficiency through a structured process that involves converting models, optimizing them, and evaluating performance before and after optimization.',\n",
       "  3: 'The document compares various performance evaluation techniques and optimization methods for model optimization, including quantization, mixed precision, and specific tools like the Quantizer and Optimizer. It outlines steps for converting and optimizing models, applying quantization methods, and evaluating performance before and after optimization.',\n",
       "  4: 'The various optimization techniques and quantization methods for neural networks are compared in terms of performance evaluation and model efficiency through a structured process that includes converting models, optimizing them, and evaluating performance before and after optimization.',\n",
       "  5: 'To enable SSD detection in the perf_estimator configuration, the parameter SSD needs to be set to true.',\n",
       "  6: 'The steps involved in applying user-defined optimization methods using the Optimizer Template include creating custom templates, preparing the model to be optimized, and validating the optimized model.',\n",
       "  7: 'Specifying the output model path in the conversion process of machine learning models is significant because it determines where the converted model will be saved, ensuring that the user can access and utilize the model after the conversion is complete.',\n",
       "  8: \"The purpose of the 'simulator' in the provided dictionary is to define a structure that includes a metric for measurement, a threshold value for determining agreement or disagreement, and a path to the dataset for model inference.\",\n",
       "  9: 'The purpose of Softmax Bias Correction in advanced quantization methods is to add a float bias to the Softmax output layer to reduce the performance degradation caused by quantization.'},\n",
       " 'context_recall': {0: 1.0,\n",
       "  1: 1.0,\n",
       "  2: 1.0,\n",
       "  3: 1.0,\n",
       "  4: 1.0,\n",
       "  5: 1.0,\n",
       "  6: 1.0,\n",
       "  7: 1.0,\n",
       "  8: 0.0,\n",
       "  9: 1.0},\n",
       " 'factual_correctness': {0: 0.0,\n",
       "  1: 0.0,\n",
       "  2: 0.0,\n",
       "  3: 0.72,\n",
       "  4: 0.0,\n",
       "  5: 0.5,\n",
       "  6: 0.57,\n",
       "  7: 0.33,\n",
       "  8: 0.12,\n",
       "  9: 0.57},\n",
       " 'faithfulness': {0: 0.7333333333333333,\n",
       "  1: 0.0,\n",
       "  2: 1.0,\n",
       "  3: 0.84375,\n",
       "  4: 0.0,\n",
       "  5: 0.3333333333333333,\n",
       "  6: 0.6666666666666666,\n",
       "  7: 1.0,\n",
       "  8: 0.8461538461538461,\n",
       "  9: 0.5}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.to_pandas().to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragtools-all-in-one",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
