{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chardet\n",
    "from langchain_text_splitters import CharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "def load_markdown(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        encoding = result['encoding']\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on\n",
    "    )\n",
    "\n",
    "    with open(data_path, 'rt', encoding=encoding) as file:\n",
    "        data_string = file.read()\n",
    "        documents = markdown_splitter.split_text(data_string)\n",
    "\n",
    "        # 파일명을 metadata에 추가\n",
    "        domain = data_path  # os.path.basename(data_path)\n",
    "        for doc in documents:\n",
    "            if not doc.metadata:\n",
    "                doc.metadata = {}\n",
    "            doc.metadata[\"domain\"] = domain  # Document 객체의 metadata 속성에 파일명 추가\n",
    "\n",
    "        return documents\n",
    "\n",
    "\n",
    "def load_txt(data_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        encoding = result['encoding']\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    with open(data_path, 'r', encoding=encoding) as file:\n",
    "        data_string = file.read().split(\"\\n\")\n",
    "        domain = data_path  # os.path.basename(data_path)\n",
    "        documents = text_splitter.create_documents(data_string)\n",
    "\n",
    "        for doc in documents:\n",
    "            if not doc.metadata:\n",
    "                doc.metadata = {}\n",
    "            doc.metadata[\"domain\"] = domain  # Document 객체의 metadata 속성에 파일명 추가\n",
    "\n",
    "        return documents\n",
    "\n",
    "\n",
    "def load_general(base_dir):\n",
    "    data = []\n",
    "    cnt = 0\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if \".txt\" in file:\n",
    "                cnt += 1\n",
    "                data += load_txt(os.path.join(root, file))\n",
    "\n",
    "    print(f\"the number of txt files is : {cnt}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_document(base_dir):\n",
    "    data = []\n",
    "    cnt = 0\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if \".md\" in file:\n",
    "                cnt += 1\n",
    "                data += load_markdown(os.path.join(root, file))\n",
    "\n",
    "    print(f\"the number of md files is : {cnt}\")\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_markdown_files(source_dir):\n",
    "    md_data = load_document(base_dir=source_dir)\n",
    "    text_data = load_general(base_dir=source_dir)\n",
    "\n",
    "    return md_data + text_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.synthesizers.abstract_query import ComparativeAbstractQuerySynthesizer\n",
    "# from ragas.testset.synthesizers.base import BaseSynthesizer\n",
    "# from ragas.testset.synthesizers.base_query import QuerySynthesizer\n",
    "from ragas.testset.synthesizers.specific_query import SpecificQuerySynthesizer\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "ragas_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "# critic_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "# embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "generator = TestsetGenerator.from_langchain(generator_llm)\n",
    "md_files = get_markdown_files(source_dir=\"/Users/yechanyun/YYC/work/project/AI_Application/rag/data/exynos-ai-studio-docs-main\")\n",
    "test_set = generator.generate_with_langchain_docs(md_files,\n",
    "                                                  testset_size=1,\n",
    "                                                  query_distribution=[\n",
    "                                                    (ComparativeAbstractQuerySynthesizer(llm=ragas_llm), 0.5),\n",
    "                                                    (SpecificQuerySynthesizer(llm=ragas_llm), 0.5),],\n",
    "                                                  with_debugging_logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = test_set.to_evaluation_dataset()\n",
    "evaluation_dataset\n",
    "print(evaluation_dataset.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset.to_jsonl(\"./evaluation_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.to_jsonl(\"./testset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsk_ragtools import text_gen as tg\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_jsonl('./evaluation_dataset.jsonl')\n",
    "evaluation_dataset_dict = evaluation_dataset.dict()\n",
    "\n",
    "for i, obj in enumerate(evaluation_dataset_dict[\"samples\"]):\n",
    "  print(\n",
    "    f\"\\rGenerating RAGAS json...({i+1}/{len(evaluation_dataset_dict['samples'])})\",\n",
    "    end=\"\",\n",
    "    flush=True,\n",
    "  )\n",
    "  response = tg.answer_question(obj[\"user_input\"])\n",
    "  obj.update({\"retrieved_contexts\": response[\"list\"], \"response\": response[\"response\"]})\n",
    "\n",
    "print(evaluation_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = EvaluationDataset.from_dict(mapping=evaluation_dataset_dict[\"samples\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas import evaluate\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "metrics = [LLMContextRecall(), FactualCorrectness(), Faithfulness()]\n",
    "metrics_names = [m.name for m in metrics]\n",
    "print(metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLMContextRecall(_required_columns={<MetricType.SINGLE_TURN: 'single_turn'>: {'retrieved_contexts', 'reference', 'user_input'}}, llm=None, name='context_recall', context_recall_prompt=ContextRecallClassificationPrompt(instruction=Given a context, and an answer, analyze each sentence in the answer and classify if the sentence can be attributed to the given context or not. Use only 'Yes' (1) or 'No' (0) as a binary classification. Output json with reason., examples=[(QCA(question='What can you tell me about albert Albert Einstein?', context=\"Albert Einstein (14 March 1879 - 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass-energy equivalence formula E = mc2, which arises from relativity theory, has been called 'the world's most famous equation'. He received the 1921 Nobel Prize in Physics 'for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect', a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.\", answer='Albert Einstein born in 14 March 1879 was  German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905.  Einstein moved to Switzerland in 1895'), ContextRecallClassifications(classifications=[ContextRecallClassification(statement='Albert Einstein, born on 14 March 1879, was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time.', reason='The date of birth of Einstein is mentioned clearly in the context.', attributed=1), ContextRecallClassification(statement='He received the 1921 Nobel Prize in Physics for his services to theoretical physics.', reason='The exact sentence is present in the given context.', attributed=1), ContextRecallClassification(statement='He published 4 papers in 1905.', reason='There is no mention about papers he wrote in the given context.', attributed=0), ContextRecallClassification(statement='Einstein moved to Switzerland in 1895.', reason='There is no supporting evidence for this in the given context.', attributed=0)]))], language=english), max_retries=1, _reproducibility=1), Faithfulness(_required_columns={<MetricType.SINGLE_TURN: 'single_turn'>: {'retrieved_contexts', 'response', 'user_input'}}, llm=None, name='faithfulness', nli_statements_message=NLIStatementPrompt(instruction=Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context., examples=[(NLIStatementInput(context='John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', statements=['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.']), NLIStatementOutput(statements=[StatementFaithfulnessAnswer(statement='John is majoring in Biology.', reason=\"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", verdict=0), StatementFaithfulnessAnswer(statement='John is taking a course on Artificial Intelligence.', reason='The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', verdict=0), StatementFaithfulnessAnswer(statement='John is a dedicated student.', reason='The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', verdict=1), StatementFaithfulnessAnswer(statement='John has a part-time job.', reason='There is no information given in the context about John having a part-time job.', verdict=0)])), (NLIStatementInput(context='Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', statements=['Albert Einstein was a genius.']), NLIStatementOutput(statements=[StatementFaithfulnessAnswer(statement='Albert Einstein was a genius.', reason='The context and statement are unrelated', verdict=0)]))], language=english), statement_prompt=LongFormAnswerPrompt(instruction=Given a question, an answer, and sentences from the answer analyze the complexity of each sentence given under 'sentences' and break down each sentence into one or more fully understandable statements while also ensuring no pronouns are used in each statement. Format the outputs in JSON., examples=[(FaithfulnessStatements(question='Who was Albert Einstein and what is he best known for?', answer='He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.', sentences={0: 'He was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time.', 1: 'He was best known for developing the theory of relativity, he also made important contributions to the development of the theory of quantum mechanics.'}), SentencesSimplified(sentences=[SentenceComponents(sentence_index=0, simpler_statements=['Albert Einstein was a German-born theoretical physicist.', 'Albert Einstein is recognized as one of the greatest and most influential physicists of all time.']), SentenceComponents(sentence_index=1, simpler_statements=['Albert Einstein was best known for developing the theory of relativity.', 'Albert Einstein also made important contributions to the development of the theory of quantum mechanics.'])]))], language=english), sentence_segmenter=<pysbd.segmenter.Segmenter object at 0x000001DE90956750>, max_retries=1, _reproducibility=1), FactualCorrectness(_required_columns={<MetricType.SINGLE_TURN: 'single_turn'>: {'response', 'reference'}}, llm=None, name='factual_correctness', mode='f1', atomicity='low', coverage='low', claim_decomposition_prompt=ClaimDecompositionPrompt(instruction=\n",
      "    Decompose and break down each of the input sentences into one or more standalone statements. Each statement should be a standalone claim that can be independently verified.\n",
      "    Follow the level of atomicity and coverage as shown in the examples.\n",
      "    , examples=[(ClaimDecompositionInput(response='Charles Babbage was a French mathematician, philosopher, and food critic.', sentences=['Charles Babbage was a French mathematician, philosopher, and food critic.']), ClaimDecompositionOutput(decomposed_claims=[['Charles Babbage was a mathematician and philosopher.']])), (ClaimDecompositionInput(response='Albert Einstein was a German theoretical physicist. He developed the theory of relativity and also contributed to the development of quantum mechanics.', sentences=['Albert Einstein was a German theoretical physicist.', 'He developed the theory of relativity and also contributed to the development of quantum mechanics.']), ClaimDecompositionOutput(decomposed_claims=[['Albert Einstein was a German physicist.'], ['Albert Einstein developed relativity and contributed to quantum mechanics.']]))], language=english), nli_prompt=NLIStatementPrompt(instruction=Your task is to judge the faithfulness of a series of statements based on a given context. For each statement you must return verdict as 1 if the statement can be directly inferred based on the context or 0 if the statement can not be directly inferred based on the context., examples=[(NLIStatementInput(context='John is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.', statements=['John is majoring in Biology.', 'John is taking a course on Artificial Intelligence.', 'John is a dedicated student.', 'John has a part-time job.']), NLIStatementOutput(statements=[StatementFaithfulnessAnswer(statement='John is majoring in Biology.', reason=\"John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.\", verdict=0), StatementFaithfulnessAnswer(statement='John is taking a course on Artificial Intelligence.', reason='The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI.', verdict=0), StatementFaithfulnessAnswer(statement='John is a dedicated student.', reason='The context states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication.', verdict=1), StatementFaithfulnessAnswer(statement='John has a part-time job.', reason='There is no information given in the context about John having a part-time job.', verdict=0)])), (NLIStatementInput(context='Photosynthesis is a process used by plants, algae, and certain bacteria to convert light energy into chemical energy.', statements=['Albert Einstein was a genius.']), NLIStatementOutput(statements=[StatementFaithfulnessAnswer(statement='Albert Einstein was a genius.', reason='The context and statement are unrelated', verdict=0)]))], language=english)), SemanticSimilarity(_required_columns={<MetricType.SINGLE_TURN: 'single_turn'>: {'response', 'reference'}}, embeddings=None, llm=None, name='semantic_similarity', is_cross_encoder=False, threshold=None)]\n",
      "<class 'ragas.metrics._context_recall.LLMContextRecall'>\n",
      "['context_recall', 'faithfulness', 'factual_correctness', 'semantic_similarity']\n"
     ]
    }
   ],
   "source": [
    "import ragas.metrics as ragas_metrics\n",
    "\n",
    "metrics_candidates = [\"LLMContextRecall\", \"Faithfulness\", \"FactualCorrectness\", \"SemanticSimilarity\"]\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for m in metrics_candidates:\n",
    "    metrics.append(getattr(ragas_metrics, m)())\n",
    "\n",
    "print(metrics)\n",
    "print(type(metrics[0]))\n",
    "metrics_names = [m.name for m in metrics]\n",
    "print(metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pandas().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\work\\project\\AI_Application\\rag\\source\\ragtools-all-in-one\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\work\\project\\AI_Application\\rag\\source\\ragtools-all-in-one\\Lib\\site-packages\\ragas\\prompt\\base.py:9: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.llms.prompt import PromptValue\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Header 1': 'System requirement', 'Header 2': 'Software', 'location': 'C:\\\\work\\\\project\\\\AI_Application\\\\rag\\\\data\\\\exynos-ai-studio-docs-main\\\\system_requirements\\\\system_requirements.md'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Samples: 100%|██████████| 2/2 [00:09<00:00,  4.69s/it]                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluationDataset(features=['eval_sample', 'synthesizer_name'], len=2)\n"
     ]
    }
   ],
   "source": [
    "from actions.ragas_testset_creator import RagasTestsetCreator\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "args = {\n",
    "    \"action\": \"createTestset\",\n",
    "    \"json_filename\": \".\\\\testset_with_context.json\",\n",
    "    \"csv_filename\": \"\",\n",
    "    \"basepath\": \"\",\n",
    "    \"gpt_model\": \"gpt-4o-mini\",\n",
    "    \"dataset_source_dir\": \"C:\\\\work\\\\project\\\\AI_Application\\\\rag\\\\data\\\\exynos-ai-studio-docs-main\",\n",
    "    \"testset_test_size\": 2,\n",
    "    \"testset_comparative_query_ratio\": 0.5,\n",
    "    \"testset_specific_query_ratio\": 0.5,\n",
    "    \"testset_filename\": \"testset.json\",\n",
    "    \"eval_result_filename\": \".\\\\eval_result.json\"\n",
    "}\n",
    "\n",
    "\n",
    "ragasTestsetCreator = RagasTestsetCreator(logger)\n",
    "test_set, generator = ragasTestsetCreator.main(args[\"dataset_source_dir\"],\n",
    "                            args[\"testset_test_size\"],\n",
    "                            args[\"testset_comparative_query_ratio\"],\n",
    "                            args[\"testset_specific_query_ratio\"],\n",
    "                            args[\"gpt_model\"],\n",
    "                            args[\"testset_filename\"])\n",
    "\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestsetSample(eval_sample=SingleTurnSample(user_input='What role does the NPU Simulator play in the ELT framework?', retrieved_contexts=None, reference_contexts=['ELT](elt/elt.md)\\n- [Model Analyzer](elt/model_analyzer/model_analyzer.md)\\n- [SCVT](elt/scvt/scvt.md)\\n- [Quantizer](elt/quantizer/quantizer.md)\\n- [NPU Simulator](elt/npu_simulator/npu_simulator.md)\\n- [Performance Estimator](elt/performance_estimator/performance_estimator.md)\\n- [Graph Generator](elt/graph_generator/graph_generator.md)\\n- ['], response=None, multi_responses=None, reference='The NPU Simulator plays a role in the ELT framework, although the specific details of its function are not provided in the context.', rubric=None), synthesizer_name='SpecificQuerySynthesizer')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.samples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Node(id: f9e4f4, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: dd8576, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'title', 'embedding', 'keyphrases']),\n",
       " Node(id: 089a7d, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: 0f397a, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: 4481ee, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: 9a009c, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: 608e72, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: ce8254, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: 30949a, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 36a0a1, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: b8f2ed, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: c4cf6f, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: 3bceec, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding', 'title', 'embedding', 'keyphrases']),\n",
       " Node(id: 0ef2f1, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: 10c6d2, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: 73d72f, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: d0e243, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'keyphrases', 'title', 'embedding']),\n",
       " Node(id: 49b76a, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: 1b5241, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'embedding', 'keyphrases', 'title']),\n",
       " Node(id: 3b970c, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding', 'embedding', 'keyphrases', 'title']),\n",
       " Node(id: 7d2dc6, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: 22a80e, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding', 'title', 'embedding', 'keyphrases']),\n",
       " Node(id: 592ff4, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: 08571b, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'title', 'embedding', 'keyphrases']),\n",
       " Node(id: 5461f5, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 6602b6, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 6530d3, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 89a6b3, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 636204, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: bacf38, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 26451e, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 4d50f3, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: b1d4fe, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: c91e52, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 993a5b, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 97f751, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'summary', 'headlines', 'summary_embedding', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: a93fff, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: e6d48c, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: acdffa, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: c3335a, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 502e6e, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 5233b0, type: NodeType.CHUNK, properties: ['page_content', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: e2383b, type: NodeType.CHUNK, properties: ['page_content', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: b50d58, type: NodeType.CHUNK, properties: ['page_content', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: 68b8c4, type: NodeType.CHUNK, properties: ['page_content', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: d08d3e, type: NodeType.CHUNK, properties: ['page_content', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: afb4d0, type: NodeType.CHUNK, properties: ['page_content', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: 233c1b, type: NodeType.CHUNK, properties: ['page_content', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: dcb708, type: NodeType.CHUNK, properties: ['page_content', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: 8a9f35, type: NodeType.CHUNK, properties: ['page_content', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: 5e71e8, type: NodeType.CHUNK, properties: ['page_content', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: 80e195, type: NodeType.CHUNK, properties: ['page_content', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: 5e8735, type: NodeType.CHUNK, properties: ['page_content', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: 87e8fb, type: NodeType.CHUNK, properties: ['page_content', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: 7880f8, type: NodeType.CHUNK, properties: ['page_content', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: a33363, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 75495c, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 92e4de, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: b188e6, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 080a52, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: de8467, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: d949ac, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 32c364, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 7e0ed6, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 04fed2, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 0c2e25, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 9fa5f2, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: f2fdc2, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 9e24e5, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 3c04a8, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: c08e6e, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 8c0208, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 192f19, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 0ae3ac, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 0dc25f, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: afb931, type: NodeType.CHUNK, properties: ['page_content', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: 48afec, type: NodeType.CHUNK, properties: ['page_content', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: a86678, type: NodeType.CHUNK, properties: ['page_content', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: 136111, type: NodeType.CHUNK, properties: ['page_content', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: 599bd3, type: NodeType.CHUNK, properties: ['page_content', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: e90f4d, type: NodeType.CHUNK, properties: ['page_content', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: 71f2e3, type: NodeType.CHUNK, properties: ['page_content', 'embedding', 'title', 'keyphrases']),\n",
       " Node(id: fbb56a, type: NodeType.CHUNK, properties: ['page_content', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: bb4e11, type: NodeType.CHUNK, properties: ['page_content', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: f7b843, type: NodeType.CHUNK, properties: ['page_content', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: f945a5, type: NodeType.CHUNK, properties: ['page_content', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: fd17b4, type: NodeType.CHUNK, properties: ['page_content', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: 3b36b8, type: NodeType.CHUNK, properties: ['page_content', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: c31a9c, type: NodeType.CHUNK, properties: ['page_content', 'keyphrases', 'embedding', 'title']),\n",
       " Node(id: d16f6d, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: ae47d5, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: da9d98, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 08d46b, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: f7bb69, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding']),\n",
       " Node(id: 38795a, type: NodeType.CHUNK, properties: ['page_content', 'title', 'keyphrases', 'embedding'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.knowledge_graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.78999999999999\n",
      "0.933641975308642\n",
      "0.7017592592592592\n",
      "0.8935446048146275\n",
      "0.9601230498780658\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\".\\\\eval_result_241018.json\", \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "context_recall = 0\n",
    "factual_correctness = 0\n",
    "faithfulness = 0\n",
    "semantic_similarity = 0\n",
    "for i in json_data[\"context_recall\"].keys():\n",
    "    context_recall += json_data[\"context_recall\"][f\"{i}\"]\n",
    "for i in json_data[\"factual_correctness\"].keys():\n",
    "    factual_correctness += json_data[\"factual_correctness\"][f\"{i}\"]\n",
    "for i in json_data[\"faithfulness\"].keys():\n",
    "    faithfulness += json_data[\"faithfulness\"][f\"{i}\"]\n",
    "for i in json_data[\"semantic_similarity\"].keys():\n",
    "    semantic_similarity += json_data[\"semantic_similarity\"][f\"{i}\"]\n",
    "\n",
    "print(factual_correctness)\n",
    "print(context_recall/len(json_data[\"context_recall\"]))\n",
    "print(factual_correctness/len(json_data[\"factual_correctness\"]))\n",
    "print(faithfulness/len(json_data[\"faithfulness\"]))\n",
    "print(semantic_similarity/len(json_data[\"semantic_similarity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from trex_ai_chatbot_tools import text_gen as tg\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def testModerationApi(question):\n",
    "    start_time = time.time()\n",
    "    response = client.moderations.create(\n",
    "        model=\"omni-moderation-latest\",\n",
    "        input=question\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    time_spent = end_time - start_time\n",
    "\n",
    "    print(response.dict())\n",
    "    print(f\"Time spent: {time_spent:.2f} seconds\")\n",
    "\n",
    "    return time_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I would like to know the device specifications provided by 1st AI Challenger.']\n",
      "{'id': 'modr-0e50f7951f3d167457d09f89f0669c35', 'model': 'omni-moderation-latest', 'results': [{'categories': {'harassment': False, 'harassment_threatening': False, 'hate': False, 'hate_threatening': False, 'illicit': False, 'illicit_violent': False, 'self_harm': False, 'self_harm_instructions': False, 'self_harm_intent': False, 'sexual': False, 'sexual_minors': False, 'violence': False, 'violence_graphic': False, 'harassment/threatening': False, 'hate/threatening': False, 'illicit/violent': False, 'self-harm/intent': False, 'self-harm/instructions': False, 'self-harm': False, 'sexual/minors': False, 'violence/graphic': False}, 'category_applied_input_types': {'harassment': ['text'], 'harassment_threatening': ['text'], 'hate': ['text'], 'hate_threatening': ['text'], 'illicit': ['text'], 'illicit_violent': ['text'], 'self_harm': ['text'], 'self_harm_instructions': ['text'], 'self_harm_intent': ['text'], 'sexual': ['text'], 'sexual_minors': ['text'], 'violence': ['text'], 'violence_graphic': ['text'], 'harassment/threatening': ['text'], 'hate/threatening': ['text'], 'illicit/violent': ['text'], 'self-harm/intent': ['text'], 'self-harm/instructions': ['text'], 'self-harm': ['text'], 'sexual/minors': ['text'], 'violence/graphic': ['text']}, 'category_scores': {'harassment': 1.3765463368900109e-05, 'harassment_threatening': 3.6478537675800286e-06, 'hate': 5.144221374220898e-06, 'hate_threatening': 1.1125607488784412e-06, 'illicit': 0.007646249365563964, 'illicit_violent': 3.250358452365473e-05, 'self_harm': 6.605214485464791e-06, 'self_harm_instructions': 2.0785170435063726e-06, 'self_harm_intent': 3.024195853395102e-06, 'sexual': 2.3782205064034188e-05, 'sexual_minors': 7.967300986103147e-06, 'violence': 2.0988308820313112e-05, 'violence_graphic': 5.649793328376294e-06, 'harassment/threatening': 3.6478537675800286e-06, 'hate/threatening': 1.1125607488784412e-06, 'illicit/violent': 3.250358452365473e-05, 'self-harm/intent': 3.024195853395102e-06, 'self-harm/instructions': 2.0785170435063726e-06, 'self-harm': 6.605214485464791e-06, 'sexual/minors': 7.967300986103147e-06, 'violence/graphic': 5.649793328376294e-06}, 'flagged': False}]}\n",
      "Time spent: 0.41 seconds\n",
      "Average Time Spent : 0.41 seconds\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "json_file_path = \"C:\\\\work\\\\project\\\\AI_Application\\\\rag\\\\source\\\\ragas_evaluation\\\\241101\\\\testset_241018_with_contexts_v0.2.1.json\"\n",
    "with open(json_file_path, \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "random_testset_sample = random.sample(json_data[\"samples\"], 1)\n",
    "\n",
    "queries = [sample[\"user_input\"] for sample in random_testset_sample]\n",
    "print(queries)\n",
    "\n",
    "total_timespent = 0\n",
    "\n",
    "for q in queries:\n",
    "    total_timespent += testModerationApi(q)\n",
    "\n",
    "average_timespent = total_timespent / len(queries)\n",
    "print(f\"Average Time Spent : {average_timespent:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have enough knowledge about Edge AI, but I want to do a project using the Exynos AI Studio.\n"
     ]
    }
   ],
   "source": [
    "print(queries[24])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragtools-all-in-one",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
