{
    "user_input": {
        "0": "How do optimization techniques and their evaluations compare in reports on floating-point hardware and quantization?",
        "1": "How do quantization methods and optimization techniques compare in performance and efficiency?",
        "2": "How do the various model conversion techniques and optimization methods compare in terms of performance and efficiency across different hardware platforms, specifically focusing on the use of floating-point hardware, the Optimizer, and advanced quantization methods such as SmoothQuant and Softmax Bias Correction?",
        "3": "How do the performance evaluation metrics and optimization techniques compare across different reports for models utilizing floating-point hardware, various optimizers, and advanced quantization methods?",
        "4": "comparison AMD Ryzen 7 Intel Core i7 hardware specifications optimization techniques performance floating-point quantization Optimizer",
        "5": "What is the significance of the default path {workspace}/DATA/db_spec.yaml in the context of database generation?",
        "6": "What is the purpose of the 'userdb' parameter in the simulator configuration?",
        "7": "What are the different optimization methods used for each model type mentioned in the text?",
        "8": "What is the purpose of the 'check op support status' feature in the model_analyzer?",
        "9": "What are the different System on Chip types mentioned in the model_analyzer documentation?"
    },
    "retrieved_contexts": {
        "0": [
            "Section: Quantizer\n# Quantizer\n## Advanced Quantization Methods\nContent:\nCurrently supported advanced quantization methods are as follows.\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\n\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\n\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\n\t- A method that tune the weights range of the channels in one tensor to reduce quantization error",
            "Section: Quantizer\n# Quantizer\n## Basic Quantization Methods\nContent:\n### Fixed Precision Quantization\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision\n### Mixed Precision Quantization\nAllows different parts of the model to use different precisions<br>\nTwo approaches are supported:\n- Mixed precision by name: Users specify precisions for specific activation or weight names\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4",
            "Section: # Quantizer\n## Basic Quantization Methods\nContent:\n### Fixed Precision Quantization\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision.\n### Mixed Precision Quantization\nAllows different parts of the model to use different precisions<br>\nTwo approaches are supported:\n- Mixed precision by name: Users specify precisions for specific activation or weight names\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4",
            "Section: # Quantizer\nContent:\n## Advanced Quantization Methods\nCurrently supported advanced quantization methods are as follows.\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\n\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\n\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\n\t- A method that tune the weights range of the channels in one tensor to reduce quantization error\n- Dummy Convolution for the Concat Operator\n\t- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.\n## Debug API\n### Layer-wise mixed precision quantiztion debug API\nThe quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary.",
            "Section: Optimizer\n# Optimizer\nContent:\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\n- shape_inference<br>\n\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion<br>\n\tIn order for the .nnc extension model to operate on a device, the input and output shapes of all operators in the model must be in four dimensions.",
            "Section: # Optimizer\nContent:\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\n- shape_inference<br>\n\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion<br>\n\tTo deploy a model with a .nnc extension on a device, all operators must have input and output shapes with four dimensions. To facilitate this, the 4D conversion process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. ",
            "Section: # Quantizer\nContent:\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API.",
            "Section: Quantizer\n# Quantizer\nContent:\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization. Users can apply mixed precision quantization to models by specifying activation names or operators.",
            "Section: # Simulator\nContent:\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\nSimulator includes the following features(with example codes):",
            "Section: Explanation of model optimization flow\n# Model Optimization Flow\n## LVM\nContent:\n1. **ONNX-to-CNNX Conversion**\n\tConverts an opset16 ONNX model to CNNX format.\n\toutput_path : `{result_dir}/cnnx`\n2. **Optimization**\n\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\n\toutput_path : `{result_dir}/optimized`\n3. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n4. **Quantization**\n\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\n\toutput_path :\n\t`{result_dir}/smoothquant`\n\t`{result_dir}/mixed_precision_quant`\n\t`{result_dir}/softmax_bias_correction`\n5. **Performance Evaluation of Quantization**\n\tCompares the inference results of the model before and after quantization.\n6. **CNNX-to-SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`"
        ],
        "1": [
            "Section: Quantizer\n# Quantizer\n## Advanced Quantization Methods\nContent:\nCurrently supported advanced quantization methods are as follows.\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\n\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\n\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\n\t- A method that tune the weights range of the channels in one tensor to reduce quantization error",
            "Section: # Quantizer\nContent:\n## Advanced Quantization Methods\nCurrently supported advanced quantization methods are as follows.\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\n\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\n\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\n\t- A method that tune the weights range of the channels in one tensor to reduce quantization error\n- Dummy Convolution for the Concat Operator\n\t- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.\n## Debug API\n### Layer-wise mixed precision quantiztion debug API\nThe quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary.",
            "Section: Quantizer\n# Quantizer\n## Basic Quantization Methods\nContent:\n### Fixed Precision Quantization\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision\n### Mixed Precision Quantization\nAllows different parts of the model to use different precisions<br>\nTwo approaches are supported:\n- Mixed precision by name: Users specify precisions for specific activation or weight names\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4",
            "Section: # Quantizer\n## Basic Quantization Methods\nContent:\n### Fixed Precision Quantization\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision.\n### Mixed Precision Quantization\nAllows different parts of the model to use different precisions<br>\nTwo approaches are supported:\n- Mixed precision by name: Users specify precisions for specific activation or weight names\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4",
            "Section: # Quantizer\nContent:\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API.",
            "Section: Quantizer\n# Quantizer\nContent:\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization. Users can apply mixed precision quantization to models by specifying activation names or operators.",
            "Section: Explanation of model optimization flow\n# Model Optimization Flow\n## LVM\nContent:\n1. **ONNX-to-CNNX Conversion**\n\tConverts an opset16 ONNX model to CNNX format.\n\toutput_path : `{result_dir}/cnnx`\n2. **Optimization**\n\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\n\toutput_path : `{result_dir}/optimized`\n3. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n4. **Quantization**\n\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\n\toutput_path :\n\t`{result_dir}/smoothquant`\n\t`{result_dir}/mixed_precision_quant`\n\t`{result_dir}/softmax_bias_correction`\n5. **Performance Evaluation of Quantization**\n\tCompares the inference results of the model before and after quantization.\n6. **CNNX-to-SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`",
            "Section: # Model Optimization Flow\n## LVM\nContent:\n1. **ONNX-to-CNNX Conversion**\n\tConverts an opset16 ONNX model to CNNX format.\n\toutput_path : `{result_dir}/cnnx`\n2. **Optimization**\n\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\n\toutput_path : `{result_dir}/optimized`\n3. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n4. **Quantization**\n\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\n\toutput_path :\n\t`{result_dir}/smoothquant`\n\t`{result_dir}/mixed_precision_quant`\n\t`{result_dir}/softmax_bias_correction`\n5. **Performance Evaluation of Quantization**\n\tCompares the inference results of the model before and after quantization.\n6. **CNNX-to-SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`",
            "Section: # Model Optimization Flow\n## CV\nContent:\n1. **ONNX-to-CNNX Conversion**\n\tConverts an opset16 ONNX model to CNNX format.\n\toutput_path : `{result_dir}/cnnx`\n2. **Optimization**\n\tPasses the CNNX model through Simplifier and applies an Optimization template.\n\toutput_path : `{result_dir}/optimized`\n3. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n4. **Quantization**\n\tApplies Fixed Precision Quantization to the optimized CNNX model.\n\toutput_path : `{result_dir}/quantized`\n5. **Performance Evaluation of Quantization**\n\tCompares the inference results of the model before and after quantization.\n6. **CNNX-to-SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`",
            "Section: Explanation of model optimization flow\n# Model Optimization Flow\n## CV\nContent:\n1. **ONNX-to-CNNX Conversion**\n\tConverts an opset16 ONNX model to CNNX format.\n\toutput_path : `{result_dir}/cnnx`\n2. **Optimization**\n\tPasses the CNNX model through Simplifier and applies an Optimization template.\n\toutput_path : `{result_dir}/optimized`\n3. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n4. **Quantization**\n\tApplies Fixed Precision Quantization to the optimized CNNX model.\n\toutput_path : `{result_dir}/quantized`\n5. **Performance Evaluation of Quantization**\n\tCompares the inference results of the model before and after quantization.\n6. **CNNX-to-SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`"
        ],
        "2": [
            "Section: Quantizer\n# Quantizer\n## Advanced Quantization Methods\nContent:\nCurrently supported advanced quantization methods are as follows.\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\n\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\n\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\n\t- A method that tune the weights range of the channels in one tensor to reduce quantization error",
            "Section: # Quantizer\nContent:\n## Advanced Quantization Methods\nCurrently supported advanced quantization methods are as follows.\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\n\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\n\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\n\t- A method that tune the weights range of the channels in one tensor to reduce quantization error\n- Dummy Convolution for the Concat Operator\n\t- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.\n## Debug API\n### Layer-wise mixed precision quantiztion debug API\nThe quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary.",
            "Section: # Optimizer\nContent:\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\n- shape_inference<br>\n\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion<br>\n\tTo deploy a model with a .nnc extension on a device, all operators must have input and output shapes with four dimensions. To facilitate this, the 4D conversion process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. ",
            "Section: Optimizer\n# Optimizer\nContent:\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\n- shape_inference<br>\n\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion<br>\n\tIn order for the .nnc extension model to operate on a device, the input and output shapes of all operators in the model must be in four dimensions.",
            "Section: Quantizer\n# Quantizer\n## Basic Quantization Methods\nContent:\n### Fixed Precision Quantization\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision\n### Mixed Precision Quantization\nAllows different parts of the model to use different precisions<br>\nTwo approaches are supported:\n- Mixed precision by name: Users specify precisions for specific activation or weight names\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4",
            "Section: # Quantizer\n## Basic Quantization Methods\nContent:\n### Fixed Precision Quantization\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision.\n### Mixed Precision Quantization\nAllows different parts of the model to use different precisions<br>\nTwo approaches are supported:\n- Mixed precision by name: Users specify precisions for specific activation or weight names\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4",
            "Section: # Model Optimization Flow\n## LVM\nContent:\n1. **ONNX-to-CNNX Conversion**\n\tConverts an opset16 ONNX model to CNNX format.\n\toutput_path : `{result_dir}/cnnx`\n2. **Optimization**\n\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\n\toutput_path : `{result_dir}/optimized`\n3. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n4. **Quantization**\n\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\n\toutput_path :\n\t`{result_dir}/smoothquant`\n\t`{result_dir}/mixed_precision_quant`\n\t`{result_dir}/softmax_bias_correction`\n5. **Performance Evaluation of Quantization**\n\tCompares the inference results of the model before and after quantization.\n6. **CNNX-to-SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`",
            "Section: Optimizer\n# Optimizer\nContent:\n- support optimization method.<br>\n\tOptimizer supports many optimization features so that the model works efficiently on the device.\n\t- Fold\n\t\t- GeGLU\n\t\t- GroupNorm\n\t\t- LayerNorm\n\t\t- PReLU\n\t\t- RMSNorm\n\t- Fuse\n\t\t- SiLU\n\t\t- BatchNorm into Convolution\n\t\t- Cast\n\t\t- Deconvolution bias\n\t\t- Math\n\t- Insert\n\t\t- Depthwise Convolution for activation\n\t- Replace\n\t\t- Average Pooling to Depthwise convolution\n\t\t- Eltwise concat convolution\n\t\t- Convolution kernel 1 to 3\n\t\t- Matrix multiplication to dynamic convolution\n\t\t- ReduceMean to Global average pool\n\t\t- ReduceSum to Convolution\n\t\t- Slice to Split\n\t- Change attribute\n\t\t- axis of softmax\nIn addition, Optimizer Template, one of the Optimizer's powerful features, supports custom optimization methods that users have.",
            "Section: Explanation of model optimization flow\n# Model Optimization Flow\n## LVM\nContent:\n1. **ONNX-to-CNNX Conversion**\n\tConverts an opset16 ONNX model to CNNX format.\n\toutput_path : `{result_dir}/cnnx`\n2. **Optimization**\n\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\n\toutput_path : `{result_dir}/optimized`\n3. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n4. **Quantization**\n\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\n\toutput_path :\n\t`{result_dir}/smoothquant`\n\t`{result_dir}/mixed_precision_quant`\n\t`{result_dir}/softmax_bias_correction`\n5. **Performance Evaluation of Quantization**\n\tCompares the inference results of the model before and after quantization.\n6. **CNNX-to-SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`",
            "Section: # Optimizer\nContent:\nThe channel axes can be determined by certain layers, such as Convolutional and Linear layers, which have fixed channel axes at positions 1 and -1, respectively. However, they can be modified by operators like MatMul and Reshape, which can expand or reduce them. To account for these changes, the conversion process tracks channel axes in both the forward and backward pass. As a result, the 4D conversion process may require additional time to complete.\n- support optimization method.<br>\n\tOptimizer supports many optimization features so that the model works efficiently on the device.\n\t- Fold\n\t\t- GeGLU\n\t\t- GeLU\n\t\t- GroupNorm\n\t\t- LayerNorm\n\t\t- PReLU\n\t\t- RMSNorm\n\t\t- SiLU"
        ],
        "3": [
            "Section: Optimizer\n# Optimizer\nContent:\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\n- shape_inference<br>\n\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion<br>\n\tIn order for the .nnc extension model to operate on a device, the input and output shapes of all operators in the model must be in four dimensions.",
            "Section: # Optimizer\nContent:\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\n- shape_inference<br>\n\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion<br>\n\tTo deploy a model with a .nnc extension on a device, all operators must have input and output shapes with four dimensions. To facilitate this, the 4D conversion process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. ",
            "Section: # Quantizer\nContent:\n## Advanced Quantization Methods\nCurrently supported advanced quantization methods are as follows.\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\n\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\n\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\n\t- A method that tune the weights range of the channels in one tensor to reduce quantization error\n- Dummy Convolution for the Concat Operator\n\t- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.\n## Debug API\n### Layer-wise mixed precision quantiztion debug API\nThe quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary.",
            "Section: Quantizer\n# Quantizer\n## Advanced Quantization Methods\nContent:\nCurrently supported advanced quantization methods are as follows.\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\n\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\n\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\n\t- A method that tune the weights range of the channels in one tensor to reduce quantization error",
            "Section: # Quantizer\n## Basic Quantization Methods\nContent:\n### Fixed Precision Quantization\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision.\n### Mixed Precision Quantization\nAllows different parts of the model to use different precisions<br>\nTwo approaches are supported:\n- Mixed precision by name: Users specify precisions for specific activation or weight names\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4",
            "Section: Quantizer\n# Quantizer\n## Basic Quantization Methods\nContent:\n### Fixed Precision Quantization\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision\n### Mixed Precision Quantization\nAllows different parts of the model to use different precisions<br>\nTwo approaches are supported:\n- Mixed precision by name: Users specify precisions for specific activation or weight names\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4",
            "Section: # Model Optimization Flow\n## LVM\nContent:\n1. **ONNX-to-CNNX Conversion**\n\tConverts an opset16 ONNX model to CNNX format.\n\toutput_path : `{result_dir}/cnnx`\n2. **Optimization**\n\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\n\toutput_path : `{result_dir}/optimized`\n3. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n4. **Quantization**\n\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\n\toutput_path :\n\t`{result_dir}/smoothquant`\n\t`{result_dir}/mixed_precision_quant`\n\t`{result_dir}/softmax_bias_correction`\n5. **Performance Evaluation of Quantization**\n\tCompares the inference results of the model before and after quantization.\n6. **CNNX-to-SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`",
            "Section: Explanation of model optimization flow\n# Model Optimization Flow\n## LVM\nContent:\n1. **ONNX-to-CNNX Conversion**\n\tConverts an opset16 ONNX model to CNNX format.\n\toutput_path : `{result_dir}/cnnx`\n2. **Optimization**\n\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\n\toutput_path : `{result_dir}/optimized`\n3. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n4. **Quantization**\n\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\n\toutput_path :\n\t`{result_dir}/smoothquant`\n\t`{result_dir}/mixed_precision_quant`\n\t`{result_dir}/softmax_bias_correction`\n5. **Performance Evaluation of Quantization**\n\tCompares the inference results of the model before and after quantization.\n6. **CNNX-to-SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`",
            "Section: # Model Optimization Flow\n## CV\nContent:\n1. **ONNX-to-CNNX Conversion**\n\tConverts an opset16 ONNX model to CNNX format.\n\toutput_path : `{result_dir}/cnnx`\n2. **Optimization**\n\tPasses the CNNX model through Simplifier and applies an Optimization template.\n\toutput_path : `{result_dir}/optimized`\n3. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n4. **Quantization**\n\tApplies Fixed Precision Quantization to the optimized CNNX model.\n\toutput_path : `{result_dir}/quantized`\n5. **Performance Evaluation of Quantization**\n\tCompares the inference results of the model before and after quantization.\n6. **CNNX-to-SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`",
            "Section: Explanation of model optimization flow\n# Model Optimization Flow\n## CV\nContent:\n1. **ONNX-to-CNNX Conversion**\n\tConverts an opset16 ONNX model to CNNX format.\n\toutput_path : `{result_dir}/cnnx`\n2. **Optimization**\n\tPasses the CNNX model through Simplifier and applies an Optimization template.\n\toutput_path : `{result_dir}/optimized`\n3. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n4. **Quantization**\n\tApplies Fixed Precision Quantization to the optimized CNNX model.\n\toutput_path : `{result_dir}/quantized`\n5. **Performance Evaluation of Quantization**\n\tCompares the inference results of the model before and after quantization.\n6. **CNNX-to-SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`"
        ],
        "4": [
            "Section: Optimizer\n# Optimizer\nContent:\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\n- shape_inference<br>\n\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion<br>\n\tIn order for the .nnc extension model to operate on a device, the input and output shapes of all operators in the model must be in four dimensions.",
            "Section: # Optimizer\nContent:\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\n- shape_inference<br>\n\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion<br>\n\tTo deploy a model with a .nnc extension on a device, all operators must have input and output shapes with four dimensions. To facilitate this, the 4D conversion process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. ",
            "Section: Quantizer\n# Quantizer\n## Advanced Quantization Methods\nContent:\nCurrently supported advanced quantization methods are as follows.\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\n\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\n\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\n\t- A method that tune the weights range of the channels in one tensor to reduce quantization error",
            "Section: Quantizer\n# Quantizer\n## Basic Quantization Methods\nContent:\n### Fixed Precision Quantization\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision\n### Mixed Precision Quantization\nAllows different parts of the model to use different precisions<br>\nTwo approaches are supported:\n- Mixed precision by name: Users specify precisions for specific activation or weight names\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4",
            "Section: # Quantizer\n## Basic Quantization Methods\nContent:\n### Fixed Precision Quantization\nApplies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision.\n### Mixed Precision Quantization\nAllows different parts of the model to use different precisions<br>\nTwo approaches are supported:\n- Mixed precision by name: Users specify precisions for specific activation or weight names\n- Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4",
            "Section: # Quantizer\nContent:\n## Advanced Quantization Methods\nCurrently supported advanced quantization methods are as follows.\n- Softmax Bias Correction (https://arxiv.org/abs/2309.01729)\n\t- A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization\n- SmoothQuant (https://arxiv.org/abs/2211.10438)\n\t- A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation\n- Cross Layer Equalization (https://arxiv.org/abs/1906.04721)\n\t- A method that tune the weights range of the channels in one tensor to reduce quantization error\n- Dummy Convolution for the Concat Operator\n\t- The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved.\n## Debug API\n### Layer-wise mixed precision quantiztion debug API\nThe quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary.",
            "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
            "Section: # System requirement\nContent:\n## Hardware\n### CPU\n- Recommended specifications:\n\t- Latest AMD Ryzen 7 or Intel Core i7 processor with 4 cores or more\n\t- Architecture: 64-bit (x86-64)\n- Minimum specifications:\n\t- 2GHz dual-core processor\n\t- Architecture: 64-bit (x86-64)\n### Memory\n- Minimum 8GB RAM (16GB or more recommended)\n- Sufficient disk space (minimum 100GB recommended)\n## Software\n- OS : Linux (based on Ubuntu 22.04)\n- NVIDIA driver version : 450.80.02 or later\n- Docker : 19.03 or later (with NVIDIA Container Toolkit support)\n- NVIDIA Container Toolkit (nvidia-docker2)",
            "Section: # Quantizer\nContent:\nQuantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API.",
            "Section: # Simulator\nContent:\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\nSimulator includes the following features(with example codes):"
        ],
        "5": [
            "Section: # **How to use**\n## **Detailed explanation for elt yaml file**\nContent:\nmodel_analyzer : dict**\n- **check : bool** : Check the op support status\n- **device : str** : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6]\n- **level : int** : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint)\n- **snc_input : bool** : analyze the snc model, false: analyze the original model\n\n**database_gen : dict**\n- **database_spec : str** : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml)\n\n**converter : dict**\n- **device : str** : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7]",
            "Section: # **How to use**\n## **Detailed explanation for elt yaml file**\nContent:\n- **skip_old_snc_optimizer : bool** : true, skip old snc optimizer in old2new\n- **snc_converter : bool** : True, convert old snc to new snc, set it to false when input is new snc\n- **test_vector_gen : bool** : Enable testvector geneartion after quantization.\n- **tv_input : str** : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5)\n- **use_randomdb : bool** : Use randomdb for profiling data set\n- **userdb : str** : Profling data set path (default path is {workspace}/DATA/database.txt)\n\n**compiler : dict**\n- **assign_cpu : str** : Assign specific layer to cpu device\n- **assign_dsp : str** : Assign specific layer to dsp device",
            "Section: # **How to use**\n## **eht yaml file**\nContent:\nThe optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type.",
            "Section: # **How to use**\n## **Detailed explanation for elt yaml file**\nContent:\n- **output_conversion : str** : Add a Tensor2Cell format converter node at end of network\n- **packed_ucgo : bool** : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs\n- **preemption : bool** : Setting priority of NNC while compiling\n- **quant_type : str** : quantiztion type\n- **sync_npu_dsp : bool** :\n\n**simulator : dict**\n- **data_format : str** : Indicate the position of channel of input [channel_first, channel_last]\n- **use_randomdb : bool** : Use randomdb to forward, just support single input\n- **userdb : str** : Simulation data set path (default path is {workspace}/DATA/data.txt)",
            "Section: # **How to use**\n## **Detailed explanation for elt yaml file**\nContent:\nperf_estimator : dict**\n- **O2_enable : bool** : O2 optimization (true or false)\n- **O2_fm_forwarding : bool** : feature-map forwarding (true or false)\n- **SEG : bool** : Set true if input model is Deeplab V3+\n- **SSD : bool** : Set true if input model is SSD detection\n- **bit_width_factor_FM : int** : Select feature map bit width factor (1 or 2)\n- **bit_width_factor_FP16 : bool** : Set bit width factor as floating point (true or false)\n- **bit_width_factor_weight : int** : Select weight bit width factor (1 or 2)\n- **core_num : int** : 1 for single core, 2 for instance-1\n- **device : str** : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a]\n- **json_report : bool** : Enable report json format",
            "Section: # **How to use**\n## **Detailed explanation for elt yaml file**\nContent:\n- **test_type : str** : ENN running mode [lib, service]\n- **tv_threshold : float** : The value is used for tolerance threshold of output match verification.\n- **bitmatch_test : bool** : if set true, visual profiler will compile nnc first\n- **core_num : str** : The number of NPU core. [single, multiple]\n- **device : str** : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7]\n- **device_id : str** : id of the device connected to the server or PC running the enntools docker\n- **remote_ssh_config_path : str** : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml)\n- **ssh_bool : str** : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE]",
            "Section: Frontend API\n# **Frontend API**\n## **Detailed explanation for optimization yaml file**\nContent:\n- **alpha : float** : smoothquant migration strength\n- **calibration_data_path : string** : The path to the representative data to be used for calibration\n- **calibration_args : dict :** Arguments for processing calibration\n\t- **samples : int** : How many calibration data samples to use\n\t- **seed : int** : A value set as a seed for random selection\n\n**simulator : dict**\n- **metric : string**: The metric to be used for measurement",
            "Section: # **How to use**\n## **Detailed explanation for eht yaml file**\nContent:\n- **calibration_data_path : string** : The path to the representative data to be used for calibration\n- **calibration_args : dict :** Arguments for processing calibration\n\t- **samples : int** : How many calibration data samples to use\n\t- **seed : int** : A value set as a seed for random selection\n- **add_dummy_conv: bool** : Whether apply the dummy conv algorithm\n- **input_dtype: dict** : Input data type for the quantized model\n- **output_dtype: dict** : Output data type for the quantized model\n\n**simulator : dict",
            "Section: # **How to use**\n## **Detailed explanation for eht yaml file**\nContent:\n- **metric : string**: The metric to be used for measurement\n- **threshold : float** : The threshold value of the metric that determines agreement / disagreement\n- **input_data_path : string**: The path to the dataset for model inference\n\n**optimizer : dict**\n- **skip_4_dim_conversion : bool** : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion.\n- **overwrite_input_shapes : dict** : Enter the input shape for models with undefined input shapes.\n- **custom_template_path : dict**: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value.",
            "Section: # Optimizer\n## How to Create Custom Templates\n### How to apply a custom template\nContent:\nWrite the path of the custom template and the class name of the template. Following the above example, it can be written as follows:\n```yaml\ninput_model_path: {INPUT_MODEL_PATH}\noutput_folder_path: {OUTPUT_MODEL_PATH}\ninput_model_format: onnx\nmodel_type: CV\nquantizer:\n\t\t~~~~~\nsimulator:\n\t\t~~~~~~\noptimizer:\n\tcustom_template_path: {}\n\toverwrite_input_shapes:\n\t\tdata: [1,4,384,384]\n\tskip_4_dim_conversion: false\n\tdevice_name: default\n\tcustom_template_path:\n\t\t- TemplateFuseMath: /path/to/your/custome/template.py:\n```"
        ],
        "6": [
            "Section: Backend API\n# **Backend API**\n## **Simulator**\n### `get_inference_session`\nContent:\nGetInferenceSessionParameters schema details**\n**input_model_path : string**\nSpecifies the file path of the input model. It should point to a file, not a parent folder.\n\n**input_encondings_path : string**\nIndicates the file path for the encodings file of the cnnx model.\n\n**use_cuda : bool**\nDetermines the inference method: when true, uses GPU for inference; when false, performs inference using CPU only.\n\n**Example of run_inference API**```python\nfrom simulator.schema.base import InferenceSessionParameters\nfrom simulator.api import Simulator\nparams = InferenceSessionParameters(\n\tinput_model_path={input_model_path}\n\tinput_encodings_path={input_encodings_path},\n\tuse_cuda={input_data_path},\n\t)\nres = Simulator.get_inference_session(params)\n```",
            "Section: Backend API\n# **Backend API**\n## **Simulator**\n### `run_inference`\nContent:\nInferenceRunnerParameters schema details**\n**input_model_path : string**\nSpecifies the file path of the input model. It should point to a file, not a parent folder.\n\n**input_encondings_path : string**\nIndicates the file path for the encodings file of the cnnx model.\n\n**input_data_path : string**\nSpecifies the path to the folder containing data required for inference.\n\n**use_cuda : bool**\nDetermines the inference method: when true, uses GPU for inference; when false, performs inference using CPU only.\n\n**layerwise_check_name : list**\nAccepts input values to retrieve intermediate featuremap information along with the output during inference. When featuremap names are provided, the inference runner's return value includes the values of those featuremaps appended.\n\n**Example of run_inference API**\n```python\nfrom simulator.schema.base import InferenceRunnerParameters\nfrom simulator.api import Simulator\nparams = InferenceRunnerParameters(\n\tinput_model_path={input_model_path}\n\tinput_encodings_path={input_encodings_path},\n\tinput_data_path={input_data_path},\n\t)\nres = Simulator.run_inference(params)\n```",
            "Section: # **How to use**\n## **Detailed explanation for elt yaml file**\nContent:\n- **output_conversion : str** : Add a Tensor2Cell format converter node at end of network\n- **packed_ucgo : bool** : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs\n- **preemption : bool** : Setting priority of NNC while compiling\n- **quant_type : str** : quantiztion type\n- **sync_npu_dsp : bool** :\n\n**simulator : dict**\n- **data_format : str** : Indicate the position of channel of input [channel_first, channel_last]\n- **use_randomdb : bool** : Use randomdb to forward, just support single input\n- **userdb : str** : Simulation data set path (default path is {workspace}/DATA/data.txt)",
            "Section: Backend API\n# **Backend API**\n## **Simulator**\n### `compare_model_by_inference`\nContent:\nDetermines the inference method: when true, uses GPU for inference; when false, performs inference using CPU only.\n\n**Example of compare_model_by_inference API**\n```python\nfrom simulator.schema.base import InferenceCheckerParameters\nfrom simulator.api import Simulator\nparams = InferenceCheckerParameters(\n\t\t\tinput_data_path = \"/path/to/data/dir/\",\n\t\t\tinput_model_path_0 = \"/path/to/model_0.onnx\",\n\t\t\tinput_encodings_path_0 = \"/path/to/model_0.encodings\",\n\t\t\tinput_model_path_1 = \"/path/to/model_1.onnx\",\n\t\t\tinput_encodings_path_1 = \"/path/to/model_1.encodings\",\n\t\t\tmetric = \"snr\",\n\t\t\tthreshold = 30,\n\t\t\tuse_cuda = True\n\t\t)\nres = Simulator.compare_model_by_inference(params)\n```",
            "Section: Dataset preparation\n# **Dataset preparation**\nContent:\nThis is a guideline for preparing the input dataset for the simulator and calibration dataset for the quantizer module.\n## **Detailed instruction for dataset preparation**\nAnalyze model inputs using Netron. The example model receives three inputs: encoder_hidden_states, sample, and timestep.\n\nCreate a JSON file named 'input_folder_mapping.json'. In this file, define key-value pairs where each key represents a model input name and its corresponding value is the name of the folder containing the input data for that model input.\n```json\n{\n\t\"encoder_hidden_states\": \"input_encoder_hidden_states\",\n\t\"sample\": \"sample\",\n\t\"timestep\": \"timestep\"\n}\n```\nAfter preparing the dataset and JSON file, create a dataset directory and place it in the specified location.\n\nIt's important to note that the file names for each input should be identical, as shown in the example above.\n## Dataset Format Support\nCurrently, the supported data type is npy files consisting of float type. Support for other data formats is planned for the future.| Data Format | Support |\n|--------------------------------------------------------|---------|\n| numpy array(.npy, float) | O |\n| numpy array(.npy, int) | TBD |\n| binary data(.bin) | TBD |\n| image(.jpg) | TBD |",
            "Section: # **How to use**\n## **Detailed explanation for eht yaml file**\nContent:\n- **calibration_data_path : string** : The path to the representative data to be used for calibration\n- **calibration_args : dict :** Arguments for processing calibration\n\t- **samples : int** : How many calibration data samples to use\n\t- **seed : int** : A value set as a seed for random selection\n- **add_dummy_conv: bool** : Whether apply the dummy conv algorithm\n- **input_dtype: dict** : Input data type for the quantized model\n- **output_dtype: dict** : Output data type for the quantized model\n\n**simulator : dict",
            "Section: # **How to use**\n## **Detailed explanation for elt yaml file**\nContent:\n- **skip_old_snc_optimizer : bool** : true, skip old snc optimizer in old2new\n- **snc_converter : bool** : True, convert old snc to new snc, set it to false when input is new snc\n- **test_vector_gen : bool** : Enable testvector geneartion after quantization.\n- **tv_input : str** : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5)\n- **use_randomdb : bool** : Use randomdb for profiling data set\n- **userdb : str** : Profling data set path (default path is {workspace}/DATA/database.txt)\n\n**compiler : dict**\n- **assign_cpu : str** : Assign specific layer to cpu device\n- **assign_dsp : str** : Assign specific layer to dsp device",
            "Section: Simulator\n# Simulator\nContent:\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\nSimulator includes the following features:\n- `get_quantization_sim`: Attain a CNNX quantization simulation session for manual run.\n- `load_input_data`: Load the input data from the given dataset path.\n- `run_inference`: Conduct inference on a CNNX model.\n- `compare_model_by_inference`: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric.\n- `compare_model_by_layer` (TBD): Compare two CNNX models on intermediate tensors for each layer. To be released.",
            "Section: Using Adb to Execute\n# Using Adb to Execute\n## Executing Native Program on the ERD Board\nContent:\nAfter copying the necessary files to the ERD board, execute the native program using the following commands:\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```\n> When nnc_model_tester is built from Windows, execute permission must be provided.\n> ```shell\n> adb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n> ```\nThese commands perform the following actions:\n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively.",
            "Section: Frontend API\n# **Frontend API**\n## **Detailed explanation for optimization yaml file**\nContent:\n- **alpha : float** : smoothquant migration strength\n- **calibration_data_path : string** : The path to the representative data to be used for calibration\n- **calibration_args : dict :** Arguments for processing calibration\n\t- **samples : int** : How many calibration data samples to use\n\t- **seed : int** : A value set as a seed for random selection\n\n**simulator : dict**\n- **metric : string**: The metric to be used for measurement"
        ],
        "7": [
            "Section: # Model Optimization Flow\nContent:\nThis section details the optimization flow according to model type. Currently, optimization scenarios are predetermined for specific model types, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). We plan to update the features to allow flexible scheduling of optimization functions.",
            "Section: Explanation of model optimization flow\n# Model Optimization Flow\nContent:\nThis section details the optimization flow according to model type. Currently, optimization scenarios are predetermined for specific model types, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). We plan to update the features to allow flexible scheduling of optimization functions.",
            "Section: Optimizer\n# Optimizer\nContent:\n- support optimization method.<br>\n\tOptimizer supports many optimization features so that the model works efficiently on the device.\n\t- Fold\n\t\t- GeGLU\n\t\t- GroupNorm\n\t\t- LayerNorm\n\t\t- PReLU\n\t\t- RMSNorm\n\t- Fuse\n\t\t- SiLU\n\t\t- BatchNorm into Convolution\n\t\t- Cast\n\t\t- Deconvolution bias\n\t\t- Math\n\t- Insert\n\t\t- Depthwise Convolution for activation\n\t- Replace\n\t\t- Average Pooling to Depthwise convolution\n\t\t- Eltwise concat convolution\n\t\t- Convolution kernel 1 to 3\n\t\t- Matrix multiplication to dynamic convolution\n\t\t- ReduceMean to Global average pool\n\t\t- ReduceSum to Convolution\n\t\t- Slice to Split\n\t- Change attribute\n\t\t- axis of softmax\nIn addition, Optimizer Template, one of the Optimizer's powerful features, supports custom optimization methods that users have.",
            "Section: # Model Optimization Flow\n## LLM\nContent:\n1. **Optimization**\n\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\n\toutput_path : `{result_dir}/cnnx`\n2. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n\toutput_path : `{result_dir}/optimized`\n3. **CNNX-to SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`",
            "Section: Explanation of model optimization flow\n# Model Optimization Flow\n## LLM\nContent:\n1. **Optimization**\n\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\n\toutput_path : `{result_dir}/cnnx`\n2. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n\toutput_path : `{result_dir}/optimized`\n3. **CNNX-to SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`",
            "Section: Explanation of model optimization flow\n# Model Optimization Flow\n## CV\nContent:\n1. **ONNX-to-CNNX Conversion**\n\tConverts an opset16 ONNX model to CNNX format.\n\toutput_path : `{result_dir}/cnnx`\n2. **Optimization**\n\tPasses the CNNX model through Simplifier and applies an Optimization template.\n\toutput_path : `{result_dir}/optimized`\n3. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n4. **Quantization**\n\tApplies Fixed Precision Quantization to the optimized CNNX model.\n\toutput_path : `{result_dir}/quantized`\n5. **Performance Evaluation of Quantization**\n\tCompares the inference results of the model before and after quantization.\n6. **CNNX-to-SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`",
            "Section: Explanation of model optimization flow\n# Model Optimization Flow\n## LVM\nContent:\n1. **ONNX-to-CNNX Conversion**\n\tConverts an opset16 ONNX model to CNNX format.\n\toutput_path : `{result_dir}/cnnx`\n2. **Optimization**\n\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\n\toutput_path : `{result_dir}/optimized`\n3. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n4. **Quantization**\n\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\n\toutput_path :\n\t`{result_dir}/smoothquant`\n\t`{result_dir}/mixed_precision_quant`\n\t`{result_dir}/softmax_bias_correction`\n5. **Performance Evaluation of Quantization**\n\tCompares the inference results of the model before and after quantization.\n6. **CNNX-to-SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`",
            "Section: # Model Optimization Flow\n## CV\nContent:\n1. **ONNX-to-CNNX Conversion**\n\tConverts an opset16 ONNX model to CNNX format.\n\toutput_path : `{result_dir}/cnnx`\n2. **Optimization**\n\tPasses the CNNX model through Simplifier and applies an Optimization template.\n\toutput_path : `{result_dir}/optimized`\n3. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n4. **Quantization**\n\tApplies Fixed Precision Quantization to the optimized CNNX model.\n\toutput_path : `{result_dir}/quantized`\n5. **Performance Evaluation of Quantization**\n\tCompares the inference results of the model before and after quantization.\n6. **CNNX-to-SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`",
            "Section: # Model Optimization Flow\n## LVM\nContent:\n1. **ONNX-to-CNNX Conversion**\n\tConverts an opset16 ONNX model to CNNX format.\n\toutput_path : `{result_dir}/cnnx`\n2. **Optimization**\n\tPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.\n\toutput_path : `{result_dir}/optimized`\n3. **Performance Evaluation of Optimization**\n\tCompares the inference results of the model before and after optimization.\n4. **Quantization**\n\tApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.\n\toutput_path :\n\t`{result_dir}/smoothquant`\n\t`{result_dir}/mixed_precision_quant`\n\t`{result_dir}/softmax_bias_correction`\n5. **Performance Evaluation of Quantization**\n\tCompares the inference results of the model before and after quantization.\n6. **CNNX-to-SNC Conversion**\n\tConverts the CNNX model to SNC format.\n\toutput_path : `{result_dir}/snc`",
            "Section: Optimizer\n# Optimizer\nContent:\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\n- shape_inference<br>\n\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion<br>\n\tIn order for the .nnc extension model to operate on a device, the input and output shapes of all operators in the model must be in four dimensions."
        ],
        "8": [
            "Section: # **How to use**\n## **Detailed explanation for elt yaml file**\nContent:\nmodel_analyzer : dict**\n- **check : bool** : Check the op support status\n- **device : str** : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6]\n- **level : int** : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint)\n- **snc_input : bool** : analyze the snc model, false: analyze the original model\n\n**database_gen : dict**\n- **database_spec : str** : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml)\n\n**converter : dict**\n- **device : str** : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7]",
            "Section: Support Matrix\n# Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
            "Section: Support Matrix\n# Support Matrix\n## Supported Operators\nContent:\n| 98 | NEQUAL | NOT_EQUAL | | | O | |\n| 99 | LESS | LESS | | | O | O |\n| 100 | GREATER | GREATER | | | O | O |\n| 101 | GREATER_EQUAL | GREATER_EQUAL | | | O | O |\n| 102 | LESS_EQUAL | LESS_EQUAL | | | O | O |\n| 103 | ADD_N | ADD_N | | | O | O |\n| 104 | TOPK_V2 | TOPK_V2 | | | | O |\n| 105 | LOG_SOFTMAX | LOG_SOFTMAX | | | O | O |\n| 106 | FLOOR_MOD | FLOOR_MOD | | | O | O |\n| 107 | SEGMENT_SUM | SEGMENT_SUM | | | O | O |\n| 108 | GELU | GELU | | | O | O |",
            "Section: Backend API\n# **Backend API**\n## **Simulator**\n### `compare_model_by_inference`\nContent:\nDetermines the inference method: when true, uses GPU for inference; when false, performs inference using CPU only.\n\n**Example of compare_model_by_inference API**\n```python\nfrom simulator.schema.base import InferenceCheckerParameters\nfrom simulator.api import Simulator\nparams = InferenceCheckerParameters(\n\t\t\tinput_data_path = \"/path/to/data/dir/\",\n\t\t\tinput_model_path_0 = \"/path/to/model_0.onnx\",\n\t\t\tinput_encodings_path_0 = \"/path/to/model_0.encodings\",\n\t\t\tinput_model_path_1 = \"/path/to/model_1.onnx\",\n\t\t\tinput_encodings_path_1 = \"/path/to/model_1.encodings\",\n\t\t\tmetric = \"snr\",\n\t\t\tthreshold = 30,\n\t\t\tuse_cuda = True\n\t\t)\nres = Simulator.compare_model_by_inference(params)\n```",
            "Section: Backend API\n# **Backend API**\n## **Simulator**\n### `compare_model_by_inference`\nContent:\nInferenceCheckerParameters object details**\n**input_model_path0 : string**\nThe onnx model file path to be compared.\n\n**input_encondings_path_0 : string**\nThe model encodings file path for the onnx model 0.\n\n**input_model_path_1 : string**\nThe onnx model file path to be compared.\n\n**input_encondings_path_1 : string**\nThe model encodings file path for the onnx model 1.\n\n**input_data_path : string**\nSpecifies the path to the folder containing data required for inference.\n\n**meric : string**\nMetric in which the error is calculated (\u201csnr\u201d, \u201crmse\u201d, \u2026)\n\n**threshold : float**\nThe threshold by which two models are considered as equivalent.\n\n**use_cuda : bool",
            "Section: Support Matrix\n# Support Matrix\n## Supported Operators\nContent:\n| 57 | REDUCE_MEAN | REDUCE_MEAN | O | O | O | O |\n| 58 | REDUCE_MIN | REDUCE_MIN | O | O | O | O |\n| 59 | REDUCE_SUM | SUM | O | | O | O |\n| 60 | RELU | RELU | O | O | O | O |\n| 61 | RELU_0_TO_1 | RELU_0_TO_1 | | O | O | O |\n| 62 | RELU6 | RELU6 | O | O | O | O |\n| 63 | RELUN | RELUN | | O | | |\n| 64 | RESHAPE | RESHAPE | O | O | O | O |\n| 65 | RESIZE_BILINEAR | RESIZE_BILINEAR | O | O | O | O |\n| 66 | RESIZE_BILINEAR_DS | RESIZE_BILINEAR | | O | | O |",
            "Section: # Simulator\nContent:\n- `compare_model_by_layer` : Compare two CNNX models on intermediate tensors for each layer.\n\t```python\n\tfrom simulator imoprt api\n\t\tparams = api.LayerwiseCheckerParameters(\n\t\t\tinput_data_path = \"/path/to/input/data/,\n\t\t\tinput_model_path_0 = \"/path/to/model/0.onnx\",\n\t\t\tinput_encodings_path_0 = \"/path/to/model/0.encodings\",\n\t\t\tinput_model_path_1 = \"/path/to/model/0.onnx\",\n\t\t\tinput_encodings_path_1 = \"/path/to/model/0.encodings\",\n\t\t\tmetric = \"snr\",\n\t\t\tthreshold = 100,\n\t\t\tuse_cuda = False,\n\t\t\tlayer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")]\n\t\t\texport_featuremap = True,\n\t\t\texport_featuremap_path = \"/path/to/save/exported/featuremap\"\n\t\t)\n\t\tres = api.compare_model_by_layer(params)\n\t\tprint(\"Layerwise check\")\n\t\tfor input_name, result in res.items():\n\t\t\t_, result_dict = result\n\t\t\tfor output_name, snr_value in result_dict.items():\n\t\t\t\tprint(f\"{output_name}: {snr_value}\")\n\t```",
            "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
            "Section: Support Matrix\n# Support Matrix\n## Supported Operators\nContent:\n| 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O |\n| 37 | HARD_SWISH | HARD_SWISH | O | O | O | O |\n| 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O |\n| 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O |\n| 40 | LOG | LOG | | | O | O |\n| 41 | LOGISTIC | LOGISTIC | O | O | O | O |\n| 42 | MAXIMUM | MAXIMUM | | O | O | O |\n| 43 | MAXPOOL | MAX_POOL_2D | O | O | O | O |\n| 44 | MEAN | MEAN | | O | O | O |\n| 45 | MINIMUM | MINIMUM | | O | O | O |",
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon."
        ],
        "9": [
            "Section: # **How to use**\n## **Detailed explanation for elt yaml file**\nContent:\nmodel_analyzer : dict**\n- **check : bool** : Check the op support status\n- **device : str** : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6]\n- **level : int** : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint)\n- **snc_input : bool** : analyze the snc model, false: analyze the original model\n\n**database_gen : dict**\n- **database_spec : str** : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml)\n\n**converter : dict**\n- **device : str** : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7]",
            "Content:\n# Introduction to Exynos AI Studio\nExynos AI Studio** is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models.\n## System overview diagram",
            "Content:\n# [Device Selection]\nThe Device Selection feature allows users to select from multiple devices. This feature can be accessed from the drop-down menu that is located at the top of the Device Farm page.\nRules for using Device Selection feature:\n- Click the Device drop-down menu to display a list of registered devices in a drawer menu format.\n- By default, the device listed at the top is selected.\n- Only one device can be selected at a time. The details of the selected device is displayed in the Device area.\n- Multiple Device types are displayed in the Reservation Status, however, only one device can be connected at any given time.\n\tTo select a device:\n1. Click the Device drop-down menu, and select the required device.\n2. The Device area is refreshed to display the information of the selected device.\n# [Processor Specification Check]\nProcessor refers to the System on Chip (SoC) developed by Samsung Electronics. It integrates high-performance CPU, GPU, NPU, and memory management features, which provide enhanced processing power and graphics performance.\nTo check the processor specifications:\n1. Click Learn more from the Device area.\n2. You will be directed to a page that provides the details of the currently displayed processor.\n\t\u2003",
            "Section: Exynos AI Studio User Guide (TBD)\nContent:\n# Introduction to Exynos AI High-Level Toolchain (EHT)\nEHT** is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models.\n## System overview diagram",
            "Content:\n# Introduction to Exynos AI High-Level Toolchain (EHT)\nEHT** is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models.\n## System overview diagram\n## Converter\n## Optimizer\n## Quantizer\n## Simulator\n## Model Optimization Flow",
            "Section: Optimizer\n# Optimizer\nContent:\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\n- shape_inference<br>\n\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion<br>\n\tIn order for the .nnc extension model to operate on a device, the input and output shapes of all operators in the model must be in four dimensions.",
            "Section: # Optimizer\nContent:\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\n- shape_inference<br>\n\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion<br>\n\tTo deploy a model with a .nnc extension on a device, all operators must have input and output shapes with four dimensions. To facilitate this, the 4D conversion process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. ",
            "Section: Simulator\n# Simulator\nContent:\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\nSimulator includes the following features:\n- `get_quantization_sim`: Attain a CNNX quantization simulation session for manual run.\n- `load_input_data`: Load the input data from the given dataset path.\n- `run_inference`: Conduct inference on a CNNX model.\n- `compare_model_by_inference`: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric.\n- `compare_model_by_layer` (TBD): Compare two CNNX models on intermediate tensors for each layer. To be released.",
            "Section: # Simulator\nContent:\nSimulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution.\nQuantized inference is performed using _simulated quantization_. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process.\nSimulator includes the following features(with example codes):",
            "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board."
        ]
    },
    "reference_contexts": {
        "0": [
            "Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with features like shape_inference, which allows users to define or modify input shapes. It also includes a 4D conversion process to ensure all operators have four-dimensional input and output shapes, tracking channel axes during this conversion. The Optimizer supports various optimization methods to improve efficiency, including techniques like Fold, GeGLU, and LayerNorm, among others. Additionally, the Optimizer Template allows users to create custom optimization methods.",
            "The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It utilizes simulated quantization to mimic quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models based on inference outputs or intermediate tensors. Example codes demonstrate how to use these features, such as running inference and comparing models layer-wise using SNR as a metric.",
            "Recommended specifications include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. Minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.",
            "The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, the Dummy Convolution for the Concat Operator can be applied during quantization to maintain activation scales, potentially improving model performance.",
            "To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands in the project directory.",
            "The Converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets for exporting. Example code snippets demonstrate how to use these converters effectively.",
            "The model needs to be prepared in ONNX format for optimization.",
            "The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located in the DATA folder created after running the 'enntools init' command.",
            "The document outlines specifications for model optimization, including model type (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model measurement, along with input data paths for inference. Additionally, the optimizer section specifies options for skipping dimension conversion, overwriting input shapes, and applying custom optimization templates.",
            "For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.",
            "Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating custom templates, preparing the model for optimization, and validating the optimized model.",
            "The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.",
            "Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.",
            "Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in a provided figure.",
            "The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.",
            "The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels. It requires a quantized CNNX model and a debug dictionary as input.",
            "The provided text contains an image reference labeled 'overview_diagram', indicating a visual representation of information.",
            "The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before executing a conversion command with 'enntools'.",
            "This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization, such as ensuring the previous node is a convolution layer and that the arithmetic operator follows it. The optimization method updates the convolution node's value based on the arithmetic node's value and removes the original node from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases for optimization. The trigger_op method activates the optimization when specific conditions are met, particularly for four basic arithmetic operations with a four-dimensional input feature map.",
            "The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators to apply MPQ, and the module also offers a MPQ debug API.",
            "The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a directory in the container, using the 'exynos_ai_studio:0.1.0' image.",
            "The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.",
            "The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module being optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component allows for optional modifications to the optimized model.",
            "The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.",
            "EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.",
            "The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, and evaluating performance before and after optimization. It also includes applying fixed precision quantization to the optimized model and evaluating performance again. Finally, the CNNX model is converted to SNC format.",
            "The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantization, simulation, and optimization, detailing parameters like input shapes and device name.",
            "EHT supports ONNX opset versions 13 to 17.",
            "The configuration values are utilized across different modules of elt.",
            "The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.",
            "This document provides guidelines for preparing the input dataset.",
            "The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.",
            "The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.",
            "The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support and analyzes models based on specified levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section focuses on data format and simulation dataset paths. The performance estimator provides options for optimization and bit width factors. Lastly, the profiler section details modes for power consumption and performance, along with profiling targets and device configurations.",
            "The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).",
            "The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.",
            "The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision."
        ],
        "1": [
            "Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with features like shape_inference, which allows users to define or modify input shapes. It also includes a 4D conversion process to ensure all operators have four-dimensional input and output shapes, tracking channel axes during this conversion. The Optimizer supports various optimization methods to improve efficiency, including techniques like Fold, GeGLU, and LayerNorm, among others. Additionally, the Optimizer Template allows users to create custom optimization methods.",
            "The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It utilizes simulated quantization to mimic quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models based on inference outputs or intermediate tensors. Example codes demonstrate how to use these features, such as running inference and comparing models layer-wise using SNR as a metric.",
            "Recommended specifications include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. Minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.",
            "The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, the Dummy Convolution for the Concat Operator can be applied during quantization to maintain activation scales, potentially improving model performance.",
            "To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands in the project directory.",
            "The Converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets for exporting. Example code snippets demonstrate how to use these converters effectively.",
            "The model needs to be prepared in ONNX format for optimization.",
            "The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located in the DATA folder created after running the 'enntools init' command.",
            "The document outlines specifications for model optimization, including model type (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model measurement, along with input data paths for inference. Additionally, the optimizer section specifies options for skipping dimension conversion, overwriting input shapes, and applying custom optimization templates.",
            "For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.",
            "Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating custom templates, preparing the model for optimization, and validating the optimized model.",
            "The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.",
            "Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.",
            "Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in a provided figure.",
            "The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.",
            "The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels. It requires a quantized CNNX model and a debug dictionary as input.",
            "The provided text contains an image reference labeled 'overview_diagram', indicating a visual representation of information.",
            "The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before executing a conversion command with 'enntools'.",
            "This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization, such as ensuring the previous node is a convolution layer and that the arithmetic operator follows it. The optimization method updates the convolution node's value based on the arithmetic node's value and removes the original node from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases for optimization. The trigger_op method activates the optimization when specific conditions are met, particularly for four basic arithmetic operations with a four-dimensional input feature map.",
            "The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators to apply MPQ, and the module also offers a MPQ debug API.",
            "The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a directory in the container, using the 'exynos_ai_studio:0.1.0' image.",
            "The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.",
            "The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module being optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component allows for optional modifications to the optimized model.",
            "The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.",
            "EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.",
            "The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, and evaluating performance before and after optimization. It also includes applying fixed precision quantization to the optimized model and evaluating performance again. Finally, the CNNX model is converted to SNC format.",
            "The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantization, simulation, and optimization, detailing parameters like input shapes and device name.",
            "EHT supports ONNX opset versions 13 to 17.",
            "The configuration values are utilized across different modules of elt.",
            "The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.",
            "This document provides guidelines for preparing the input dataset.",
            "The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.",
            "The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.",
            "The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support and analyzes models based on specified levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section focuses on data format and simulation dataset paths. The performance estimator provides options for optimization and bit width factors. Lastly, the profiler section details modes for power consumption and performance, along with profiling targets and device configurations.",
            "The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).",
            "The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.",
            "The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision."
        ],
        "2": [
            "Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with features like shape_inference, which allows users to define or modify input shapes. It also includes a 4D conversion process to ensure all operators have four-dimensional input and output shapes, tracking channel axes during this conversion. The Optimizer supports various optimization methods to improve efficiency, including techniques like Fold, GeGLU, and LayerNorm, among others. Additionally, the Optimizer Template allows users to create custom optimization methods.",
            "The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It utilizes simulated quantization to mimic quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models based on inference outputs or intermediate tensors. Example codes demonstrate how to use these features, such as running inference and comparing models layer-wise using SNR as a metric.",
            "Recommended specifications include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. Minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.",
            "The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, the Dummy Convolution for the Concat Operator can be applied during quantization to maintain activation scales, potentially improving model performance.",
            "To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands in the project directory.",
            "The Converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets for exporting. Example code snippets demonstrate how to use these converters effectively.",
            "The model needs to be prepared in ONNX format for optimization.",
            "The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located in the DATA folder created after running the 'enntools init' command.",
            "The document outlines specifications for model optimization, including model type (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model measurement, along with input data paths for inference. Additionally, the optimizer section specifies options for skipping dimension conversion, overwriting input shapes, and applying custom optimization templates.",
            "For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.",
            "Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating custom templates, preparing the model for optimization, and validating the optimized model.",
            "The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.",
            "Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.",
            "Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in a provided figure.",
            "The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.",
            "The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels. It requires a quantized CNNX model and a debug dictionary as input.",
            "The provided text contains an image reference labeled 'overview_diagram', indicating a visual representation of information.",
            "The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before executing a conversion command with 'enntools'.",
            "This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization, such as ensuring the previous node is a convolution layer and that the arithmetic operator follows it. The optimization method updates the convolution node's value based on the arithmetic node's value and removes the original node from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases for optimization. The trigger_op method activates the optimization when specific conditions are met, particularly for four basic arithmetic operations with a four-dimensional input feature map.",
            "The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators to apply MPQ, and the module also offers a MPQ debug API.",
            "The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a directory in the container, using the 'exynos_ai_studio:0.1.0' image.",
            "The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.",
            "The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module being optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component allows for optional modifications to the optimized model.",
            "The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.",
            "EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.",
            "The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, and evaluating performance before and after optimization. It also includes applying fixed precision quantization to the optimized model and evaluating performance again. Finally, the CNNX model is converted to SNC format.",
            "The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantization, simulation, and optimization, detailing parameters like input shapes and device name.",
            "EHT supports ONNX opset versions 13 to 17.",
            "The configuration values are utilized across different modules of elt.",
            "The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.",
            "This document provides guidelines for preparing the input dataset.",
            "The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.",
            "The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.",
            "The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support and analyzes models based on specified levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section focuses on data format and simulation dataset paths. The performance estimator provides options for optimization and bit width factors. Lastly, the profiler section details modes for power consumption and performance, along with profiling targets and device configurations.",
            "The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).",
            "The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.",
            "The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision."
        ],
        "3": [
            "Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with features like shape_inference, which allows users to define or modify input shapes. It also includes a 4D conversion process to ensure all operators have four-dimensional input and output shapes, tracking channel axes during this conversion. The Optimizer supports various optimization methods to improve efficiency, including techniques like Fold, GeGLU, and LayerNorm, among others. Additionally, the Optimizer Template allows users to create custom optimization methods.",
            "The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It utilizes simulated quantization to mimic quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models based on inference outputs or intermediate tensors. Example codes demonstrate how to use these features, such as running inference and comparing models layer-wise using SNR as a metric.",
            "Recommended specifications include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. Minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.",
            "The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, the Dummy Convolution for the Concat Operator can be applied during quantization to maintain activation scales, potentially improving model performance.",
            "To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands in the project directory.",
            "The Converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets for exporting. Example code snippets demonstrate how to use these converters effectively.",
            "The model needs to be prepared in ONNX format for optimization.",
            "The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located in the DATA folder created after running the 'enntools init' command.",
            "The document outlines specifications for model optimization, including model type (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model measurement, along with input data paths for inference. Additionally, the optimizer section specifies options for skipping dimension conversion, overwriting input shapes, and applying custom optimization templates.",
            "For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.",
            "Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating custom templates, preparing the model for optimization, and validating the optimized model.",
            "The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.",
            "Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.",
            "Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in a provided figure.",
            "The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.",
            "The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels. It requires a quantized CNNX model and a debug dictionary as input.",
            "The provided text contains an image reference labeled 'overview_diagram', indicating a visual representation of information.",
            "The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before executing a conversion command with 'enntools'.",
            "This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization, such as ensuring the previous node is a convolution layer and that the arithmetic operator follows it. The optimization method updates the convolution node's value based on the arithmetic node's value and removes the original node from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases for optimization. The trigger_op method activates the optimization when specific conditions are met, particularly for four basic arithmetic operations with a four-dimensional input feature map.",
            "The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators to apply MPQ, and the module also offers a MPQ debug API.",
            "The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a directory in the container, using the 'exynos_ai_studio:0.1.0' image.",
            "The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.",
            "The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module being optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component allows for optional modifications to the optimized model.",
            "The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.",
            "EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.",
            "The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, and evaluating performance before and after optimization. It also includes applying fixed precision quantization to the optimized model and evaluating performance again. Finally, the CNNX model is converted to SNC format.",
            "The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantization, simulation, and optimization, detailing parameters like input shapes and device name.",
            "EHT supports ONNX opset versions 13 to 17.",
            "The configuration values are utilized across different modules of elt.",
            "The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.",
            "This document provides guidelines for preparing the input dataset.",
            "The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.",
            "The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.",
            "The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support and analyzes models based on specified levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section focuses on data format and simulation dataset paths. The performance estimator provides options for optimization and bit width factors. Lastly, the profiler section details modes for power consumption and performance, along with profiling targets and device configurations.",
            "The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).",
            "The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.",
            "The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision."
        ],
        "4": [
            "Models are trained on CPUs and GPUs, but face challenges on Exynos chips due to hardware constraints. The Optimizer enhances model performance with features like shape_inference, which allows users to define or modify input shapes. It also includes a 4D conversion process to ensure all operators have four-dimensional input and output shapes, tracking channel axes during this conversion. The Optimizer supports various optimization methods to improve efficiency, including techniques like Fold, GeGLU, and LayerNorm, among others. Additionally, the Optimizer Template allows users to create custom optimization methods.",
            "The simulator performs inference on a CNNX model using an ONNX model file and quantization encodings. It utilizes simulated quantization to mimic quantized operations by clipping floating point values into ranges. Key features include obtaining a quantization simulation session, loading input data, running inference, and comparing models based on inference outputs or intermediate tensors. Example codes demonstrate how to use these features, such as running inference and comparing models layer-wise using SNR as a metric.",
            "Recommended specifications include a latest AMD Ryzen 7 or Intel Core i7 processor with at least 4 cores and a 64-bit architecture. Minimum specifications require a 2GHz dual-core processor, also with a 64-bit architecture.",
            "The currently supported advanced quantization methods include Softmax Bias Correction, which adds a float bias to the Softmax output to mitigate performance degradation from quantization; SmoothQuant, which smooths activation outliers by shifting quantization difficulty from activations to weights; and Cross Layer Equalization, which tunes the weight range of channels to reduce quantization error. Additionally, the Dummy Convolution for the Concat Operator can be applied during quantization to maintain activation scales, potentially improving model performance.",
            "To use enntools, create a folder and place an ONNX file with the same name inside it. Then, initialize the project to generate the necessary files for enntools commands in the project directory.",
            "The Converter facilitates model conversion between various intermediate representations (IRs), allowing users to optimize their PyTorch and TensorFlow models and convert them into SNC models. It includes four main components: the PyTorch2ONNX Converter, which exports PyTorch models to ONNX; the TF2ONNX Converter, which converts TF or TFLITE models to ONNX; the ONNX2CNNX Converter, which transforms ONNX models to CNNX; and the CNNX2SNC Converter, which converts CNNX models to SNC. Each converter requires specific input parameters, such as input dimensions and ONNX opsets for exporting. Example code snippets demonstrate how to use these converters effectively.",
            "The model needs to be prepared in ONNX format for optimization.",
            "The dataset must be in .h5 format with keys matching the model's input names for proper mapping. The Exynos AI Studio requires float32 data types, so the internal data in the h5 file must be in fp32 format. The h5 file should be located in the DATA folder created after running the 'enntools init' command.",
            "The document outlines specifications for model optimization, including model type (CV, LVM, LLM) and quantization parameters such as weight and activation precision, mixed precision quantization settings, calibration data paths, and dummy convolution options. It also details the simulator metrics and thresholds for model measurement, along with input data paths for inference. Additionally, the optimizer section specifies options for skipping dimension conversion, overwriting input shapes, and applying custom optimization templates.",
            "For optimal performance, a minimum of 8GB RAM is required, with 16GB or more recommended, along with at least 100GB of disk space.",
            "Optimizer is a tool for optimizing models, allowing users to create custom optimization methods through the \"Optimizer Template.\" The process involves creating custom templates, preparing the model for optimization, and validating the optimized model.",
            "The optimization flow is tailored to specific model types, including computer vision, large language models, and large vision models. Future updates will enable flexible scheduling of optimization functions.",
            "Exynos AI Studio is a software that optimizes and converts neural network models, providing functionalities like quantization, conversion, optimization, and compilation to generate NNC models.",
            "Users can select between eht and elt for conversion based on the mode, with the workflow illustrated in a provided figure.",
            "The process involves three main steps: first, the CNNX model is optimized using Simplifier and 4-Dimensional Conversion, with results saved in a specified output path. Second, the performance of the optimized model is evaluated by comparing inference results before and after optimization. Finally, the CNNX model is converted to SNC format, with the output also saved in a designated path.",
            "The quantizer offers a mixed precision quantization debug API, allowing users to choose different activations and weights with varying precision levels. It requires a quantized CNNX model and a debug dictionary as input.",
            "The provided text contains an image reference labeled 'overview_diagram', indicating a visual representation of information.",
            "The instructions involve creating a workspace directory, copying specific files into it, and initializing a tool called 'enntools'. Additionally, there is a step to modify a configuration file and set a mode before executing a conversion command with 'enntools'.",
            "This text explains how to create custom templates for fusing arithmetic operations into convolution layers. It introduces classes like TemplateStepInternal, TemplateCaseInternal, and OptimizerTemplateInternal to manage code behavior. The process involves defining conditions for optimization, such as ensuring the previous node is a convolution layer and that the arithmetic operator follows it. The optimization method updates the convolution node's value based on the arithmetic node's value and removes the original node from the model. The StepFuseMath class implements the optimization logic, while CaseFuseMath checks conditions for fusing operations. The TemplateFuseMath class organizes the cases for optimization. The trigger_op method activates the optimization when specific conditions are met, particularly for four basic arithmetic operations with a four-dimensional input feature map.",
            "The Quantizer module in EHT applies both basic and advanced quantization methods to input models, including fixed precision and mixed precision quantization (MPQ). Users can specify activation names or operators to apply MPQ, and the module also offers a MPQ debug API.",
            "The command runs a Docker container named 'exynos_ai_studio_container' with GPU support, mapping a local dataset directory to a directory in the container, using the 'exynos_ai_studio:0.1.0' image.",
            "The optimization file provides information for setting input and output model locations and configurations for each module, with optimization methods varying by model type.",
            "The Optimizer template outlines how to optimize a model, featuring components such as the Optimize Name, Trigger condition, Origin Module, Origin condition, Optimized Module, and Optimization. The Trigger condition initiates the optimization based on operator type, while the Origin Module is the module being optimized. The Origin condition ensures the module is a candidate for optimization, and the Optimized Module represents the result after optimization. The Optimization component allows for optional modifications to the optimized model.",
            "The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model through simplification and dimensional conversion, and evaluating performance before and after optimization. It also includes quantization using methods like Smooth Quantization and Mixed Precision Quantization, followed by performance evaluation of the quantized model. Finally, the CNNX model is converted to SNC format.",
            "EHT is a software that optimizes neural networks, including computer vision, large language, and large vision models. It provides functionalities like quantization and model optimization to create SNC models.",
            "The process involves several steps: converting an ONNX model to CNNX format, optimizing the CNNX model, and evaluating performance before and after optimization. It also includes applying fixed precision quantization to the optimized model and evaluating performance again. Finally, the CNNX model is converted to SNC format.",
            "The text provides a YAML configuration for a custom template in a machine learning model setup, specifying paths for input and output models, model format, and type. It includes sections for quantization, simulation, and optimization, detailing parameters like input shapes and device name.",
            "EHT supports ONNX opset versions 13 to 17.",
            "The configuration values are utilized across different modules of elt.",
            "The Optimizer Templates execute a flow where the node first encounters the trigger_op method. It then checks the origin_condition of the case, and if the condition is met, it proceeds to optimize the module through specified steps.",
            "This document provides guidelines for preparing the input dataset.",
            "The model supports mixed precision through two approaches: by name, where users specify precisions for specific activations or weights, and by operator, where users define precisions for different operator types, such as quantizing all outputs of the Add operator to INT4.",
            "The document contains a structured list of topics related to model optimization and analysis, including sections on EHT, converters, optimizers, quantizers, simulators, and performance estimators. It also includes links to quick start guides, system requirements, and usage instructions.",
            "The provided text outlines various configuration options for a model analysis, database generation, conversion, compilation, simulation, performance estimation, and profiling. Each section specifies parameters such as device types, optimization levels, data formats, and quantization options. The model analyzer checks operation support and analyzes models based on specified levels. The converter section includes options for enabling quantization and graph optimization. The compiler section allows for assigning specific layers to different devices and includes various optimization settings. The simulator section focuses on data format and simulation dataset paths. The performance estimator provides options for optimization and bit width factors. Lastly, the profiler section details modes for power consumption and performance, along with profiling targets and device configurations.",
            "The system requirements include Linux based on Ubuntu 22.04, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later with NVIDIA Container Toolkit support, and the NVIDIA Container Toolkit (nvidia-docker2).",
            "The input contains an image reference labeled 'overview_diagram', indicating a visual representation of information, but does not provide any textual content to summarize.",
            "The model uses static uniform quantization for both weights and activations, allowing users to define the precision (bit-width) for each. The entire model is quantized to the chosen precision."
        ],
        "5": [
            "database_gen : dict**\n- **database_spec : str**: genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml)  \n---  \n**"
        ],
        "6": [
            "simulator : dict**\n- **data_format : str**: Indicate the position of channel of input [channel_first, channel_last]\n- **use_randomdb : bool**: Use randomdb to forward, just support single input\n- **userdb : str**: Simulation data set path (default path is {workspace}/DATA/data.txt)  \n---  \n**"
        ],
        "7": [
            "model_type : string**  \nSpecify the type of the input model. Optimization is performed differently depending on the model type, with details as follows  \n- CV\n- LVM\n- LLM  \n---  \n**"
        ],
        "8": [
            "model_analyzer : dict**\n- **check : bool**: Check the op support status\n- **device : str**: System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6]\n- **level : int** :  level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint)\n- **snc_input : bool** : analyze the snc model, false: analyze the original model  \n---  \n**"
        ],
        "9": [
            "model_analyzer : dict**\n- **check : bool**: Check the op support status\n- **device : str**: System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6]\n- **level : int** :  level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint)\n- **snc_input : bool** : analyze the snc model, false: analyze the original model  \n---  \n**"
        ]
    },
    "response": {
        "0": "In the model optimization flow, optimization techniques and their evaluations are reported in two main stages for comparing performance: \n\n1. **Performance Evaluation of Optimization**: After applying optimization techniques such as `Simplifier` and `4-Dimensional Conversion`, and applying an optimization template, the performance of the model is evaluated by comparing the inference results before and after these optimizations. \n\n2. **Performance Evaluation of Quantization**: Once quantization techniques such as `Smooth Quantization`, `Mixed Precision Quantization`, and `Softmax Bias Correction` are applied to the CNNX FP32 model, the performance of the model is evaluated again by comparing the inference results before and after quantization.\n\nThese evaluations help assess how much the optimizations improve or maintain the model's performance and how quantization impacts the final model inference results on Exynos hardware.",
        "1": "In the context of model optimization using the ENN SDK, quantization methods and optimization techniques serve different purposes and have various effects on performance and efficiency:\n\n1. **Quantization Methods**: \n   - **Performance**: Quantization is aimed at reducing the model size and computation load by lowering the precision of the model parameters (weights and activations). Techniques such as Fixed Precision Quantization, Mixed Precision Quantization, and advanced methods like SmoothQuant and Softmax Bias Correction are used to adapt the model for low-precision execution without significant performance degradation.\n   - **Efficiency**: Quantization leverages lower-bit precision to enhance inference speed and reduce memory footprint. However, it is crucial to balance these benefits against the potential loss of model accuracy.\n\n2. **Optimization Techniques**:\n   - **Performance**: Optimization techniques such as the Simplifier and 4-Dimensional Conversion improve the computational graph of the model to make inferences faster and more efficient. These processes aim to streamline the model structure without altering its precision.\n   - **Efficiency**: The primary goal is to enhance efficiency by optimizing the model\u2019s execution paths and reducing redundant operations, thus providing a more efficient utilization of the available hardware resources.\n\nIn summary, while both optimization and quantization aim to improve model execution, quantization focuses on reducing precision to increase speed and decrease consumption of resources, while optimization aims to refine the model\u2019s execution process for increased throughput without changing its precision.",
        "2": "In terms of performance and efficiency across different hardware platforms, the comparison of model conversion techniques, optimization methods, and advanced quantization methods can be broken down as follows:\n\n1. **Floating-point Hardware (CPUs and GPUs)**:\n   - Models are typically trained and generated on floating-point hardware like CPUs and GPUs, which handle floating-point operations efficiently. However, these devices may not perform optimally on Exynos chips due to hardware constraints that require specific optimizations for improved performance.\n\n2. **Optimizer**:\n   - The Optimizer in the ENN SDK is designed to increase the efficiency of models on Exynos chips. It includes features such as shape inference (to adjust input shapes), 4D conversion (ensuring all operator shapes are four-dimensional), and various optimization techniques like folding, fusing, and replacing operations to reduce computation and memory usage. These optimizations specifically target the Exynos hardware to improve execution speed and resource utilization.\n\n3. **Advanced Quantization Methods**:\n   - **SmoothQuant**: This method addresses quantization difficulties by migrating challenges from activations to weights, applying a mathematically equivalent transformation. It smooths out activation outliers, which can lead to better quantization results and improved overall model performance on hardware with limited precision support.\n   \n   - **Softmax Bias Correction**: It adds a float bias to the Softmax output layer to counteract quantization-induced performance degradation. This technique is particularly useful in maintaining the accuracy of the model after quantization when running on hardware with limited floating-point precision capabilities.\n\nBy optimizing models through these methods, performance and efficiency can be significantly enhanced on Exynos platforms, which may otherwise struggle with models initially designed for traditional floating-point hardware.",
        "3": "We do not have the information you requested. If you wish to contact support about this inquiry, please send an email to seed.ai@samsung.com.",
        "4": "The choice between AMD Ryzen 7 and Intel Core i7 processors can impact the performance of optimization techniques, particularly in tasks involving floating-point operations and quantization using the ENN SDK.\n\n**Hardware Specifications:**\n1. **AMD Ryzen 7** - Typically offers strong multi-core performance, which is useful for parallel processing tasks and can be advantageous for certain optimization and quantization processes.\n2. **Intel Core i7** - Known for its single-core performance, which may benefit workloads that are not heavily parallelized.\n\n**Performance in Optimization Techniques:**\n- **Floating-point Operations:** Both processors are capable of handling floating-point computations, but optimizing these for Exynos chips involves reducing reliance on floating-point precision due to hardware constraints.\n- **Quantization:** The Quantizer module of ENN SDK can apply fixed and mixed precision quantization to reduce the model size and improve performance on Exynos chips.\n\n**Optimization Techniques:**\n- **Optimizer:** In the context of Exynos, using the optimizer involves:\n  - **Shape Inference:** Modifying input shapes to optimize for model inference.\n  - **4dim Conversion:** Ensuring that models use four-dimensional input and output shapes for compatibility and optimal performance on Exynos hardware.\n\nIn summary, both processors are suitable for developing models with the ENN SDK, but the optimization for deployment on Exynos chips relies more on the techniques provided by the SDK (e.g., quantization and shape optimization) rather than the specific hardware used during development.",
        "5": "In the context of database generation, the default path `{workspace}/DATA/db_spec.yaml` is used as the specification path for the generation dataset. This path indicates where the specification file for the dataset is expected to be located by default. The `database_gen` dictionary in the YAML configuration file uses this path to define where to find or store the dataset specification, which is crucial for generating the required database for model processing or analysis.",
        "6": "The purpose of the 'userdb' parameter in the simulator configuration is to specify the path to the simulation data set. By default, the path is set to `{workspace}/DATA/data.txt`. This parameter is used to provide the necessary data for simulation purposes when using the simulator.",
        "7": "The text mentions three model types, and the following optimization methods are used for each:\n\n1. **Large Language Models (LLM):**\n   - Simplifier\n   - 4-Dimensional Conversion\n   - Optimization Template\n\n2. **Computer Vision (CV) Models:**\n   - Simplifier\n   - Optimization Template\n   - Fixed Precision Quantization\n\n3. **Large Vision Models (LVM):**\n   - Simplifier\n   - 4-Dimensional Conversion\n   - Optimization Template\n   - Smooth Quantization\n   - Mixed Precision Quantization\n   - Softmax Bias Correction",
        "8": "The 'check op support status' feature in the `model_analyzer` of the ENN SDK is used to verify whether the operations (ops) present in a model are supported by the ENN SDK. This helps ensure compatibility and identify unsupported operations that may need adjustments before the model can be executed using the SDK.",
        "9": "The different System on Chip (SoC) types mentioned in the model_analyzer documentation are Gen-5, Gen-5a, Gen-5b, and Gen-6."
    },
    "reference": {
        "0": "The text compares optimization techniques and evaluations in reports on floating-point hardware and quantization by detailing various optimization methods supported by the Optimizer, such as Fold, GeGLU, and LayerNorm, and advanced quantization methods like Softmax Bias Correction, SmoothQuant, and Cross Layer Equalization. It also discusses the process of model conversion and optimization, including the use of the simulator for inference and quantization, and highlights the importance of specific hardware specifications for optimal performance.",
        "1": "Quantization methods and optimization techniques compare in performance and efficiency by utilizing various advanced methods like Softmax Bias Correction and SmoothQuant for quantization, while optimization techniques include features like shape_inference and custom optimization templates to enhance model performance.",
        "2": "Various model conversion techniques and optimization methods, including the Optimizer and advanced quantization methods like SmoothQuant and Softmax Bias Correction, enhance performance and efficiency across different hardware platforms by addressing hardware constraints, improving model performance, and reducing quantization errors.",
        "3": "The performance evaluation metrics and optimization techniques for models utilizing floating-point hardware, various optimizers, and advanced quantization methods are compared through the Optimizer's features, which enhance model performance, and the simulator's ability to perform inference and compare models based on outputs or intermediate tensors.",
        "4": "The comparison between AMD Ryzen 7 and Intel Core i7 highlights that both processors are recommended for optimal performance in model optimization, with specifications including at least 4 cores and a 64-bit architecture.",
        "5": "The default path {workspace}/DATA/db_spec.yaml is significant as it specifies the location where the database generation dataset specification is stored.",
        "6": "The 'userdb' parameter in the simulator configuration specifies the simulation data set path, with the default path being {workspace}/DATA/data.txt.",
        "7": "The optimization methods used for each model type mentioned in the text are not specified, but it is indicated that optimization is performed differently depending on the model type, which includes CV, LVM, and LLM.",
        "8": "The purpose of the 'check op support status' feature in the model_analyzer is to determine whether specific operations are supported on the given system.",
        "9": "The different System on Chip types mentioned in the model_analyzer documentation are Gen-5, Gen-5a, Gen-5b, and Gen-6."
    },
    "context_recall": {
        "0": 0.0,
        "1": 0.5,
        "2": 1.0,
        "3": 0.5,
        "4": 1.0,
        "5": 1.0,
        "6": 1.0,
        "7": 1.0,
        "8": 1.0,
        "9": 1.0
    },
    "factual_correctness": {
        "0": 0.0,
        "1": 0.57,
        "2": 0.41,
        "3": 0.0,
        "4": 0.17,
        "5": 0.67,
        "6": 0.8,
        "7": 0.33,
        "8": 0.67,
        "9": 1.0
    },
    "faithfulness": {
        "0": 0.5714285714285714,
        "1": 0.5909090909090909,
        "2": 1.0,
        "3": 0.0,
        "4": 0.5789473684210527,
        "5": 0.8,
        "6": 1.0,
        "7": 0.6,
        "8": 0.6,
        "9": 1.0
    },
    "semantic_similarity": {
        "0": 0.8894194437412237,
        "1": 0.9170151513042262,
        "2": 0.9100273198617802,
        "3": 0.706490304712285,
        "4": 0.886783697437311,
        "5": 0.9543705307278674,
        "6": 0.982600012120382,
        "7": 0.891607220735152,
        "8": 0.9385663027635431,
        "9": 0.977510034780328
    }
}